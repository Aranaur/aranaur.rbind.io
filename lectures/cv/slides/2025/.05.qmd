---
title: "–ú–æ–¥—É–ª—å–Ω—ñ—Å—Ç—å üî•PyTorch"
subtitle: "–¢–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó –∫–æ–º–ø'—é—Ç–µ—Ä–Ω–æ–≥–æ –∑–æ—Ä—É"
author: "–Ü–≥–æ—Ä –ú—ñ—Ä–æ—à–Ω–∏—á–µ–Ω–∫–æ"
institute: –ö–ù–£ —ñ–º–µ–Ω—ñ –¢–∞—Ä–∞—Å–∞ –®–µ–≤—á–µ–Ω–∫–∞, –§–Ü–¢
from: markdown+emoji
title-slide-attributes:
    data-background-iframe: .05_files/libs/colored-particles/index.html
language: _language-ua.yml
footer: <a href="https://aranaur.rbind.io/lectures">üîó–¢–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó –∫–æ–º–ø'—é—Ç–µ—Ä–Ω–æ–≥–æ –∑–æ—Ä—É</a>
format:
  revealjs: 
    transition: fade
    chalkboard: true
    logo: fit.png
    slide-number: true
    # toc: true
    # toc-depth: 1
    mouse-wheel: true
    width: 1350  
    height: 759.375
    highlight-style: github
    fig-width: 9
    fig-height: 5
    fig-format: svg
    fig-align: center
    theme: [default, custom.scss]
#   gfm:
#     mermaid-format: png
preload-iframes: true
jupyter: python3
execute: 
  echo: true
  warning: false
editor_options: 
  chunk_output_type: console
---

```{python}
#| include: false

import torch
from torch import nn
import matplotlib.pyplot as plt
device = "cuda" if torch.cuda.is_available() else "cpu"


# Define colors
red_pink   = "#e64173"
turquoise  = "#20B2AA"
orange     = "#FFA500"
red        = "#fb6107"
blue       = "#181485"
navy       = "#150E37FF"
green      = "#8bb174"
yellow     = "#D8BD44"
purple     = "#6A5ACD"
slate      = "#314f4f"
```

## –©–æ —Ç–∞–∫–µ –º–æ–¥—É–ª—å–Ω—ñ—Å—Ç—å? {.smaller}

- **–ú–æ–¥—É–ª—å–Ω—ñ—Å—Ç—å** --- —Ü–µ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—è —Ä–æ–∑—Ä–æ–±–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–Ω–æ–≥–æ –∑–∞–±–µ–∑–ø–µ—á–µ–Ω–Ω—è, —è–∫–∞ –ø–µ—Ä–µ–¥–±–∞—á–∞—î —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è —Å–∏—Å—Ç–µ–º–∏ –Ω–∞ –æ–∫—Ä–µ–º—ñ, –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ –∞–±–æ –º–æ–¥—É–ª—ñ.
- –ö–æ–∂–µ–Ω –º–æ–¥—É–ª—å –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –∑–∞ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è **–ø–µ–≤–Ω–æ—ó —Ñ—É–Ω–∫—Ü—ñ—ó** –∞–±–æ –Ω–∞–±–æ—Ä—É —Ñ—É–Ω–∫—Ü—ñ–π.
- –ú–æ–¥—É–ª—ñ –º–æ–∂—É—Ç—å –±—É—Ç–∏ —Ä–æ–∑—Ä–æ–±–ª–µ–Ω—ñ, –ø—Ä–æ—Ç–µ—Å—Ç–æ–≤–∞–Ω—ñ —Ç–∞ –ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω—ñ –æ–∫—Ä–µ–º–æ, —â–æ —Å–ø—Ä–∏—è—î –∫—Ä–∞—â—ñ–π **–æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—ó –∫–æ–¥—É** —Ç–∞ –ø–æ–ª–µ–≥—à—É—î –π–æ–≥–æ —Ä–æ–∑—É–º—ñ–Ω–Ω—è.
- –ú–æ–¥—É–ª—å–Ω—ñ—Å—Ç—å –¥–æ–∑–≤–æ–ª—è—î **–ø–æ–≤—Ç–æ—Ä–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è** –∫–æ–¥—É, –æ—Å–∫—ñ–ª—å–∫–∏ –º–æ–¥—É–ª—ñ –º–æ–∂—É—Ç—å –±—É—Ç–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω—ñ –≤ —Ä—ñ–∑–Ω–∏—Ö —á–∞—Å—Ç–∏–Ω–∞—Ö –ø—Ä–æ–≥—Ä–∞–º–∏ –∞–±–æ –Ω–∞–≤—ñ—Ç—å –≤ —ñ–Ω—à–∏—Ö –ø—Ä–æ–µ–∫—Ç–∞—Ö.
- –í–æ–Ω–∞ —Ç–∞–∫–æ–∂ —Å–ø—Ä–∏—è—î —Å–ø—Ä–æ—â–µ–Ω–Ω—é –ø—Ä–æ—Ü–µ—Å—É **–Ω–∞–ª–∞–≥–æ–¥–∂–µ–Ω–Ω—è** —Ç–∞ **–æ–Ω–æ–≤–ª–µ–Ω–Ω—è** –ø—Ä–æ–≥—Ä–∞–º–Ω–æ–≥–æ –∑–∞–±–µ–∑–ø–µ—á–µ–Ω–Ω—è, –æ—Å–∫—ñ–ª—å–∫–∏ –∑–º—ñ–Ω–∏ –≤ –æ–¥–Ω–æ–º—É –º–æ–¥—É–ª—ñ –Ω–µ –≤–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ —ñ–Ω—à—ñ –º–æ–¥—É–ª—ñ.

## –ü—Ä–∏–∫–ª–∞–¥

- `data_setup.py` ‚Äî —Ñ–∞–π–ª –¥–ª—è –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∏ —Ç–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö —É —Ä–∞–∑—ñ –ø–æ—Ç—Ä–µ–±–∏.
- `engine.py` ‚Äî —Ñ–∞–π–ª, —â–æ –º—ñ—Å—Ç–∏—Ç—å —Ä—ñ–∑–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó –Ω–∞–≤—á–∞–Ω–Ω—è.
- `model_builder.py` –∞–±–æ `model.py` ‚Äî —Ñ–∞–π–ª –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ PyTorch.
- `train.py` ‚Äî —Ñ–∞–π–ª –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –≤—Å—ñ—Ö —ñ–Ω—à–∏—Ö —Ñ–∞–π–ª—ñ–≤ —Ç–∞ –Ω–∞–≤—á–∞–Ω–Ω—è —Ü—ñ–ª—å–æ–≤–æ—ó –º–æ–¥–µ–ª—ñ PyTorch.
- `utils.py` ‚Äî —Ñ–∞–π–ª, –ø—Ä–∏—Å–≤—è—á–µ–Ω–∏–π –∫–æ—Ä–∏—Å–Ω–∏–º —É—Ç–∏–ª—ñ—Ç–∞—Ä–Ω–∏–º —Ñ—É–Ω–∫—Ü—ñ—è–º.

## Notebook vs Script {.smaller}

::: {.columns}
::: {.column}
**Notebook**

‚úÖ –õ–µ–≥–∫–æ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É–≤–∞—Ç–∏/–ø–æ—á–∞—Ç–∏ —Ä–æ–±–æ—Ç—É<br>
‚úÖ –õ–µ–≥–∫–æ –¥—ñ–ª–∏—Ç–∏—Å—è<br>
‚úÖ –î—É–∂–µ –Ω–∞–æ—á–Ω–∏–π<br><br>
‚ùå –í–∞–∂–∫–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ —è–∫—É—Å—å —á–∞—Å—Ç–∏–Ω—É<br>
‚ùå –í–∞–∂–∫–æ –ø–æ–≤—Ç–æ—Ä–Ω–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏<br>
‚ùå –¢–µ–∫—Å—Ç —Ç–∞ –≥—Ä–∞—Ñ—ñ–∫–∞ –º–æ–∂—É—Ç—å –∑–∞–≤–∞–∂–∞—Ç–∏
:::
::: {.column}
**Script**

‚úÖ –ú–æ–∂–ª–∏–≤—ñ—Å—Ç—å –æ–±'—î–¥–Ω–∞–Ω–Ω—è –∫–æ–¥—É –≤ –ø–∞–∫–µ—Ç<br>
‚úÖ –õ–µ–≥–∫–æ –ø–æ–≤—Ç–æ—Ä–Ω–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏<br>
‚úÖ –ë–∞–≥–∞—Ç–æ –ø—Ä–æ–µ–∫—Ç—ñ–≤ –∑ –≤—ñ–¥–∫—Ä–∏—Ç–∏–º –∫–æ–¥–æ–º –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å —Å–∫—Ä–∏–ø—Ç–∏<br>
‚úÖ –ë—ñ–ª—å—à—ñ –ø—Ä–æ–µ–∫—Ç–∏ –º–æ–∂–Ω–∞ –≤–∏–∫–æ–Ω—É–≤–∞—Ç–∏ –Ω–∞ —Ö–º–∞—Ä–Ω–∏—Ö —Å–µ—Ä–≤–µ—Ä–∞—Ö<br><br>
‚ùå –ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏ –Ω–µ –Ω–∞—Å—Ç—ñ–ª—å–∫–∏ –Ω–∞–æ—á–Ω—ñ
:::
:::

## PyTorch: –º–æ–¥—É–ª—å–Ω—ñ—Å—Ç—å {.smaller}

–í —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç—ñ –±–µ–∑–ª—ñ—á —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ—ó–≤ –∑ –∫–æ–¥–æ–º, —è–∫—ñ –º–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è.

```{md}
python train.py --model MODEL_NAME --batch_size BATCH_SIZE --lr LEARNING_RATE --num_epochs NUM_EPOCHS
```

<br>

. . .

```{md}
python train.py --model tinyvgg --batch_size 32 --lr 0.001 --num_epochs 10
```

<br>

. . .

```{md}
torchrun --nproc_per_node=8 train.py --model resnet50 --batch-size 128 --lr 0.5 \
--lr-scheduler cosineannealinglr --lr-warmup-epochs 5 --lr-warmup-method linear \
--auto-augment ta_wide --epochs 600 --random-erase 0.1 --weight-decay 0.00002 \
--norm-weight-decay 0.0 --label-smoothing 0.1 --mixup-alpha 0.2 --cutmix-alpha 1.0 \
--train-crop-size 176 --model-ema --val-resize-size 232 --ra-sampler --ra-reps 4
```

::: footer
[Training reference script](https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/#the-training-recipe)
:::

## –©–æ –æ—Ç—Ä–∏–º–∞—î–º–æ? {.tiny}

1. –ë—É–¥—É–≤–∞—Ç–∏ –º–æ–¥–µ–ª—ñ –≤ –æ–¥–∏–Ω —Ä—è–¥–æ–∫ –∫–æ–¥—É.
2. –°—Ç—Ä—É–∫—Ç—É—Ä—É–≤–∞—Ç–∏ –∫–æ–¥:

```{md}
going_modular/
‚îú‚îÄ‚îÄ going_modular/
‚îÇ   ‚îú‚îÄ‚îÄ data_setup.py
‚îÇ   ‚îú‚îÄ‚îÄ engine.py
‚îÇ   ‚îú‚îÄ‚îÄ model_builder.py
‚îÇ   ‚îú‚îÄ‚îÄ train.py
‚îÇ   ‚îî‚îÄ‚îÄ utils.py
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ 05_going_modular_cell_mode_tinyvgg_model.pth
‚îÇ   ‚îî‚îÄ‚îÄ 05_going_modular_script_mode_tinyvgg_model.pth
‚îî‚îÄ‚îÄ data/
    ‚îî‚îÄ‚îÄ pizza_steak_sushi/
        ‚îú‚îÄ‚îÄ train/
        ‚îÇ   ‚îú‚îÄ‚îÄ pizza/
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image01.jpeg
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
        ‚îÇ   ‚îú‚îÄ‚îÄ steak/
        ‚îÇ   ‚îî‚îÄ‚îÄ sushi/
        ‚îî‚îÄ‚îÄ test/
            ‚îú‚îÄ‚îÄ pizza/
            ‚îú‚îÄ‚îÄ steak/
            ‚îî‚îÄ‚îÄ sushi/
```

## `data_setup.py` {.tiny .scrollable}

- –§–∞–π–ª –¥–ª—è –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∏ —Ç–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö —É —Ä–∞–∑—ñ –ø–æ—Ç—Ä–µ–±–∏.

```{.python filename="data_setup.py"}
%%writefile going_modular/data_setup.py
"""
Contains functionality for creating PyTorch DataLoaders for 
image classification data.
"""
import os

from torchvision import datasets, transforms
from torch.utils.data import DataLoader

NUM_WORKERS = os.cpu_count()

def create_dataloaders(
    train_dir: str, 
    test_dir: str, 
    transform: transforms.Compose, 
    batch_size: int, 
    num_workers: int=NUM_WORKERS
):
  """Creates training and testing DataLoaders.

  Takes in a training directory and testing directory path and turns
  them into PyTorch Datasets and then into PyTorch DataLoaders.

  Args:
    train_dir: Path to training directory.
    test_dir: Path to testing directory.
    transform: torchvision transforms to perform on training and testing data.
    batch_size: Number of samples per batch in each of the DataLoaders.
    num_workers: An integer for number of workers per DataLoader.

  Returns:
    A tuple of (train_dataloader, test_dataloader, class_names).
    Where class_names is a list of the target classes.
    Example usage:
      train_dataloader, test_dataloader, class_names = \
        = create_dataloaders(train_dir=path/to/train_dir,
                             test_dir=path/to/test_dir,
                             transform=some_transform,
                             batch_size=32,
                             num_workers=4)
  """
  # Use ImageFolder to create dataset(s)
  train_data = datasets.ImageFolder(train_dir, transform=transform)
  test_data = datasets.ImageFolder(test_dir, transform=transform)

  # Get class names
  class_names = train_data.classes

  # Turn images into data loaders
  train_dataloader = DataLoader(
      train_data,
      batch_size=batch_size,
      shuffle=True,
      num_workers=num_workers,
      pin_memory=True,
  )
  test_dataloader = DataLoader(
      test_data,
      batch_size=batch_size,
      shuffle=False, # don't need to shuffle test data
      num_workers=num_workers,
      pin_memory=True,
  )

  return train_dataloader, test_dataloader, class_names
```

## –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è `data_setup.py`

```{python}
#| eval: false

# Import data_setup.py
from going_modular import data_setup

# Create train/test dataloader and get class names as a list
train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(...)
```

## `model_builder.py` {.tiny .scrollable}

- –§–∞–π–ª –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ PyTorch.

```{.python filename="model_builder.py"}
%%writefile going_modular/model_builder.py
"""
Contains PyTorch model code to instantiate a TinyVGG model.
"""
import torch
from torch import nn 

class TinyVGG(nn.Module):
  """Creates the TinyVGG architecture.

  Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.
  See the original architecture here: https://poloclub.github.io/cnn-explainer/

  Args:
    input_shape: An integer indicating number of input channels.
    hidden_units: An integer indicating number of hidden units between layers.
    output_shape: An integer indicating number of output units.
  """
  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:
      super().__init__()
      self.conv_block_1 = nn.Sequential(
          nn.Conv2d(in_channels=input_shape, 
                    out_channels=hidden_units, 
                    kernel_size=3, 
                    stride=1, 
                    padding=0),  
          nn.ReLU(),
          nn.Conv2d(in_channels=hidden_units, 
                    out_channels=hidden_units,
                    kernel_size=3,
                    stride=1,
                    padding=0),
          nn.ReLU(),
          nn.MaxPool2d(kernel_size=2,
                        stride=2)
      )
      self.conv_block_2 = nn.Sequential(
          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),
          nn.ReLU(),
          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),
          nn.ReLU(),
          nn.MaxPool2d(2)
      )
      self.classifier = nn.Sequential(
          nn.Flatten(),
          # Where did this in_features shape come from? 
          # It's because each layer of our network compresses and changes the shape of our inputs data.
          nn.Linear(in_features=hidden_units*13*13,
                    out_features=output_shape)
      )

  def forward(self, x: torch.Tensor):
      x = self.conv_block_1(x)
      x = self.conv_block_2(x)
      x = self.classifier(x)
      return x
      # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # <- leverage the benefits of operator fusion
```

## –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è `model_builder.py`

```{python}
#| eval: false

import torch
# Import model_builder.py
from going_modular import model_builder
device = "cuda" if torch.cuda.is_available() else "cpu"

# Instantiate an instance of the model from the "model_builder.py" script
torch.manual_seed(73)
model = model_builder.TinyVGG(input_shape=3,
                              hidden_units=10, 
                              output_shape=len(class_names)).to(device)
```

## `engine.py` {.tiny .scrollable}

- –§–∞–π–ª, —â–æ –º—ñ—Å—Ç–∏—Ç—å —Ä—ñ–∑–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó –Ω–∞–≤—á–∞–Ω–Ω—è.

```{.python filename="engine.py"}
%%writefile going_modular/engine.py
"""
Contains functions for training and testing a PyTorch model.
"""
import torch

from tqdm.auto import tqdm
from typing import Dict, List, Tuple

def train_step(model: torch.nn.Module, 
               dataloader: torch.utils.data.DataLoader, 
               loss_fn: torch.nn.Module, 
               optimizer: torch.optim.Optimizer,
               device: torch.device) -> Tuple[float, float]:
  """Trains a PyTorch model for a single epoch.

  Turns a target PyTorch model to training mode and then
  runs through all of the required training steps (forward
  pass, loss calculation, optimizer step).

  Args:
    model: A PyTorch model to be trained.
    dataloader: A DataLoader instance for the model to be trained on.
    loss_fn: A PyTorch loss function to minimize.
    optimizer: A PyTorch optimizer to help minimize the loss function.
    device: A target device to compute on (e.g. "cuda" or "cpu").

  Returns:
    A tuple of training loss and training accuracy metrics.
    In the form (train_loss, train_accuracy). For example:

    (0.1112, 0.8743)
  """
  # Put model in train mode
  model.train()

  # Setup train loss and train accuracy values
  train_loss, train_acc = 0, 0

  # Loop through data loader data batches
  for batch, (X, y) in enumerate(dataloader):
      # Send data to target device
      X, y = X.to(device), y.to(device)

      # 1. Forward pass
      y_pred = model(X)

      # 2. Calculate  and accumulate loss
      loss = loss_fn(y_pred, y)
      train_loss += loss.item() 

      # 3. Optimizer zero grad
      optimizer.zero_grad()

      # 4. Loss backward
      loss.backward()

      # 5. Optimizer step
      optimizer.step()

      # Calculate and accumulate accuracy metric across all batches
      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)
      train_acc += (y_pred_class == y).sum().item()/len(y_pred)

  # Adjust metrics to get average loss and accuracy per batch 
  train_loss = train_loss / len(dataloader)
  train_acc = train_acc / len(dataloader)
  return train_loss, train_acc

def test_step(model: torch.nn.Module, 
              dataloader: torch.utils.data.DataLoader, 
              loss_fn: torch.nn.Module,
              device: torch.device) -> Tuple[float, float]:
  """Tests a PyTorch model for a single epoch.

  Turns a target PyTorch model to "eval" mode and then performs
  a forward pass on a testing dataset.

  Args:
    model: A PyTorch model to be tested.
    dataloader: A DataLoader instance for the model to be tested on.
    loss_fn: A PyTorch loss function to calculate loss on the test data.
    device: A target device to compute on (e.g. "cuda" or "cpu").

  Returns:
    A tuple of testing loss and testing accuracy metrics.
    In the form (test_loss, test_accuracy). For example:

    (0.0223, 0.8985)
  """
  # Put model in eval mode
  model.eval() 

  # Setup test loss and test accuracy values
  test_loss, test_acc = 0, 0

  # Turn on inference context manager
  with torch.inference_mode():
      # Loop through DataLoader batches
      for batch, (X, y) in enumerate(dataloader):
          # Send data to target device
          X, y = X.to(device), y.to(device)

          # 1. Forward pass
          test_pred_logits = model(X)

          # 2. Calculate and accumulate loss
          loss = loss_fn(test_pred_logits, y)
          test_loss += loss.item()

          # Calculate and accumulate accuracy
          test_pred_labels = test_pred_logits.argmax(dim=1)
          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))

  # Adjust metrics to get average loss and accuracy per batch 
  test_loss = test_loss / len(dataloader)
  test_acc = test_acc / len(dataloader)
  return test_loss, test_acc

def train(model: torch.nn.Module, 
          train_dataloader: torch.utils.data.DataLoader, 
          test_dataloader: torch.utils.data.DataLoader, 
          optimizer: torch.optim.Optimizer,
          loss_fn: torch.nn.Module,
          epochs: int,
          device: torch.device) -> Dict[str, List]:
  """Trains and tests a PyTorch model.

  Passes a target PyTorch models through train_step() and test_step()
  functions for a number of epochs, training and testing the model
  in the same epoch loop.

  Calculates, prints and stores evaluation metrics throughout.

  Args:
    model: A PyTorch model to be trained and tested.
    train_dataloader: A DataLoader instance for the model to be trained on.
    test_dataloader: A DataLoader instance for the model to be tested on.
    optimizer: A PyTorch optimizer to help minimize the loss function.
    loss_fn: A PyTorch loss function to calculate loss on both datasets.
    epochs: An integer indicating how many epochs to train for.
    device: A target device to compute on (e.g. "cuda" or "cpu").

  Returns:
    A dictionary of training and testing loss as well as training and
    testing accuracy metrics. Each metric has a value in a list for 
    each epoch.
    In the form: {train_loss: [...],
                  train_acc: [...],
                  test_loss: [...],
                  test_acc: [...]} 
    For example if training for epochs=2: 
                 {train_loss: [2.0616, 1.0537],
                  train_acc: [0.3945, 0.3945],
                  test_loss: [1.2641, 1.5706],
                  test_acc: [0.3400, 0.2973]} 
  """
  # Create empty results dictionary
  results = {"train_loss": [],
      "train_acc": [],
      "test_loss": [],
      "test_acc": []
  }

  # Loop through training and testing steps for a number of epochs
  for epoch in tqdm(range(epochs)):
      train_loss, train_acc = train_step(model=model,
                                          dataloader=train_dataloader,
                                          loss_fn=loss_fn,
                                          optimizer=optimizer,
                                          device=device)
      test_loss, test_acc = test_step(model=model,
          dataloader=test_dataloader,
          loss_fn=loss_fn,
          device=device)

      # Print out what's happening
      print(
          f"Epoch: {epoch+1} | "
          f"train_loss: {train_loss:.4f} | "
          f"train_acc: {train_acc:.4f} | "
          f"test_loss: {test_loss:.4f} | "
          f"test_acc: {test_acc:.4f}"
      )

      # Update results dictionary
      results["train_loss"].append(train_loss)
      results["train_acc"].append(train_acc)
      results["test_loss"].append(test_loss)
      results["test_acc"].append(test_acc)

  # Return the filled results at the end of the epochs
  return results
```

## –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è `engine.py`

```{python}
#| eval: false

# Import engine.py
from going_modular import engine

# Use train() by calling it from engine.py
engine.train(...)
```

## `utils.py` {.tiny .scrollable}

- –§–∞–π–ª, –ø—Ä–∏—Å–≤—è—á–µ–Ω–∏–π –∫–æ—Ä–∏—Å–Ω–∏–º —É—Ç–∏–ª—ñ—Ç–∞—Ä–Ω–∏–º —Ñ—É–Ω–∫—Ü—ñ—è–º.

```{.python filename="utils.py"}
%%writefile going_modular/utils.py
"""
Contains various utility functions for PyTorch model training and saving.
"""
import torch
from pathlib import Path

def save_model(model: torch.nn.Module,
               target_dir: str,
               model_name: str):
  """Saves a PyTorch model to a target directory.

  Args:
    model: A target PyTorch model to save.
    target_dir: A directory for saving the model to.
    model_name: A filename for the saved model. Should include
      either ".pth" or ".pt" as the file extension.

  Example usage:
    save_model(model=model_0,
               target_dir="models",
               model_name="05_going_modular_tingvgg_model.pth")
  """
  # Create target directory
  target_dir_path = Path(target_dir)
  target_dir_path.mkdir(parents=True,
                        exist_ok=True)

  # Create model save path
  assert model_name.endswith(".pth") or model_name.endswith(".pt"), "model_name should end with '.pt' or '.pth'"
  model_save_path = target_dir_path / model_name

  # Save the model state_dict()
  print(f"[INFO] Saving model to: {model_save_path}")
  torch.save(obj=model.state_dict(),
             f=model_save_path)
```

## –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è `utils.py`

```{python}
#| eval: false

# Import utils.py
from going_modular import utils

# Save a model to file
save_model(model=...
           target_dir=...,
           model_name=...)
```

## `train.py` {.tiny .scrollable}

–©–æ–± —Å—Ç–≤–æ—Ä–∏—Ç–∏ `train.py`, –º–∏ –≤–∏–∫–æ–Ω–∞—î–º–æ –Ω–∞—Å—Ç—É–ø–Ω—ñ –∫—Ä–æ–∫–∏:

1. –Ü–º–ø–æ—Ä—Ç—É—î–º–æ —Ä—ñ–∑–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ, –∞ —Å–∞–º–µ `torch`, `os`, `torchvision.transforms` —ñ –≤—Å—ñ —Å–∫—Ä–∏–ø—Ç–∏ –∑ –∫–∞—Ç–∞–ª–æ–≥—É `going_modular`, `data_setup`, `engine`, `model_builder`, `utils`.
–ü—Ä–∏–º—ñ—Ç–∫–∞: –æ—Å–∫—ñ–ª—å–∫–∏ train.py –±—É–¥–µ –∑–Ω–∞—Ö–æ–¥–∏—Ç–∏—Å—è –≤ –∫–∞—Ç–∞–ª–æ–∑—ñ going_modular, –º–∏ –º–æ–∂–µ–º–æ —ñ–º–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ —ñ–Ω—à—ñ –º–æ–¥—É–ª—ñ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é import ..., –∞ –Ω–µ from going_modular import ....
2. –ù–∞–ª–∞—à—Ç—É—î–º–æ —Ä—ñ–∑–Ω—ñ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏, —Ç–∞–∫—ñ —è–∫ —Ä–æ–∑–º—ñ—Ä –ø–∞–∫–µ—Ç–∞, –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö, —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–∏—Ö–æ–≤–∞–Ω–∏—Ö –æ–¥–∏–Ω–∏—Ü—å (—ó—Ö –º–æ–∂–Ω–∞ –±—É–¥–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ –≤ –º–∞–π–±—É—Ç–Ω—å–æ–º—É –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é argparse Python).
3. –ù–∞–ª–∞—à—Ç—É—î–º–æ –∫–∞—Ç–∞–ª–æ–≥–∏ –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è.
4. –ù–∞–ª–∞—à—Ç—É—î–º–æ –∫–æ–¥, –Ω–µ–∑–∞–ª–µ–∂–Ω–∏–π –≤—ñ–¥ –ø—Ä–∏—Å—Ç—Ä–æ—é.
5. –°—Ç–≤–æ—Ä–∏–º–æ –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö.
6. –°—Ç–≤–æ—Ä–∏–º–æ DataLoaders –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é `data_setup.py`.
7. –°—Ç–≤–æ—Ä–∏–º–æ –º–æ–¥–µ–ª—å –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é `model_builder.py`.
8. –ù–∞–ª–∞—à—Ç—É—î–º–æ –∫–∞—Ç–∞–ª–æ–≥–∏ –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è.
9. –ù–∞–ª–∞—à—Ç—É—î–º–æ —Ñ—É–Ω–∫—Ü—ñ—é –≤—Ç—Ä–∞—Ç —Ç–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä.
10. –ù–∞–≤—á–∏–º–æ –º–æ–¥–µ–ª—å –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é `engine.py`.
11. –ó–±–µ—Ä–µ–∂–µ–º–æ –º–æ–¥–µ–ª—å –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é `utils.py`.

## `train.py` {.tiny .scrollable}

```{.python filename="train.py"}
%%writefile going_modular/train.py
"""
Trains a PyTorch image classification model using device-agnostic code.
"""

import os
import torch
import data_setup, engine, model_builder, utils

from torchvision import transforms


def main():
    # Setup hyperparameters
    NUM_EPOCHS = 5
    BATCH_SIZE = 32
    HIDDEN_UNITS = 10
    LEARNING_RATE = 0.001

    # Setup directories
    train_dir = "data/pizza_steak_sushi/train"
    test_dir = "data/pizza_steak_sushi/test"

    # Setup target device
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # Create transforms
    data_transform = transforms.Compose([
        transforms.Resize((64, 64)),
        transforms.ToTensor()
    ])

    # Create DataLoaders with help from data_setup.py
    train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(
        train_dir=train_dir,
        test_dir=test_dir,
        transform=data_transform,
        batch_size=BATCH_SIZE
    )

    # Create model with help from model_builder.py
    model = model_builder.TinyVGG(
        input_shape=3,
        hidden_units=HIDDEN_UNITS,
        output_shape=len(class_names)
    ).to(device)

    # Set loss and optimizer
    loss_fn = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(),
                                 lr=LEARNING_RATE)

    # Start training with help from engine.py
    engine.train(model=model,
                 train_dataloader=train_dataloader,
                 test_dataloader=test_dataloader,
                 loss_fn=loss_fn,
                 optimizer=optimizer,
                 epochs=NUM_EPOCHS,
                 device=device)

    # Save the model with help from utils.py
    utils.save_model(model=model,
                     target_dir="models",
                     model_name="05_going_modular_script_mode_tinyvgg_model.pth")


if __name__ == "__main__":
    torch.multiprocessing.freeze_support()  # –±–µ–∑–ø–µ—á–Ω–æ –Ω–∞ Windows
    main()
```

## –ó–∞–ø—É—Å–∫–∞—î–º–æ –∫–æ–¥ {.smaller}

```bash
python going_modular/train.py
```

<br>

```bash
python going_modular/train.py --model MODEL_NAME --batch_size BATCH_SIZE --lr LEARNING_RATE --num_epochs NUM_EPOCHS
```

# –î—è–∫—É—é –∑–∞ —É–≤–∞–≥—É! {.unnumbered .unlisted background-iframe=".05_files/libs/colored-particles/index.html"}

<br> <br>

{{< iconify solar book-bold >}} [–ú–∞—Ç–µ—Ä—ñ–∞–ª–∏ –∫—É—Ä—Å—É](https://aranaur.rbind.io/lectures/)

{{< iconify mdi envelope >}} ihor.miroshnychenko\@knu.ua

{{< iconify ic baseline-telegram >}} [Data Mirosh](https://t.me/araprof)

{{< iconify mdi linkedin >}} [\@ihormiroshnychenko](https://www.linkedin.com/in/ihormiroshnychenko/)

{{< iconify mdi github >}} [\@aranaur](https://github.com/Aranaur)

{{< iconify ion home >}} [aranaur.rbind.io](https://aranaur.rbind.io)
