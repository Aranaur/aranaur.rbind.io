---
title: "üî•PyTorch Transfer Learning "
subtitle: "–¢–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó –∫–æ–º–ø'—é—Ç–µ—Ä–Ω–æ–≥–æ –∑–æ—Ä—É"
author: "–Ü–≥–æ—Ä –ú—ñ—Ä–æ—à–Ω–∏—á–µ–Ω–∫–æ"
institute: –ö–ù–£ —ñ–º–µ–Ω—ñ –¢–∞—Ä–∞—Å–∞ –®–µ–≤—á–µ–Ω–∫–∞, –§–Ü–¢
from: markdown+emoji
title-slide-attributes:
    data-background-iframe: .06_files/libs/colored-particles/index.html
language: _language-ua.yml
footer: <a href="https://aranaur.rbind.io/lectures">üîó–¢–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó –∫–æ–º–ø'—é—Ç–µ—Ä–Ω–æ–≥–æ –∑–æ—Ä—É</a>
format:
  revealjs: 
    transition: fade
    chalkboard: true
    logo: fit.png
    slide-number: true
    # toc: true
    # toc-depth: 1
    mouse-wheel: true
    width: 1350  
    height: 759.375
    highlight-style: github
    fig-width: 9
    fig-height: 5
    fig-format: svg
    fig-align: center
    theme: [default, custom.scss]
#   gfm:
#     mermaid-format: png
preload-iframes: true
jupyter: python3
execute: 
  echo: true
  warning: false
editor_options: 
  chunk_output_type: console
---

```{python}
#| include: false

import torch
from torch import nn
import matplotlib.pyplot as plt
device = "cuda" if torch.cuda.is_available() else "cpu"


# Define colors
red_pink   = "#e64173"
turquoise  = "#20B2AA"
orange     = "#FFA500"
red        = "#fb6107"
blue       = "#181485"
navy       = "#150E37FF"
green      = "#8bb174"
yellow     = "#D8BD44"
purple     = "#6A5ACD"
slate      = "#314f4f"
```

## –©–æ —Ç–∞–∫–µ Transfer Learning?

- **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∑–Ω–∞–Ω—å**, –æ—Ç—Ä–∏–º–∞–Ω–∏—Ö –ø—Ä–∏ —Ä–æ–∑–≤'—è–∑–∞–Ω–Ω—ñ **–æ–¥–Ω—ñ—î—ó –∑–∞–¥–∞—á—ñ**, –¥–ª—è —Ä–æ–∑–≤'—è–∑–∞–Ω–Ω—è **—ñ–Ω—à–æ—ó –∑–∞–¥–∞—á—ñ**
- –£ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ –≥–ª–∏–±–æ–∫–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è, —Ü–µ —á–∞—Å—Ç–æ –æ–∑–Ω–∞—á–∞—î –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è **–ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞–≤—á–µ–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π**
- –û—Å–æ–±–ª–∏–≤–æ –∫–æ—Ä–∏—Å–Ω–æ —É –∫–æ–º–ø'—é—Ç–µ—Ä–Ω–æ–º—É –∑–æ—Ä—ñ, –¥–µ –≤–µ–ª–∏–∫—ñ –Ω–∞–±–æ—Ä–∏ –¥–∞–Ω–∏—Ö (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, ImageNet) –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π, —è–∫—ñ –ø–æ—Ç—ñ–º –º–æ–∂—É—Ç—å –±—É—Ç–∏ –∞–¥–∞–ø—Ç–æ–≤–∞–Ω—ñ –¥–æ —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω–∏—Ö –∑–∞–¥–∞—á

## –î–ª—è —á–æ–≥–æ –ø–æ—Ç—Ä—ñ–±–µ–Ω Transfer Learning?

–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è Transfer Learning –º–∞—î –¥–≤—ñ –æ—Å–Ω–æ–≤–Ω—ñ –ø–µ—Ä–µ–≤–∞–≥–∏:

- –ú–æ–∂–ª–∏–≤—ñ—Å—Ç—å –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —ñ—Å–Ω—É—é—á—É –º–æ–¥–µ–ª—å (–∑–∞–∑–≤–∏—á–∞–π –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É –Ω–µ–π—Ä–æ–Ω–Ω–æ—ó –º–µ—Ä–µ–∂—ñ), —è–∫–∞ –≤–∂–µ –¥–æ–≤–µ–ª–∞ —Å–≤–æ—é –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å —É –≤–∏—Ä—ñ—à–µ–Ω–Ω—ñ –∑–∞–≤–¥–∞–Ω—å.
- –ú–æ–∂–ª–∏–≤—ñ—Å—Ç—å –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —Ä–æ–±–æ—á—É –º–æ–¥–µ–ª—å, —è–∫–∞ –≤–∂–µ –Ω–∞–≤—á–∏–ª–∞—Å—è —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞—Ç–∏ –∑–∞–∫–æ–Ω–æ–º—ñ—Ä–Ω–æ—Å—Ç—ñ –Ω–∞ –¥–∞–Ω–∏—Ö, —Å—Ö–æ–∂–∏—Ö –Ω–∞ –Ω–∞—à—ñ. –¶–µ —á–∞—Å—Ç–æ –¥–æ–∑–≤–æ–ª—è—î –¥–æ—Å—è–≥—Ç–∏ —á—É–¥–æ–≤–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∑ –º–µ–Ω—à–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –≤–ª–∞—Å–Ω–∏—Ö –¥–∞–Ω–∏—Ö.

## –î–µ —à—É–∫–∞—Ç–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞–≤—á–µ–Ω—ñ –º–æ–¥–µ–ª—ñ?

- PyTorch: `torchvision.models`, `torchtext.models`, `torchaudio.models`, `torchrec.models`
- HuggingFace Hub: [–º–æ–¥–µ–ª—ñ](https://huggingface.co/models) —Ç–∞ [–Ω–∞–±–æ—Ä–∏ –¥–∞–Ω–∏—Ö](https://huggingface.co/datasets).
- `timm` (PyTorch Image Models): [—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ–π](https://github.com/huggingface/pytorch-image-models)
- Trending Papers with Code: [—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ–π](https://huggingface.co/papers/trending)

## –ü–æ—á–∏–Ω–∞—î–º–æ... {.tiny .scrollable}

```{python}
import torch
import torchvision

print(f"torch version: {torch.__version__}")
print(f"torchvision version: {torchvision.__version__}")

import matplotlib.pyplot as plt
from torch import nn
from torchvision import transforms
from torchinfo import summary

from going_modular.going_modular import data_setup, engine

# Setup device agnostic code
device = "cuda" if torch.cuda.is_available() else "cpu"
device

import gdown
import tempfile
import zipfile
import os
from pathlib import Path

file_url = 'https://drive.google.com/file/d/1Jf8jhskDmN3DxP0yk9PET5Kr44OnwUgU/view?usp=sharing'
file_id = file_url.split('/')[-2]

with tempfile.TemporaryDirectory() as tmpdir:
    print(f'–°—Ç–≤–æ—Ä–µ–Ω–æ —Ç–∏–º—á–∞—Å–æ–≤—É –ø–∞–ø–∫—É: {tmpdir}')

    zip_path = os.path.join(tmpdir, 'pizza_steak_sushi.zip')

    print('–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ñ–∞–π–ª—É...')
    gdown.download(id=file_id, output=zip_path, quiet=False)
    print('–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ.')

    extract_path = 'going_modular/pizza_steak_sushi'
    os.makedirs(extract_path, exist_ok=True)

    print(f'–†–æ–∑–∞—Ä—Ö—ñ–≤–∞—Ü—ñ—è —Ñ–∞–π–ª—É –≤ {extract_path}...')
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)
    print('–†–æ–∑–∞—Ä—Ö—ñ–≤–∞—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.')

print('–¢–∏–º—á–∞—Å–æ–≤—É –ø–∞–ø–∫—É —Ç–∞ —ó—ó –≤–º—ñ—Å—Ç –≤–∏–¥–∞–ª–µ–Ω–æ.')

# Setup Dirs
data_path = Path("going_modular/")
train_dir = data_path / "pizza_steak_sushi/train"
test_dir = data_path / "pizza_steak_sushi/test"
```

## –ì–æ—Ç—É—î–º–æ –¥–∞–Ω—ñ {.smaller}

- –†–∞–Ω—ñ—à–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ –±—É–ª–æ –≤ "—Ä—É—á–Ω–æ–º—É" —Ä–µ–∂–∏–º—ñ –≥–æ—Ç—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ –¥–æ –≤–∏–º–æ–≥ –º–æ–¥–µ–ª—ñ.
- –ó–∞—Ä–∞–∑ –±—ñ–ª—å—à—ñ—Å—Ç—å –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞–≤—á–µ–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π –º–∞—é—Ç—å –≤–±—É–¥–æ–≤–∞–Ω—ñ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó, —è–∫—ñ –º–æ–∂–Ω–∞ –∑–∞—Å—Ç–æ—Å—É–≤–∞—Ç–∏ –¥–æ –¥–∞–Ω–∏—Ö.

```
weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT
```

- `EfficientNet_B0_Weights` --- —Ü–µ –∫–ª–∞—Å, —è–∫–∏–π –º—ñ—Å—Ç–∏—Ç—å —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –≤–∞–≥–∏ –º–æ–¥–µ–ª—ñ EfficientNet_B0, –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞–≤—á–µ–Ω—É –Ω–∞ –Ω–∞–±–æ—Ä—ñ –¥–∞–Ω–∏—Ö ImageNet.
- `DEFAULT` --- —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π –Ω–∞–±—ñ—Ä –≤–∞–≥—ñ–≤, —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∏–π –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∑ —Ü—ñ—î—é –º–æ–¥–µ–ª–ª—é.

::: {.callout-note icon="false"}
–ó–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –æ–±—Ä–∞–Ω–æ—ó –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –º–æ–¥–µ–ª—ñ, –≤–∏ —Ç–∞–∫–æ–∂ –º–æ–∂–µ—Ç–µ –ø–æ–±–∞—á–∏—Ç–∏ —ñ–Ω—à—ñ –æ–ø—Ü—ñ—ó, —Ç–∞–∫—ñ —è–∫ `IMAGENET_V1` —Ç–∞ `IMAGENET_V2`, –¥–µ, —è–∫ –ø—Ä–∞–≤–∏–ª–æ, —á–∏–º –≤–∏—â–∏–π –Ω–æ–º–µ—Ä –≤–µ—Ä—Å—ñ—ó, —Ç–∏–º –∫—Ä–∞—â–µ. –î–æ–∫–ª–∞–¥–Ω—ñ—à—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –¥–∏–≤—ñ—Ç—å—Å—è –≤ [–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—ó](https://pytorch.org/vision/main/models.html) `torchvision.models`.
:::

## –ì–æ—Ç—É—î–º–æ –¥–∞–Ω—ñ {.smaller}

```{python}
weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT
weights
```

–¢–µ–ø–µ—Ä –º–∏ –º–æ–∂–µ–º–æ –æ—Ç—Ä–∏–º–∞—Ç–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó, —è–∫—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–ª–∏—Å—è –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ EfficientNet_B0 –Ω–∞ ImageNet.

```{python}
# Get the transforms used to create our pretrained weights
auto_transforms = weights.transforms()
auto_transforms
```

## DataLoaders 

```{python}
train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,
                                                                               test_dir=test_dir,
                                                                               transform=auto_transforms,
                                                                               batch_size=32)

train_dataloader, test_dataloader, class_names
```

## –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –≤ PyTorch {.smaller}

- [ResNet](https://arxiv.org/abs/1512.03385): `torchvision.models.resnet18()`, `torchvision.models.resnet50()`...
- [EfficientNet](https://arxiv.org/abs/1905.11946): `torchvision.models.efficientnet_b0()`, `torchvision.models.efficientnet_b1()`...
- [VGG](https://arxiv.org/abs/1409.1556): `torchvision.models.vgg16()`
- [Vision Transformers](https://arxiv.org/abs/2010.11929) (ViT): `torchvision.models.vit_b_16()`, `torchvision.models.vit_b_32()`...
- [ConvNext](https://arxiv.org/abs/2201.03545): `torchvision.models.convnext_tiny()`, `torchvision.models.convnext_small()`...

## –Ø–∫—É –º–æ–¥–µ–ª—å –æ–±—Ä–∞—Ç–∏? {.smaller}

–ö–æ–º–ø—Ä–æ–º—ñ—Å –º—ñ–∂ –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—é, —à–≤–∏–¥–∫–æ–¥—ñ—î—é —Ç–∞ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—é:

- –ü–æ—á–Ω—ñ—Ç—å –∑ –ø—Ä–æ—Å—Ç–æ—ó –º–æ–¥–µ–ª—ñ, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, ResNet –∞–±–æ EfficientNet.
- –Ø–∫—â–æ —É –≤–∞—Å —î –æ–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω—ñ —Ä–µ—Å—É—Ä—Å–∏, —Å–ø—Ä–æ–±—É–π—Ç–µ –±—ñ–ª—å—à —Å–∫–ª–∞–¥–Ω—ñ –º–æ–¥–µ–ª—ñ, —Ç–∞–∫—ñ —è–∫ Vision Transformers –∞–±–æ ConvNext.
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ `torchinfo.summary()` –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ —ñ —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ –º–æ–¥–µ–ª–µ–π.

–ü—Ä–∏–∫–ª–∞–¥–∏:

+ [Nutrify](https://nutrify.app/) --- –¥–æ–¥–∞—Ç–æ–∫ –¥–ª—è —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è —ó–∂—ñ, —è–∫–∏–π –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î `efficientnet_b0`.
+ [Comma.ai](https://comma.ai/) --- –∫–æ–º–ø–∞–Ω—ñ—è, —è–∫–∞ —Ä–æ–∑—Ä–æ–±–ª—è—î –∞–≤—Ç–æ–Ω–æ–º–Ω—ñ –∞–≤—Ç–æ–º–æ–±—ñ–ª—ñ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î [`efficientnet_b2`](https://geohot.github.io/blog/jekyll/update/2021/10/29/an-architecture-for-life.html).

## –ì–æ—Ç—É—î–º–æ –º–æ–¥–µ–ª—å {.scrollable}

–í–∏–∫–æ—Ä–∏—Å—Ç–∞—î–º–æ [EfficientNet_B0](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.efficientnet_b0.html). –í–µ—Ä—Å—ñ—è —Ü—ñ—î—ó –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞–≤—á–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ –¥–ª—è PyTorch –∑–¥–∞—Ç–Ω–∞ –¥–æ—Å—è–≥—Ç–∏ —Ç–æ—á–Ω–æ—Å—Ç—ñ ~77,7% —É 1000 –∫–ª–∞—Å–∞—Ö ImageNet.

```{dot}
//| echo: false
digraph G {
    rankdir="TD";
    node [shape=box, style=solid];

    input [label="Input\n224x224x3"];
    conv1 [label="Conv3x3\n112x112x32"];
    mbconv1_1 [label="MBConv1, 3x3\n112x112x16"];
    mbconv6_1 [label="MBConv6, 3x3\n56x56x24", color="brown", fontcolor="brown"];
    mbconv6_2 [label="MBConv6, 3x3\n56x56x24", color="brown", fontcolor="brown"];
    mbconv6_3 [label="MBConv6, 5x5\n28x28x40", color="blue", fontcolor="blue"];
    mbconv6_4 [label="MBConv6, 5x5\n28x28x40", color="blue", fontcolor="blue"];
    mbconv6_5 [label="MBConv6, 3x3\n28x28x80", color="brown", fontcolor="brown"];
    mbconv6_6 [label="MBConv6, 3x3\n28x28x80", color="brown", fontcolor="brown"];
    mbconv6_7 [label="MBConv6, 3x3\n28x28x80", color="brown", fontcolor="brown"];
    mbconv6_8 [label="MBConv6, 5x5\n14x14x112", color="blue", fontcolor="blue"];
    mbconv6_9 [label="MBConv6, 5x5\n14x14x112", color="blue", fontcolor="blue"];
    mbconv6_10 [label="MBConv6, 5x5\n14x14x112", color="blue", fontcolor="blue"];
    mbconv6_11 [label="MBConv6, 5x5\n7x7x192", color="blue", fontcolor="blue"];
    mbconv6_12 [label="MBConv6, 5x5\n7x7x192", color="blue", fontcolor="blue"];
    mbconv6_13 [label="MBConv6, 5x5\n7x7x192", color="blue", fontcolor="blue"];
    mbconv6_14 [label="MBConv6, 3x3\n7x7x320", color="brown", fontcolor="brown"];

    input -> conv1;
    conv1 -> mbconv1_1;
    mbconv1_1 -> mbconv6_1;
    mbconv6_1 -> mbconv6_2;
    mbconv6_2 -> mbconv6_3;
    mbconv6_3 -> mbconv6_4;
    mbconv6_4 -> mbconv6_5;
    mbconv6_5 -> mbconv6_6;
    mbconv6_6 -> mbconv6_7;
    mbconv6_7 -> mbconv6_8;
    mbconv6_8 -> mbconv6_9;
    mbconv6_9 -> mbconv6_10;
    mbconv6_10 -> mbconv6_11;
    mbconv6_11 -> mbconv6_12;
    mbconv6_12 -> mbconv6_13;
    mbconv6_13 -> mbconv6_14;
}
```

## –ì–æ—Ç—É—î–º–æ –º–æ–¥–µ–ª—å

```{python}
weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT
model = torchvision.models.efficientnet_b0(weights=weights).to(device)
model
```

---

–ú–æ–¥–µ–ª—å —Å–∫–ª–∞–¥–∞—î—Ç—å—Å—è –∑ —Ç—Ä—å–æ—Ö –æ—Å–Ω–æ–≤–Ω–∏—Ö —á–∞—Å—Ç–∏–Ω:

- `features` --- —Ü–µ –æ—Å–Ω–æ–≤–Ω–∞ —á–∞—Å—Ç–∏–Ω–∞ –º–æ–¥–µ–ª—ñ, —è–∫–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –∑–∞ –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –æ–∑–Ω–∞–∫ –∑ –≤—Ö—ñ–¥–Ω–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å. –í–æ–Ω–∞ —Å–∫–ª–∞–¥–∞—î—Ç—å—Å—è –∑ –∫—ñ–ª—å–∫–æ—Ö —à–∞—Ä—ñ–≤ –∑–≥–æ—Ä—Ç–∫–∏ (convolutional layers), —è–∫—ñ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ –æ–±—Ä–æ–±–ª—è—é—Ç—å –≤—Ö—ñ–¥–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, –≤–∏—Ç—è–≥—É—é—á–∏ –≤—Å–µ –±—ñ–ª—å—à —Å–∫–ª–∞–¥–Ω—ñ —Ç–∞ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ñ –æ–∑–Ω–∞–∫–∏.
- `avgpool` --- —Ü–µ —à–∞—Ä –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Å–µ—Ä–µ–¥–Ω—å–æ–≥–æ –ø—É–ª—ñ–Ω–≥—É (global average pooling layer), —è–∫–∏–π –∑–º–µ–Ω—à—É—î —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –≤–∏—Ö—ñ–¥–Ω–∏—Ö –æ–∑–Ω–∞–∫, –æ—Ç—Ä–∏–º–∞–Ω–∏—Ö –∑ `features`, —à–ª—è—Ö–æ–º —É—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è –∑–Ω–∞—á–µ–Ω—å –ø–æ –∫–æ–∂–Ω–æ–º—É –∫–∞–Ω–∞–ª—É.
- `classifier` --- —Ü–µ –ø–æ–≤–Ω–æ–∑–≤'—è–∑–Ω–∏–π —à–∞—Ä (fully connected layer), —è–∫–∏–π –ø—Ä–∏–π–º–∞—î –≤—Ö—ñ–¥–Ω—ñ –æ–∑–Ω–∞–∫–∏ –ø—ñ—Å–ª—è `avgpool` —ñ –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î —ó—Ö —É –≤–∏—Ö—ñ–¥–Ω—ñ –∫–ª–∞—Å–∏. –£ –≤–∏–ø–∞–¥–∫—É EfficientNet_B0, —Ü–µ–π —à–∞—Ä –º–∞—î 1000 –≤–∏—Ö—ñ–¥–Ω–∏—Ö –Ω–µ–π—Ä–æ–Ω—ñ–≤, —â–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î 1000 –∫–ª–∞—Å–∞–º ImageNet.

## –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –º–æ–¥–µ–ª—å {.smaller}

–í–∏–∫–æ—Ä–∏—Å—Ç–∞—î–º–æ `torchinfo.summary()` –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–µ—Ç–∞–ª—å–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ –º–æ–¥–µ–ª—å:

- `model`: –º–æ–¥–µ–ª—å, —è–∫—É –º–∏ —Ö–æ—á–µ–º–æ –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏.
- `input_size`: —Ä–æ–∑–º—ñ—Ä –≤—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö, —è–∫—ñ –º–æ–¥–µ–ª—å –ø—Ä–∏–π–º–∞—î (–≤–∫–ª—é—á–∞—é—á–∏ —Ä–æ–∑–º—ñ—Ä –±–∞—Ç—á—É).
- `col_names`: –Ω–∞–∑–≤–∏ –∫–æ–ª–æ–Ω–æ–∫, —è–∫—ñ –º–∏ —Ö–æ—á–µ–º–æ –±–∞—á–∏—Ç–∏ –≤ –∑–≤—ñ—Ç—ñ.
- `col_width`: —à–∏—Ä–∏–Ω–∞ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∫—Ä–∞—â–æ—ó —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—ñ.
- `row_settings`: –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è –ø–µ–≤–Ω–∏—Ö —Ä—è–¥–∫—ñ–≤, —Ç–∞–∫–∏—Ö —è–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ —Ç–∞ –ø–∞–º'—è—Ç—å.

```{python}
#| output-location: slide
summary(model=model, 
        input_size=(32, 3, 224, 224),
        col_names=["input_size", "output_size", "num_params", "trainable"],
        col_width=20,
        row_settings=["var_names"]
)
```

## –ó–∞–º–æ—Ä–æ–∂—É—î–º–æ –≤–∞–≥–∏ {.smaller}

- –ó–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è –≤–∞–≥—ñ–≤ (freezing weights) –æ–∑–Ω–∞—á–∞—î, —â–æ –º–∏ –Ω–µ –±—É–¥–µ–º–æ –æ–Ω–æ–≤–ª—é–≤–∞—Ç–∏ —Ü—ñ –≤–∞–≥–∏ –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –Ω–∞ –Ω–æ–≤–æ–º—É –Ω–∞–±–æ—Ä—ñ –¥–∞–Ω–∏—Ö.
- –¶–µ –∫–æ—Ä–∏—Å–Ω–æ, –∫–æ–ª–∏ –º–∏ —Ö–æ—á–µ–º–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞–≤—á–µ–Ω—É –º–æ–¥–µ–ª—å —è–∫ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏–π –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –æ–∑–Ω–∞–∫, –Ω–µ –∑–º—ñ–Ω—é—é—á–∏ —ó—ó –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏.

```{python}
for param in model.features.parameters():
    param.requires_grad = False
```

## –ó–º—ñ–Ω—é—î–º–æ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä {.smaller}

- `out_features` --- –∫—ñ–ª—å–∫—ñ—Å—Ç—å –≤–∏—Ö—ñ–¥–Ω–∏—Ö –Ω–µ–π—Ä–æ–Ω—ñ–≤ —É –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ, —è–∫–µ –º–∏ —Ö–æ—á–µ–º–æ –∑–º—ñ–Ω–∏—Ç–∏.

```{python}
torch.manual_seed(73)
torch.cuda.manual_seed(73)

output_shape = len(class_names)

model.classifier = torch.nn.Sequential(
    torch.nn.Dropout(p=0.2, inplace=True), 
    torch.nn.Linear(in_features=1280, 
                    out_features=output_shape,
                    bias=True)).to(device)
```

---

```{python}
#| echo: false
summary(model, 
        input_size=(32, 3, 224, 224),
        verbose=0,
        col_names=["input_size", "output_size", "num_params", "trainable"],
        col_width=20,
        row_settings=["var_names"]
)
```

## –ì–æ—Ç—É—î–º–æ —Ñ—É–Ω–∫—Ü—ñ—é –≤—Ç—Ä–∞—Ç —Ç–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä

```{python}
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
```

## –ù–∞–≤—á–∞—î–º–æ –º–æ–¥–µ–ª—å {.smaller}

- `train()` --- —Ñ—É–Ω–∫—Ü—ñ—è –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –≤ —Å–∫—Ä–∏–ø—Ç—ñ `engine.py`

```{python}
#| label: train
#| output-location: slide
torch.manual_seed(73)
torch.cuda.manual_seed(73)

from timeit import default_timer as timer 
start_time = timer()

results = engine.train(model=model,
                       train_dataloader=train_dataloader,
                       test_dataloader=test_dataloader,
                       optimizer=optimizer,
                       loss_fn=loss_fn,
                       epochs=5,
                       device=device)

end_time = timer()
print(f"[INFO] Total training time: {end_time-start_time:.3f} seconds")
```

## –û—Ü—ñ–Ω—é—î–º–æ –º–æ–¥–µ–ª—å {.smaller}

```{python}
#| output-location: slide
try:
    from helper_functions import plot_loss_curves
except:
    print("[INFO] Couldn't find helper_functions.py, downloading...")
    with open("helper_functions.py", "wb") as f:
        import requests
        request = requests.get("https://raw.githubusercontent.com/Aranaur/aranaur.rbind.io/refs/heads/main/lectures/cv/slides/2025/helper_functions.py")
        f.write(request.content)
    from helper_functions import plot_loss_curves

plot_loss_curves(results)
```

## –ü—Ä–æ–≥–Ω–æ–∑—É—î–º–æ –Ω–∞ –Ω–æ–≤–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è—Ö

- –û–¥–Ω–∞–∫–æ–≤–∞ —Ñ–æ—Ä–º–∞.
- –û–¥–Ω–∞–∫–∏–π —Ç–∏–ø –¥–∞–Ω–∏—Ö.
- –û–¥–Ω–∞–∫–æ–≤–∏–π –ø—Ä–∏—Å—Ç—Ä—ñ–π.
- –û–¥–Ω–∞–∫–æ–≤—ñ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó.

## `pred_and_plot_image()` {.smaller}

1. –í—ñ–∑—å–º—ñ—Ç—å –Ω–∞–≤—á–µ–Ω—É **–º–æ–¥–µ–ª—å**, —Å–ø–∏—Å–æ–∫ **—ñ–º–µ–Ω** –∫–ª–∞—Å—ñ–≤, **—à–ª—è—Ö** –¥–æ —Ü—ñ–ª—å–æ–≤–æ–≥–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, **—Ä–æ–∑–º—ñ—Ä** –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, **—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—é** —Ç–∞ —Ü—ñ–ª—å–æ–≤–∏–π **–ø—Ä–∏—Å—Ç—Ä—ñ–π**.
2. –í—ñ–¥–∫—Ä–∏–π—Ç–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é `PIL.Image.open()`.
3. –°—Ç–≤–æ—Ä—ñ—Ç—å **—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—é** –¥–ª—è –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è.
4. –ü–µ—Ä–µ–∫–æ–Ω–∞–π—Ç–µ—Å—è, —â–æ –º–æ–¥–µ–ª—å –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è **–Ω–∞ —Ü—ñ–ª—å–æ–≤–æ–º—É –ø—Ä–∏—Å—Ç—Ä–æ—ó**.
5. –£–≤—ñ–º–∫–Ω—ñ—Ç—å —Ä–µ–∂–∏–º **–æ—Ü—ñ–Ω–∫–∏** –º–æ–¥–µ–ª—ñ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é `model.eval()`.
6. **–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º—É–π—Ç–µ** —Ü—ñ–ª—å–æ–≤–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó, –≤–∏–∫–æ–Ω–∞–Ω–æ—ó –Ω–∞ –∫—Ä–æ—Ü—ñ 3, —ñ –¥–æ–¥–∞–π—Ç–µ –¥–æ–¥–∞—Ç–∫–æ–≤–∏–π –≤–∏–º—ñ—Ä –±–∞—Ç—á—É –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é `torch.unsqueeze(dim=0)`.
7. –ó—Ä–æ–±—ñ—Ç—å **–ø—Ä–æ–≥–Ω–æ–∑** —â–æ–¥–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, –ø–µ—Ä–µ–¥–∞–≤—à–∏ –π–æ–≥–æ –¥–æ –º–æ–¥–µ–ª—ñ, –ø–µ—Ä–µ–∫–æ–Ω–∞–≤—à–∏—Å—å, —â–æ –≤–æ–Ω–∞ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –Ω–∞ —Ü—ñ–ª—å–æ–≤–æ–º—É –ø—Ä–∏—Å—Ç—Ä–æ—ó.
8. –ü–µ—Ä–µ—Ç–≤–æ—Ä—ñ—Ç—å –≤–∏—Ö—ñ–¥–Ω—ñ **–ª–æ–≥—ñ—Ç–∏** –º–æ–¥–µ–ª—ñ **–Ω–∞ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ** –ø—Ä–æ–≥–Ω–æ–∑—É –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é `torch.softmax()`.
9. –ü–µ—Ä–µ—Ç–≤–æ—Ä—ñ—Ç—å **–π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ** –ø—Ä–æ–≥–Ω–æ–∑—É –º–æ–¥–µ–ª—ñ **–Ω–∞ –º—ñ—Ç–∫–∏** –ø—Ä–æ–≥–Ω–æ–∑—É –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é `torch.argmax()`.
10. –ü–æ–±—É–¥—É–π—Ç–µ **–≥—Ä–∞—Ñ—ñ–∫** –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é `matplotlib`.

---

```{python}
from typing import List, Tuple

from PIL import Image

# 1. Take in a trained model, class names, image path, image size, a transform and target device
def pred_and_plot_image(model: torch.nn.Module,
                        image_path: str, 
                        class_names: List[str],
                        image_size: Tuple[int, int] = (224, 224),
                        transform: torchvision.transforms = None,
                        device: torch.device=device):
    
    
    # 2. Open image
    img = Image.open(image_path)

    # 3. Create transformation for image (if one doesn't exist)
    if transform is not None:
        image_transform = transform
    else:
        image_transform = transforms.Compose([
            transforms.Resize(image_size),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225]),
        ])

    ### Predict on image ### 

    # 4. Make sure the model is on the target device
    model.to(device)

    # 5. Turn on model evaluation mode and inference mode
    model.eval()
    with torch.inference_mode():
      # 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])
      transformed_image = image_transform(img).unsqueeze(dim=0)

      # 7. Make a prediction on image with an extra dimension and send it to the target device
      target_image_pred = model(transformed_image.to(device))

    # 8. Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)
    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)

    # 9. Convert prediction probabilities -> prediction labels
    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)

    # 10. Plot image with predicted label and probability 
    plt.figure()
    plt.imshow(img)
    plt.title(f"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}")
    plt.axis(False);
```

## –ü—Ä–æ–≥–Ω–æ–∑—É—î–º–æ –Ω–∞ –Ω–æ–≤–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è—Ö {.tiny}

```{python}
#| output-location: column
import random
num_images_to_plot = 2
test_image_path_list = list(Path(test_dir).glob("*/*.jpg"))
test_image_path_sample = random.sample(population=test_image_path_list,
                                       k=num_images_to_plot)

for image_path in test_image_path_sample:
    pred_and_plot_image(model=model, 
                        image_path=image_path,
                        class_names=class_names,
                        # transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights
                        image_size=(224, 224))
```

## –ü—Ä–æ–≥–Ω–æ–∑—É—î–º–æ –Ω–∞ –≤–ª–∞—Å–Ω–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è—Ö

```{python}
#| output-location: slide
import requests

data_path = Path("data/")
custom_image_path = data_path / "yar-pizza.jpeg"

# Download the image if it doesn't already exist
if not custom_image_path.is_file():
    with open(custom_image_path, "wb") as f:
        # When downloading from GitHub, need to use the "raw" file link
        request = requests.get("https://raw.githubusercontent.com/Aranaur/aranaur.rbind.io/refs/heads/main/lectures/cv/slides/2025/img/yar-pizza.jpg")
        print(f"Downloading {custom_image_path}...")
        f.write(request.content)
else:
    print(f"{custom_image_path} already exists, skipping download.")

pred_and_plot_image(model=model,
                    image_path=custom_image_path,
                    class_names=class_names)
```

# –î—è–∫—É—é –∑–∞ —É–≤–∞–≥—É! {.unnumbered .unlisted background-iframe=".06_files/libs/colored-particles/index.html"}

<br> <br>

{{< iconify solar book-bold >}} [–ú–∞—Ç–µ—Ä—ñ–∞–ª–∏ –∫—É—Ä—Å—É](https://aranaur.rbind.io/lectures/)

{{< iconify mdi envelope >}} ihor.miroshnychenko\@knu.ua

{{< iconify ic baseline-telegram >}} [Data Mirosh](https://t.me/araprof)

{{< iconify mdi linkedin >}} [\@ihormiroshnychenko](https://www.linkedin.com/in/ihormiroshnychenko/)

{{< iconify mdi github >}} [\@aranaur](https://github.com/Aranaur)

{{< iconify ion home >}} [aranaur.rbind.io](https://aranaur.rbind.io)
