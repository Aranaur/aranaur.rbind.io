---
title: "–í–ª–∞—Å–Ω—ñ –Ω–∞–±–æ—Ä–∏ –¥–∞–Ω–∏—Ö üî•PyTorch"
subtitle: "–¢–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó –∫–æ–º–ø'—é—Ç–µ—Ä–Ω–æ–≥–æ –∑–æ—Ä—É"
author: "–Ü–≥–æ—Ä –ú—ñ—Ä–æ—à–Ω–∏—á–µ–Ω–∫–æ"
institute: –ö–ù–£ —ñ–º–µ–Ω—ñ –¢–∞—Ä–∞—Å–∞ –®–µ–≤—á–µ–Ω–∫–∞, –§–Ü–¢
from: markdown+emoji
title-slide-attributes:
    data-background-iframe: .04_files/libs/colored-particles/index.html
language: _language-ua.yml
footer: <a href="https://aranaur.rbind.io/lectures">üîó–¢–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó –∫–æ–º–ø'—é—Ç–µ—Ä–Ω–æ–≥–æ –∑–æ—Ä—É</a>
format:
  revealjs: 
    transition: fade
    chalkboard: true
    logo: fit.png
    slide-number: true
    # toc: true
    # toc-depth: 1
    mouse-wheel: true
    width: 1350  
    height: 759.375
    highlight-style: github
    fig-width: 9
    fig-height: 5
    fig-format: svg
    fig-align: center
    theme: [default, custom.scss]
#   gfm:
#     mermaid-format: png
preload-iframes: true
jupyter: python3
execute: 
  echo: true
  warning: false
editor_options: 
  chunk_output_type: console
---

```{python}
#| include: false

import torch
from torch import nn
import matplotlib.pyplot as plt
device = "cuda" if torch.cuda.is_available() else "cpu"


# Define colors
red_pink   = "#e64173"
turquoise  = "#20B2AA"
orange     = "#FFA500"
red        = "#fb6107"
blue       = "#181485"
navy       = "#150E37FF"
green      = "#8bb174"
yellow     = "#D8BD44"
purple     = "#6A5ACD"
slate      = "#314f4f"
```

## –ó–±—ñ—Ä –¥–∞–Ω–∏—Ö

::: {.columns}
::: {.column}
- –í—ñ–∑—å–º–µ–º–æ –ø—ñ–¥–≤–∏–±—ñ—Ä–∫—É –∑ [Food-101](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/)
- 101 –∫–ª–∞—Å —ó–∂—ñ
- 101000 –∑–æ–±—Ä–∞–∂–µ–Ω—å
- 75000 –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
- 25000 –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
:::
::: {.column}
- `torchvision.datasets.Food101`
- [`pizza_steak_sushi.zip`](https://drive.google.com/file/d/1Jf8jhskDmN3DxP0yk9PET5Kr44OnwUgU/view?usp=sharing)
:::
:::

## –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ google drive {.tiny}

```{python}
#| output-location: column
import gdown
import tempfile
import zipfile
import os

file_url = 'https://drive.google.com/file/d/1Jf8jhskDmN3DxP0yk9PET5Kr44OnwUgU/view?usp=sharing'
file_id = file_url.split('/')[-2]

with tempfile.TemporaryDirectory() as tmpdir:
    print(f'–°—Ç–≤–æ—Ä–µ–Ω–æ —Ç–∏–º—á–∞—Å–æ–≤—É –ø–∞–ø–∫—É: {tmpdir}')

    zip_path = os.path.join(tmpdir, 'pizza_steak_sushi.zip')

    print('–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ñ–∞–π–ª—É...')
    gdown.download(id=file_id, output=zip_path, quiet=False)
    print('–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ.')

    extract_path = 'data/pizza_steak_sushi'
    os.makedirs(extract_path, exist_ok=True)

    print(f'–†–æ–∑–∞—Ä—Ö—ñ–≤–∞—Ü—ñ—è —Ñ–∞–π–ª—É –≤ {extract_path}...')
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)
    print('–†–æ–∑–∞—Ä—Ö—ñ–≤–∞—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.')

print('–¢–∏–º—á–∞—Å–æ–≤—É –ø–∞–ø–∫—É —Ç–∞ —ó—ó –≤–º—ñ—Å—Ç –≤–∏–¥–∞–ª–µ–Ω–æ.')
```

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–∏—Ö {.smaller}

```{python}
#| code-fold: true

from pathlib import Path

def list_directory_structure_limited(path):
    """
    –ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤–∏–∫–ª–∏–∫—É. –í–∏–≤–æ–¥–∏—Ç—å –Ω–∞–∑–≤—É –∫–æ—Ä–µ–Ω–µ–≤–æ—ó –ø–∞–ø–∫–∏.
    """
    if not path.is_dir():
        print(f'–ü–æ–º–∏–ª–∫–∞: –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è {path} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞.')
        return

    print(f'–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó {path} (–ø–æ 2 —Ñ–∞–π–ª–∏ –≤ –ø–∞–ø—Ü—ñ):')
    print(f'{path.name}/')
    _walk_directory_limited(path, prefix="")

def _walk_directory_limited(path, prefix=""):
    """
    –†–µ–∫—É—Ä—Å–∏–≤–Ω–∞ –¥–æ–ø–æ–º—ñ–∂–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ–±—Ö–æ–¥—É —Ç–∞ –≤–∏–≤–µ–¥–µ–Ω–Ω—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏.
    """
    try:
        all_items = list(path.iterdir())
    except PermissionError:
        print(f"{prefix}‚îî‚îÄ‚îÄ [–ù–µ–º–∞—î –¥–æ—Å—Ç—É–ø—É]")
        return

    dirs = sorted([p for p in all_items if p.is_dir()])
    files = sorted([p for p in all_items if p.is_file()])

    files_to_show = files[:2]
    hidden_files_count = len(files) - len(files_to_show)

    items_to_display = dirs + files_to_show

    for i, item_path in enumerate(items_to_display):
        is_last_item = (i == len(items_to_display) - 1) and (hidden_files_count == 0)
        pointer = '‚îî‚îÄ‚îÄ ' if is_last_item else '‚îú‚îÄ‚îÄ '

        print(f'{prefix}{pointer}{item_path.name}')

        if item_path.is_dir():
            extension = '    ' if is_last_item else '‚îÇ   '
            _walk_directory_limited(item_path, prefix=prefix + extension)

    if hidden_files_count > 0:
        print(f'{prefix}‚îî‚îÄ‚îÄ ... —Ç–∞ —â–µ {hidden_files_count} —Ñ–∞–π–ª—ñ–≤')


directory_to_scan = Path('data/pizza_steak_sushi')

list_directory_structure_limited(directory_to_scan)
```

## –ü–µ—Ä–µ–≥–ª—è–¥ –∑–æ–±—Ä–∞–∂–µ–Ω—å 1/2 {.smaller}

```{python}
#| output-location: column

import random
from PIL import Image

image_path = Path('data/pizza_steak_sushi')

random.seed(73)

image_path_list = list(image_path.glob("*/*/*.jpg"))

random_image_path = random.choice(image_path_list)

image_class = random_image_path.parent.stem

img = Image.open(random_image_path)

print(f"Random image path: {random_image_path}")
print(f"Image class: {image_class}")
print(f"Image height: {img.height}") 
print(f"Image width: {img.width}")
img
```

## –ü–µ—Ä–µ–≥–ª—è–¥ –∑–æ–±—Ä–∞–∂–µ–Ω—å 2/2  {.smaller}

```{python}
#| output-location: column
import numpy as np
import matplotlib.pyplot as plt

img_as_array = np.asarray(img)

plt.figure(figsize=(10, 7))
plt.imshow(img_as_array)
plt.title(f"Image class: {image_class} | Image shape: {img_as_array.shape} $\\rightarrow$ [height, width, color_channels]")
plt.axis(False);
```

## –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö {.tiny}

1. –†–∏—Å—É–Ω–∫–∏ $\rightarrow$ —Ç–µ–Ω–∑–æ—Ä–∏
2. –ü–µ—Ä–µ—Ç–≤–æ—Ä—ñ—Ç—å –≤ `torch.utils.data.Dataset`, –∞ –ø–æ—Ç—ñ–º –≤ `torch.utils.data.DataLoader`.

```{python}
import torch
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
```

| **–ó–∞–¥–∞—á–∞** | **–ì–æ—Ç–æ–≤—ñ –Ω–∞–±–æ—Ä–∏ –¥–∞–Ω–∏—Ö —Ç–∞ —Ñ—É–Ω–∫—Ü—ñ—ó** |
| ----- | ----- |
| **–ó—ñ—Ä** | [`torchvision.datasets`](https://pytorch.org/vision/stable/datasets.html) |
| **–ê—É–¥—ñ–æ** | [`torchaudio.datasets`](https://pytorch.org/audio/stable/datasets.html) |
| **–¢–µ–∫—Å—Ç** | [`torchtext.datasets`](https://pytorch.org/text/stable/datasets.html) |
| **–°–∏—Å—Ç–µ–º–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π** | [`torchrec.datasets`](https://pytorch.org/torchrec/torchrec.datasets.html) |

## –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å –≤ —Ç–µ–Ω–∑–æ—Ä–∏ {.tiny}

::: {.columns}
::: {.column}
- `torchvision.transforms` - –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ –¥–ª—è –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å
- `transforms.Resize()` - –∑–º—ñ–Ω—é—î —Ä–æ–∑–º—ñ—Ä –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
- `transforms.RandomHorizontalFlip()` - –≤–∏–ø–∞–¥–∫–æ–≤–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–µ –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è
- `transforms.ToTensor()` - –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –≤ —Ç–µ–Ω–∑–æ—Ä
- `torchvision.transforms.Compose()` - –æ–±'—î–¥–Ω—É—î –∫—ñ–ª—å–∫–∞ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω—å
:::
::: {.column}

```{python}
data_transform = transforms.Compose([
    transforms.Resize(size=(64, 64)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ToTensor()
])
```
:::
:::

## –§—É–Ω–∫—Ü—ñ—è –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è {.tiny}

```{python}
#| output-location: column
def plot_transformed_images(image_paths, transform, n=3, seed=73):
    random.seed(seed)
    random_image_paths = random.sample(image_paths, k=n)
    for image_path in random_image_paths:
        with Image.open(image_path) as f:
            fig, ax = plt.subplots(1, 2)
            ax[0].imshow(f) 
            ax[0].set_title(f"Original \nSize: {f.size}")
            ax[0].axis("off")

            transformed_image = transform(f).permute(1, 2, 0) # <1>
            ax[1].imshow(transformed_image) 
            ax[1].set_title(f"Transformed \nSize: {transformed_image.shape}")
            ax[1].axis("off")

            fig.suptitle(f"Class: {image_path.parent.stem}", fontsize=16)

plot_transformed_images(image_path_list, 
                        transform=data_transform, 
                        n=1)
```

1. `permute()` –∑–º—ñ–Ω–∏—Ç—å —Ñ–æ—Ä–º—É –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ –¥–æ matplotlib (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º PyTorch –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î [C, H, W], –∞ Matplotlib ‚Äî [H, W, C])

## –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ä–∏—Å—É–Ω–∫—ñ–≤ –∑ `ImageFolder` {.tiny}

- `torchvision.datasets.ImageFolder` - –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑ –ø–∞–ø–æ–∫

```{python}
#| output-location: column
from torchvision import datasets

train_dir = image_path / "train"
test_dir = image_path / "test"

train_data = datasets.ImageFolder(root=train_dir,
                                  transform=data_transform,
                                  target_transform=None) 

test_data = datasets.ImageFolder(root=test_dir, 
                                 transform=data_transform)

print(f"Train data:\n{train_data}\nTest data:\n{test_data}")
```

## –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ 1/2

```{python}
class_names = train_data.classes
class_dict = train_data.class_to_idx

print(f"Length of training data: {len(train_data)}")
print(f"Length of testing data: {len(test_data)}")
print(f"Classes: {class_names}")
print(f"Code of classes: {class_dict}")
```

## –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ 2/2 {.smaller}

```{python}
#| output-location: column
img, label = train_data[0][0], train_data[0][1]
print(f"Image tensor:\n{img}")
print(f"Image shape: {img.shape}")
print(f"Image datatype: {img.dtype}")
print(f"Image label: {label}")
print(f"Label datatype: {type(label)}")
```

## –ü–µ—Ä–µ–≥–ª—è–¥ —Ç–µ–Ω–∑–æ—Ä—ñ–≤ –∑–æ–±—Ä–∞–∂–µ–Ω—å

```{python}
img_permute = img.permute(1, 2, 0)

print(f"Original shape: {img.shape} -> [color_channels, height, width]")
print(f"Image permute shape: {img_permute.shape} -> [height, width, color_channels]")

plt.figure(figsize=(10, 7))
plt.imshow(img.permute(1, 2, 0))
plt.axis("off")
plt.title(class_names[label], fontsize=14);
```

## –ü–µ—Ä–µ–¥–∞—î–º–æ –¥–∞–Ω—ñ –≤ DataLoader {.smaller}

- `torch.utils.data.DataLoader` - —Å—Ç–≤–æ—Ä—é—î —ñ—Ç–µ—Ä–∞—Ç–æ—Ä –Ω–∞–¥ –¥–∞—Ç–∞—Å–µ—Ç–æ–º
- `batch_size` - –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑—Ä–∞–∑–∫—ñ–≤ –≤ –æ–¥–Ω–æ–º—É –ø–∞–∫–µ—Ç—ñ
- `num_workers` - –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ—Ç–æ–∫—ñ–≤ –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö (`os.cpu_count()-1`)

```{python}
from torch.utils.data import DataLoader
print(f"Number of CPU cores: {os.cpu_count()}")
train_dataloader = DataLoader(dataset=train_data, 
                              batch_size=1,
                              num_workers=os.cpu_count()-1,
                              shuffle=True)

test_dataloader = DataLoader(dataset=test_data, 
                             batch_size=1, 
                             num_workers=os.cpu_count()-1, 
                             shuffle=False)

train_dataloader, test_dataloader
```

## –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ñ–æ—Ä–º–∏ –¥–∞–Ω–∏—Ö

```{python}
img, label = next(iter(train_dataloader))

print(f"Image shape: {img.shape} -> [batch_size, color_channels, height, width]")
print(f"Label shape: {label.shape}")
```

## –ê—É–≥–º–µ–Ω—Ç–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö

- [–Ü–ª—é—Å—Ç—Ä–∞—Ü—ñ—ó](https://docs.pytorch.org/vision/main/auto_examples/transforms/plot_transforms_illustrations.html) –∞—É–≥–º–µ–Ω—Ç–∞—Ü—ñ—ó.
- `transforms.TrivialAugmentWide()` - –≤–∏–ø–∞–¥–∫–æ–≤–µ –∞—É–≥–º–µ–Ω—Ç—É–≤–∞–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å.
- `transforms.Compose()` - –æ–±'—î–¥–Ω—É—î –∫—ñ–ª—å–∫–∞ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω—å.

![](https://pytorch.org/wp-content/uploads/2024/11/Cumulative20Accuracy20Improvements20for20ResNet50.png)

## –°–ø—Ä–æ–±—É—î–º–æ –∞—É–≥–º–µ–Ω—Ç–∞—Ü—ñ—é

- `num_magnitude_bins` - –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—ñ–≤–Ω—ñ–≤ —ñ–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—ñ –∞—É–≥–º–µ–Ω—Ç–∞—Ü—ñ—ó

```{python}
from torchvision import transforms

train_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.TrivialAugmentWide(num_magnitude_bins=31),
    transforms.ToTensor()
])

test_transforms = transforms.Compose([
    transforms.Resize((224, 224)), 
    transforms.ToTensor()
])
```

::: {.callout-note icon="false" appearance="simple"}
–ó–∞–∑–≤–∏—á–∞–π –∞—É–≥–º–µ–Ω—Ç–∞—Ü—ñ—è –Ω–µ –∑–∞—Å—Ç–æ—Å–æ–≤—É—î—Ç—å—Å—è –¥–æ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö.
:::

## –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∞—É–≥–º–µ–Ω—Ç–∞—Ü—ñ—ó

```{python}
image_path_list = list(image_path.glob("*/*/*.jpg"))

plot_transformed_images(
    image_paths=image_path_list,
    transform=train_transforms,
    n=3,
    seed=None
)
```

## –ú–æ–¥–µ–ª—å 0: TinyVGG –±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü—ñ—ó

- –ó–∞–≤–∞–Ω—Ç–∞–∂–∏–º–æ –¥–∞–Ω—ñ, —Å–ø–æ—á–∞—Ç–∫—É –ø–µ—Ä–µ—Ç–≤–æ—Ä–∏–≤—à–∏ –∫–æ–∂–Ω—É –∑ –Ω–∞—à–∏—Ö –ø–∞–ø–æ–∫ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –Ω–∞ –Ω–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é `torchvision.datasets.ImageFolder()`
- –ü–æ—Ç—ñ–º –ø–µ—Ä–µ—Ç–≤–æ—Ä–∏–º–æ —ó—Ö –Ω–∞ `DataLoader` –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é `torch.utils.data.DataLoader()`.
- –ú–∏ –≤—Å—Ç–∞–Ω–æ–≤–∏–º–æ `batch_size=32` —Ç–∞ `num_workers` –Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–æ—Ü–µ—Å–æ—Ä—ñ–≤ –Ω–∞ –Ω–∞—à—ñ–π –º–∞—à–∏–Ω—ñ.

```{python}
simple_transform = transforms.Compose([ 
    transforms.Resize((64, 64)),
    transforms.ToTensor(),
])
```

---

```{python}
#| output-location: slide
from torchvision import datasets
train_data_simple = datasets.ImageFolder(root=train_dir, transform=simple_transform)
test_data_simple = datasets.ImageFolder(root=test_dir, transform=simple_transform)

import os
from torch.utils.data import DataLoader

BATCH_SIZE = 32
NUM_WORKERS = os.cpu_count()
print(f"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.")

train_dataloader_simple = DataLoader(train_data_simple, 
                                     batch_size=BATCH_SIZE, 
                                     shuffle=True, 
                                     num_workers=NUM_WORKERS)

test_dataloader_simple = DataLoader(test_data_simple, 
                                    batch_size=BATCH_SIZE, 
                                    shuffle=False, 
                                    num_workers=NUM_WORKERS)

train_dataloader_simple, test_dataloader_simple
```

## –ú–æ–¥–µ–ª—å 0: –∫–ª–∞—Å [TinyVGG](https://poloclub.github.io/cnn-explainer/) {.tiny}

```{python}
#| output-location: column
class TinyVGG(nn.Module):
    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:
        super().__init__()
        self.conv_block_1 = nn.Sequential(
            nn.Conv2d(in_channels=input_shape, 
                      out_channels=hidden_units, 
                      kernel_size=3,
                      stride=1,
                      padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=hidden_units, 
                      out_channels=hidden_units,
                      kernel_size=3,
                      stride=1,
                      padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2,
                         stride=2)
        )
        self.conv_block_2 = nn.Sequential(
            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2)
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(in_features=hidden_units*16*16,
                      out_features=output_shape)
        )
    
    def forward(self, x: torch.Tensor):
        x = self.conv_block_1(x)
        x = self.conv_block_2(x)
        x = self.classifier(x)
        return x

torch.manual_seed(73)
model_0 = TinyVGG(input_shape=3,
                  hidden_units=10, 
                  output_shape=len(train_data.classes)).to(device)
model_0
```

## –ú–æ–¥–µ–ª—å 0: forward pass –Ω–∞ –æ–¥–Ω–æ–º—É –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—ñ {.tiny}

1. –û—Ç—Ä–∏–º–∞–π—Ç–µ –ø–∞–∫–µ—Ç –∑–æ–±—Ä–∞–∂–µ–Ω—å —ñ –º—ñ—Ç–æ–∫ –∑ DataLoader.
2. –û—Ç—Ä–∏–º–∞–π—Ç–µ –æ–¥–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑ –ø–∞–∫–µ—Ç–∞ —ñ —Ä–æ–∑–≥–æ—Ä–Ω—ñ—Ç—å –π–æ–≥–æ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é `unsqueeze()`, —â–æ–± —Ä–æ–∑–º—ñ—Ä –ø–∞–∫–µ—Ç–∞ —Å—Ç–∞–Ω–æ–≤–∏–≤ 1 (—Ç–æ–±—Ç–æ –π–æ–≥–æ —Ñ–æ—Ä–º–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–ª–∞ –º–æ–¥–µ–ª—ñ).
3. –í–∏–∫–æ–Ω–∞–π—Ç–µ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—é –Ω–∞ –æ–¥–Ω–æ–º—É –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—ñ (–æ–±–æ–≤'—è–∑–∫–æ–≤–æ –Ω–∞–¥—ñ—à–ª—ñ—Ç—å –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –Ω–∞ —Ü—ñ–ª—å–æ–≤–∏–π –ø—Ä–∏—Å—Ç—Ä—ñ–π).
4. –í–∏–≤–µ–¥—ñ—Ç—å –Ω–∞ –µ–∫—Ä–∞–Ω —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —Ç–µ, —â–æ –≤—ñ–¥–±—É–≤–∞—î—Ç—å—Å—è, —ñ –ø–µ—Ä–µ—Ç–≤–æ—Ä—ñ—Ç—å –Ω–µ–æ–±—Ä–æ–±–ª–µ–Ω—ñ –≤–∏—Ö—ñ–¥–Ω—ñ –ª–æ–≥—ñ—Ç–∏ –º–æ–¥–µ–ª—ñ –Ω–∞ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é `torch.softmax()` (–æ—Å–∫—ñ–ª—å–∫–∏ –º–∏ –ø—Ä–∞—Ü—é—î–º–æ –∑ –±–∞–≥–∞—Ç–æ–∫–ª–∞—Å–æ–≤–∏–º–∏ –¥–∞–Ω–∏–º–∏), –∞ –ø–æ—Ç—ñ–º –ø–µ—Ä–µ—Ç–≤–æ—Ä—ñ—Ç—å –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –Ω–∞ –º—ñ—Ç–∫–∏ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é `torch.argmax()`.

---

```{python}
img_batch, label_batch = next(iter(train_dataloader_simple))

img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]
print(f"Single image shape: {img_single.shape}\n")

model_0.eval()
with torch.inference_mode():
    pred = model_0(img_single.to(device))
    
print(f"Output logits:\n{pred}\n")
print(f"Output prediction probabilities:\n{torch.softmax(pred, dim=1)}\n")
print(f"Output prediction label:\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\n")
print(f"Actual label:\n{label_single}")
```

## –ú–æ–¥–µ–ª—å 0: `torchinfo` {.tiny}

- `torchinfo.summary()` - –≤–∏–≤–æ–¥–∏—Ç—å —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –º–æ–¥–µ–ª—å PyTorch

```{python}
#| output-location: column
try: 
    import torchinfo
except:
    !pip install torchinfo
    import torchinfo
    
from torchinfo import summary
summary(model_0, input_size=[1, 3, 64, 64])
```

## –ú–æ–¥–µ–ª—å 0: `train_step()` {.tiny}

1. `train_step()` - –≤–∏–∫–æ–Ω—É—î –æ–¥–∏–Ω –∫—Ä–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è
2. `test_step()` - –≤–∏–∫–æ–Ω—É—î –æ–¥–∏–Ω –∫—Ä–æ–∫ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
3. `train()` - –≤–∏–∫–æ–Ω—É—î —Ü–∏–∫–ª –Ω–∞–≤—á–∞–Ω–Ω—è

```{python}
def train_step(model: torch.nn.Module, 
               dataloader: torch.utils.data.DataLoader, 
               loss_fn: torch.nn.Module, 
               optimizer: torch.optim.Optimizer):
    # Put model in train mode
    model.train()
    
    # Setup train loss and train accuracy values
    train_loss, train_acc = 0, 0
    
    # Loop through data loader data batches
    for batch, (X, y) in enumerate(dataloader):
        # Send data to target device
        X, y = X.to(device), y.to(device)

        # 1. Forward pass
        y_pred = model(X)

        # 2. Calculate  and accumulate loss
        loss = loss_fn(y_pred, y)
        train_loss += loss.item() 

        # 3. Optimizer zero grad
        optimizer.zero_grad()

        # 4. Loss backward
        loss.backward()

        # 5. Optimizer step
        optimizer.step()

        # Calculate and accumulate accuracy metrics across all batches
        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)
        train_acc += (y_pred_class == y).sum().item()/len(y_pred)

    # Adjust metrics to get average loss and accuracy per batch 
    train_loss = train_loss / len(dataloader)
    train_acc = train_acc / len(dataloader)
    return train_loss, train_acc
```

## –ú–æ–¥–µ–ª—å 0: `test_step()` {.tiny}

```{python}
def test_step(model: torch.nn.Module, 
              dataloader: torch.utils.data.DataLoader, 
              loss_fn: torch.nn.Module):
    # Put model in eval mode
    model.eval() 
    
    # Setup test loss and test accuracy values
    test_loss, test_acc = 0, 0
    
    # Turn on inference context manager
    with torch.inference_mode():
        # Loop through DataLoader batches
        for batch, (X, y) in enumerate(dataloader):
            # Send data to target device
            X, y = X.to(device), y.to(device)
    
            # 1. Forward pass
            test_pred_logits = model(X)

            # 2. Calculate and accumulate loss
            loss = loss_fn(test_pred_logits, y)
            test_loss += loss.item()
            
            # Calculate and accumulate accuracy
            test_pred_labels = test_pred_logits.argmax(dim=1)
            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))
            
    # Adjust metrics to get average loss and accuracy per batch 
    test_loss = test_loss / len(dataloader)
    test_acc = test_acc / len(dataloader)
    return test_loss, test_acc
```

## –ú–æ–¥–µ–ª—å 0: –æ–±'—î–¥–Ω—É—î–º–æ –≤ `train()` {.tiny}

```{python}
from tqdm.auto import tqdm

# 1. Take in various parameters required for training and test steps
def train(model: torch.nn.Module, 
          train_dataloader: torch.utils.data.DataLoader, 
          test_dataloader: torch.utils.data.DataLoader, 
          optimizer: torch.optim.Optimizer,
          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),
          epochs: int = 5):
    
    # 2. Create empty results dictionary
    results = {"train_loss": [],
        "train_acc": [],
        "test_loss": [],
        "test_acc": []
    }
    
    # 3. Loop through training and testing steps for a number of epochs
    for epoch in tqdm(range(epochs)):
        train_loss, train_acc = train_step(model=model,
                                           dataloader=train_dataloader,
                                           loss_fn=loss_fn,
                                           optimizer=optimizer)
        test_loss, test_acc = test_step(model=model,
            dataloader=test_dataloader,
            loss_fn=loss_fn)
        
        # 4. Print out what's happening
        print(
            f"Epoch: {epoch+1} | "
            f"train_loss: {train_loss:.4f} | "
            f"train_acc: {train_acc:.4f} | "
            f"test_loss: {test_loss:.4f} | "
            f"test_acc: {test_acc:.4f}"
        )

        # 5. Update results dictionary
        # Ensure all data is moved to CPU and converted to float for storage
        results["train_loss"].append(train_loss.item() if isinstance(train_loss, torch.Tensor) else train_loss)
        results["train_acc"].append(train_acc.item() if isinstance(train_acc, torch.Tensor) else train_acc)
        results["test_loss"].append(test_loss.item() if isinstance(test_loss, torch.Tensor) else test_loss)
        results["test_acc"].append(test_acc.item() if isinstance(test_acc, torch.Tensor) else test_acc)

    # 6. Return the filled results at the end of the epochs
    return results
```

## –ú–æ–¥–µ–ª—å 0: —Ç—Ä–µ–Ω—É—î–º–æ! {.tiny}

```{python}
#| output-location: column
# Set random seeds
torch.manual_seed(73) 
torch.cuda.manual_seed(73)

# Set number of epochs
NUM_EPOCHS = 5

# Recreate an instance of TinyVGG
model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) 
                  hidden_units=10, 
                  output_shape=len(train_data.classes)).to(device)

# Setup loss function and optimizer
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)

# Start the timer
from timeit import default_timer as timer 
start_time = timer()

# Train model_0 
model_0_results = train(model=model_0, 
                        train_dataloader=train_dataloader_simple,
                        test_dataloader=test_dataloader_simple,
                        optimizer=optimizer,
                        loss_fn=loss_fn, 
                        epochs=NUM_EPOCHS)

# End the timer and print out how long it took
end_time = timer()
print(f"Total training time: {end_time-start_time:.3f} seconds")
```

## –ú–æ–¥–µ–ª—å 0: –≥—Ä–∞—Ñ—ñ–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ {.tiny}

```{python}
#| output-location: column

from typing import Tuple, Dict, List

def plot_loss_curves(results: Dict[str, List[float]]):
    
    # Get the loss values of the results dictionary (training and test)
    loss = results['train_loss']
    test_loss = results['test_loss']

    # Get the accuracy values of the results dictionary (training and test)
    accuracy = results['train_acc']
    test_accuracy = results['test_acc']

    # Figure out how many epochs there were
    epochs = range(len(results['train_loss']))

    # Setup a plot 
    plt.figure(figsize=(15, 7))

    # Plot loss
    plt.subplot(1, 2, 1)
    plt.plot(epochs, loss, label='train_loss')
    plt.plot(epochs, test_loss, label='test_loss')
    plt.title('Loss')
    plt.xlabel('Epochs')
    plt.legend()

    # Plot accuracy
    plt.subplot(1, 2, 2)
    plt.plot(epochs, accuracy, label='train_accuracy')
    plt.plot(epochs, test_accuracy, label='test_accuracy')
    plt.title('Accuracy')
    plt.xlabel('Epochs')
    plt.legend();
```

---

```{python}
plot_loss_curves(model_0_results)
```

---

![](img/loss_curves.jpg)

::: footer
[Google, Overfitting: Interpreting loss curves](https://developers.google.com/machine-learning/crash-course/overfitting/interpreting-loss-curves)
:::

## –©–æ —Ä–æ–±–∏—Ç–∏ –ø—Ä–∏ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—ñ?

::: incremental
- –ó–±—ñ–ª—å—à–∏—Ç–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö
- –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è
- –°–ø—Ä–æ—Å—Ç–∏—Ç–∏ –º–æ–¥–µ–ª—å
- [–ê—É–≥–º–µ–Ω—Ç–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö](https://developers.google.com/machine-learning/glossary#data-augmentation)
- [Transfer learning](https://developers.google.com/machine-learning/glossary#transfer-learning)
- Dropout-—à–∞—Ä–∏
- –ó–º–µ–Ω—à–µ–Ω–Ω—è –∫—Ä–æ–∫—É –Ω–∞–≤—á–∞–Ω–Ω—è
- [–†–∞–Ω–Ω—è –∑—É–ø–∏–Ω–∫–∞](https://developers.google.com/machine-learning/glossary#early_stopping)
:::

## –©–æ —Ä–æ–±–∏—Ç–∏ –ø—Ä–∏ –Ω–µ–¥–æ–Ω–∞–≤—á–∞–Ω–Ω—ñ?

- –ó–±—ñ–ª—å—à–∏—Ç–∏ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ
- –¢—Ä–µ–Ω—É–≤–∞—Ç–∏ –¥–æ–≤—à–µ
- –ó–º–µ–Ω—à–∏—Ç–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—é
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —ñ–Ω—à—ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏
- [Transfer learning](https://developers.google.com/machine-learning/glossary#transfer-learning)

## –ú–æ–¥–µ–ª—å 1: TinyVGG –∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü—ñ—î—é

```{python}
train_transform_trivial_augment = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.TrivialAugmentWide(num_magnitude_bins=31),
    transforms.ToTensor() 
])

test_transform = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.ToTensor()
])
```

## –ú–æ–¥–µ–ª—å 1: DataLoader –∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü—ñ—î—é {.tiny}

```{python}
#| output-location: column
train_data_augmented = datasets.ImageFolder(train_dir, transform=train_transform_trivial_augment)
test_data_simple = datasets.ImageFolder(test_dir, transform=test_transform)

train_data_augmented, test_data_simple
```

---

```{python}
# Turn Datasets into DataLoader's
import os
BATCH_SIZE = 32
NUM_WORKERS = os.cpu_count()

torch.manual_seed(42)
train_dataloader_augmented = DataLoader(train_data_augmented, 
                                        batch_size=BATCH_SIZE, 
                                        shuffle=True,
                                        num_workers=NUM_WORKERS)

test_dataloader_simple = DataLoader(test_data_simple, 
                                    batch_size=BATCH_SIZE, 
                                    shuffle=False, 
                                    num_workers=NUM_WORKERS)

train_dataloader_augmented, test_dataloader
```

## –ú–æ–¥–µ–ª—å 1: –ø–µ—Ä–µ–≥–ª—è–¥ {.tiny}

```{python}
#| output-location: column
torch.manual_seed(73)
model_1 = TinyVGG(
    input_shape=3,
    hidden_units=10,
    output_shape=len(train_data_augmented.classes)).to(device)
model_1
```

## –ú–æ–¥–µ–ª—å 1: —Ç—Ä–µ–Ω—É—î–º–æ! {.tiny}

```{python}
#| output-location: column

# Set random seeds
torch.manual_seed(73) 
torch.cuda.manual_seed(73)

# Set number of epochs
NUM_EPOCHS = 5

# Setup loss function and optimizer
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(params=model_1.parameters(), lr=0.001)

# Start the timer
from timeit import default_timer as timer 
start_time = timer()

# Train model_1
model_1_results = train(model=model_1, 
                        train_dataloader=train_dataloader_augmented,
                        test_dataloader=test_dataloader_simple,
                        optimizer=optimizer,
                        loss_fn=loss_fn, 
                        epochs=NUM_EPOCHS)

# End the timer and print out how long it took
end_time = timer()
print(f"Total training time: {end_time-start_time:.3f} seconds")
```

## –ú–æ–¥–µ–ª—å 1: —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ {.tiny}

```{python}
plot_loss_curves(model_1_results)
```

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–æ–¥–µ–ª–µ–π 1/2

```{python}
import pandas as pd
model_0_df = pd.DataFrame(model_0_results)
model_1_df = pd.DataFrame(model_1_results)
model_0_df
```

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–æ–¥–µ–ª–µ–π 2/2

```{python}
#| code-fold: true
# Setup a plot 
plt.figure(figsize=(15, 10))

# Get number of epochs
epochs = range(len(model_0_df))

# Plot train loss
plt.subplot(2, 2, 1)
plt.plot(epochs, model_0_df["train_loss"], label="Model 0")
plt.plot(epochs, model_1_df["train_loss"], label="Model 1")
plt.title("Train Loss")
plt.xlabel("Epochs")
plt.legend()

# Plot test loss
plt.subplot(2, 2, 2)
plt.plot(epochs, model_0_df["test_loss"], label="Model 0")
plt.plot(epochs, model_1_df["test_loss"], label="Model 1")
plt.title("Test Loss")
plt.xlabel("Epochs")
plt.legend()

# Plot train accuracy
plt.subplot(2, 2, 3)
plt.plot(epochs, model_0_df["train_acc"], label="Model 0")
plt.plot(epochs, model_1_df["train_acc"], label="Model 1")
plt.title("Train Accuracy")
plt.xlabel("Epochs")
plt.legend()

# Plot test accuracy
plt.subplot(2, 2, 4)
plt.plot(epochs, model_0_df["test_acc"], label="Model 0")
plt.plot(epochs, model_1_df["test_acc"], label="Model 1")
plt.title("Test Accuracy")
plt.xlabel("Epochs")
plt.legend();
```

## –ó–∞–≤–∞–Ω—Ç–∞–∂–∏–º–æ –≤–ª–∞—Å–Ω–µ —Ñ–æ—Ç–æ

```{python}
# Download custom image
import requests

data_path = Path("data/")
custom_image_path = data_path / "yar-pizza.jpeg"

# Download the image if it doesn't already exist
if not custom_image_path.is_file():
    with open(custom_image_path, "wb") as f:
        # When downloading from GitHub, need to use the "raw" file link
        request = requests.get("https://raw.githubusercontent.com/Aranaur/aranaur.rbind.io/refs/heads/main/lectures/cv/slides/2025/img/yar-pizza.jpg")
        print(f"Downloading {custom_image_path}...")
        f.write(request.content)
else:
    print(f"{custom_image_path} already exists, skipping download.")
```

## –ü–µ—Ä–µ–¥–∞—î–º–æ –≤–ª–∞—Å–Ω–µ —Ñ–æ—Ç–æ –≤ –º–æ–¥–µ–ª—å

```{python}
#| output-location: column
import torchvision

# Read in custom image
custom_image_uint8 = torchvision.io.read_image(str(custom_image_path))

# Print out image data
print(f"Custom image tensor:\n{custom_image_uint8}\n")
print(f"Custom image shape: {custom_image_uint8.shape}\n")
print(f"Custom image dtype: {custom_image_uint8.dtype}")
```

## –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ 1/2 {.smaller .scrollable}

```{python}
#| error: true
#| output-location: column

model_1.eval()
with torch.inference_mode():
    model_1(custom_image_uint8.to(device))
```

## –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ 2/2

```{python}
#| output-location: column

custom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)

custom_image = custom_image / 255. 

print(f"Custom image tensor:\n{custom_image}\n")
print(f"Custom image shape: {custom_image.shape}\n")
print(f"Custom image dtype: {custom_image.dtype}")
```

## –ü–µ—Ä–µ–≥–ª—è–¥ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è {.smaller}

```{python}
#| output-location: column
plt.imshow(custom_image.permute(1, 2, 0))
plt.title(f"Image shape: {custom_image.shape}")
plt.axis(False);
```

## –ó–º—ñ–Ω—é—î–º–æ —Ä–æ–∑–º—ñ—Ä

```{python}
custom_image_transform = transforms.Compose([
    transforms.Resize((64, 64)),
])

custom_image_transformed = custom_image_transform(custom_image)

print(f"Original shape: {custom_image.shape}")
print(f"New shape: {custom_image_transformed.shape}")
```

## –°–ø—Ä–æ–±—É—î–º–æ –ø–µ—Ä–µ–¥–∞—Ç–∏ –≤ –º–æ–¥–µ–ª—å {.scrollable}

```{python}
#| error: true

model_1.eval()
with torch.inference_mode():
    custom_image_pred = model_1(custom_image_transformed)
```

## –í–∏–ø—Ä–∞–≤–ª—è—î–º–æ: –ø–µ—Ä–µ–¥–∞—î–º–æ –Ω–∞ –æ–±—Ä–∞–Ω–∏–π –ø—Ä–∏—Å—Ç—Ä—ñ–π  {.scrollable}

```{python}
#| error: true
#| output-location: column

model_1.eval()
with torch.inference_mode():
    custom_image_pred = model_1(custom_image_transformed.to(device))
```

## –í–∏–ø—Ä–∞–≤–ª—è—î–º–æ: –¥–æ–¥–∞—î–º–æ batch dimension

```{python}
model_1.eval()
with torch.inference_mode():
    custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0)
    
    print(f"Custom image transformed shape: {custom_image_transformed.shape}")
    print(f"Unsqueezed custom image shape: {custom_image_transformed_with_batch_size.shape}")
    
    custom_image_pred = model_1(custom_image_transformed.unsqueeze(dim=0).to(device))
```

---

::: {.callout-note}
- **–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ñ —Ç–∏–ø–∏ –¥–∞–Ω–∏—Ö** ‚Äî –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –æ—á—ñ–∫—É—î `torch.float32`, —Ç–æ–¥—ñ —è–∫ –Ω–∞—à–µ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –±—É–ª–æ `uint8`.
- **–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π –ø—Ä–∏—Å—Ç—Ä—ñ–π** ‚Äî –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –±—É–ª–∞ –Ω–∞ —Ü—ñ–ª—å–æ–≤–æ–º—É –ø—Ä–∏—Å—Ç—Ä–æ—ó (—É –Ω–∞—à–æ–º—É –≤–∏–ø–∞–¥–∫—É ‚Äî GPU), —Ç–æ–¥—ñ —è–∫ –Ω–∞—à—ñ —Ü—ñ–ª—å–æ–≤—ñ –¥–∞–Ω—ñ —â–µ –Ω–µ –±—É–ª–∏ –ø–µ—Ä–µ–º—ñ—â–µ–Ω—ñ –Ω–∞ —Ü—ñ–ª—å–æ–≤–∏–π –ø—Ä–∏—Å—Ç—Ä—ñ–π.
- **–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ñ —Ñ–æ—Ä–º–∏** ‚Äî –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –æ—á—ñ–∫—É–≤–∞–ª–∞ –≤—Ö—ñ–¥–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ñ–æ—Ä–º–∏ [N, C, H, W] –∞–±–æ [batch_size, color_channels, height, width], —Ç–æ–¥—ñ —è–∫ –Ω–∞—à –≤–ª–∞—Å–Ω–∏–π —Ç–µ–Ω–∑–æ—Ä –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –º–∞–≤ —Ñ–æ—Ä–º—É [color_channels, height, width].
:::

## –ü—Ä–æ–≥–Ω–æ–∑

```{python}
# Print out prediction logits
print(f"Prediction logits: {custom_image_pred}")

# Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)
custom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)
print(f"Prediction probabilities: {custom_image_pred_probs}")

# Convert prediction probabilities -> prediction labels
custom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1)
print(f"Prediction label: {custom_image_pred_label}")
```

## –í–∏–≤–æ–¥–∏–º–æ –º—ñ—Ç–∫—É –∫–ª–∞—Å—É

```{python}
custom_image_pred_class = class_names[custom_image_pred_label.cpu()]
custom_image_pred_class
```

## –ó–±–∏—Ä–∞—î–º–æ –≤—Å–µ –≤ –∫—É–ø—É {.tiny}

```{python}
def pred_and_plot_image(model: torch.nn.Module, 
                        image_path: str, 
                        class_names: List[str] = None, 
                        transform=None,
                        device: torch.device = device):
    
    # 1. Load in image and convert the tensor values to float32
    target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)
    
    # 2. Divide the image pixel values by 255 to get them between [0, 1]
    target_image = target_image / 255. 
    
    # 3. Transform if necessary
    if transform:
        target_image = transform(target_image)
    
    # 4. Make sure the model is on the target device
    model.to(device)
    
    # 5. Turn on model evaluation mode and inference mode
    model.eval()
    with torch.inference_mode():
        # Add an extra dimension to the image
        target_image = target_image.unsqueeze(dim=0)
    
        # Make a prediction on image with an extra dimension and send it to the target device
        target_image_pred = model(target_image.to(device))
        
    # 6. Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)
    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)

    # 7. Convert prediction probabilities -> prediction labels
    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)
    
    # 8. Plot the image alongside the prediction and prediction probability
    plt.imshow(target_image.squeeze().permute(1, 2, 0)) # make sure it's the right size for matplotlib
    if class_names:
        title = f"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}"
    else: 
        title = f"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}"
    plt.title(title)
    plt.axis(False);
```

## –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ñ—É–Ω–∫—Ü—ñ—é {.tiny}

```{python}
#| output-location: column
pred_and_plot_image(model=model_1,
                    image_path=custom_image_path,
                    class_names=class_names,
                    transform=custom_image_transform,
                    device=device)
```


# –î—è–∫—É—é –∑–∞ —É–≤–∞–≥—É! {.unnumbered .unlisted background-iframe=".04_files/libs/colored-particles/index.html"}

<br> <br>

{{< iconify solar book-bold >}} [–ú–∞—Ç–µ—Ä—ñ–∞–ª–∏ –∫—É—Ä—Å—É](https://aranaur.rbind.io/lectures/)

{{< iconify mdi envelope >}} ihor.miroshnychenko\@knu.ua

{{< iconify ic baseline-telegram >}} [Data Mirosh](https://t.me/araprof)

{{< iconify mdi linkedin >}} [\@ihormiroshnychenko](https://www.linkedin.com/in/ihormiroshnychenko/)

{{< iconify mdi github >}} [\@aranaur](https://github.com/Aranaur)

{{< iconify ion home >}} [aranaur.rbind.io](https://aranaur.rbind.io)
