---
title: "–í—Å—Ç—É–ø –¥–æ PySpark"
subtitle: "–ú–æ–¥–µ–ª—ñ —Ç–∞ –º–µ—Ç–æ–¥–∏ –æ–±—Ä–æ–±–∫–∏ –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö"
author: "–Ü–≥–æ—Ä –ú—ñ—Ä–æ—à–Ω–∏—á–µ–Ω–∫–æ"
institute: –ö–ù–£ —ñ–º–µ–Ω—ñ –¢–∞—Ä–∞—Å–∞ –®–µ–≤—á–µ–Ω–∫–∞, –§–Ü–¢
from: markdown+emoji
title-slide-attributes:
    data-background-iframe: .01_files/libs/colored-particles/index.html
language: _language-ua.yml
footer: <a href="https://aranaur.rbind.io/lectures/mm_big_data/">üîó–ú–æ–¥–µ–ª—ñ —Ç–∞ –º–µ—Ç–æ–¥–∏ –æ–±—Ä–æ–±–∫–∏ –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö</a>
format:
  revealjs: 
    code-line-numbers: false
    # center: true
    navigation-mode: vertical
    transition: fade
    background-transition: fade
    # controls-layout: bottom-right
    chalkboard: true
    logo: fit.png
    slide-number: true
    toc: true
    toc-depth: 1
    mouse-wheel: true
    width: 1350  
    height: 759.375
    highlight-style: github
    # fig-width: 9
    # fig-height: 5
    fig-format: svg
    theme: [default, custom.scss]
    mermaid:
      theme: forest
  # gfm:
  #   mermaid-format: png
preload-iframes: true
jupyter: python3
execute: 
  echo: true
  warning: false
editor_options: 
  chunk_output_type: console

revealjs-plugins:
  - verticator
---

```{python}
#| label: setup
#| include: false

# Import libraries
from scipy.stats import (
    binom
)
import numpy as numpy
from seaborn import distplot
from matplotlib import pyplot
import seaborn
import random

import sys

# Import libraries
# import numpy as numpy
import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns

from IPython.display import Markdown
from tabulate import tabulate

# Define colors
red_pink   = "#e64173"
turquoise  = "#20B2AA"
orange     = "#FFA500"
red        = "#fb6107"
blue       = "#181485"
navy       = "#150E37FF"
green      = "#8bb174"
yellow     = "#D8BD44"
purple     = "#6A5ACD"
slate      = "#314f4f"
```

# Apache Spark —Ç–∞ PySpark

##  –©–æ —Ç–∞–∫–µ PySpark?

- Apache Spark --- —Ü–µ —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–∞ –æ–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –∑ –≤—ñ–¥–∫—Ä–∏—Ç–∏–º –≤–∏—Ö—ñ–¥–Ω–∏–º –∫–æ–¥–æ–º, –ø—Ä–∏–∑–Ω–∞—á–µ–Ω–∞ –¥–ª—è —à–≤–∏–¥–∫–æ—ó –æ–±—Ä–æ–±–∫–∏ –≤–µ–ª–∏–∫–∏—Ö –æ–±—Å—è–≥—ñ–≤ –¥–∞–Ω–∏—Ö. 

- PySpark --- —Ü–µ —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å Python –¥–ª—è Apache Spark. –í—ñ–Ω –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–æ–±–ª—è—î –≤–µ–ª–∏–∫—ñ –Ω–∞–±–æ—Ä–∏ –¥–∞–Ω–∏—Ö –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –ø–∞—Ä–∞–ª–µ–ª—å–Ω–∏—Ö –æ–±—á–∏—Å–ª–µ–Ω—å —É —Ä–æ–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å–∞—Ö Python, —ñ–¥–µ–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏, –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –ø–µ—Ä–µ–¥–∞–≤–∞–Ω–Ω—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ, –º–∞—à–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è, –∞–Ω–∞–ª—ñ–∑—É –¥–∞–Ω–∏—Ö —Ç–∞ SQL-–∑–∞–ø–∏—Ç—ñ–≤. 

- PySpark –ø—ñ–¥—Ç—Ä–∏–º—É—î —Ç–∞–∫—ñ –≥–∞–ª—É–∑—ñ, —è–∫ —Ñ—ñ–Ω–∞–Ω—Å–∏, –æ—Ö–æ—Ä–æ–Ω–∞ –∑–¥–æ—Ä–æ–≤'—è —Ç–∞ –µ–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞ –∫–æ–º–µ—Ä—Ü—ñ—è, –∑–∞–≤–¥—è–∫–∏ —à–≤–∏–¥–∫–æ—Å—Ç—ñ —Ç–∞ –º–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω–æ—Å—Ç—ñ.

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ PySpark? {.smaller}

- –ê–Ω–∞–ª—ñ—Ç–∏–∫–∞ –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö
- –†–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –¥–∞–Ω–∏—Ö
- –ü–æ—Ç–æ–∫–æ–≤—ñ –¥–∞–Ω—ñ –≤ —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ
- –ú–∞—à–∏–Ω–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –≤–µ–ª–∏–∫–∏—Ö –º–∞—Å–∏–≤–∞—Ö –¥–∞–Ω–∏—Ö
- ETL —Ç–∞ ELT –∫–æ–Ω–≤–µ—î—Ä–∏
- –†–æ–±–æ—Ç–∞ –∑ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–∏–º–∏ –¥–∂–µ—Ä–µ–ª–∞–º–∏ –¥–∞–Ω–∏—Ö:
    1. CSV
    2. JSON
    3. Parquet
    4. ...

## –ö–ª–∞—Å—Ç–µ—Ä Spark

–ö–ª–∞—Å—Ç–µ—Ä Spark --- —Ü–µ –≥—Ä—É–ø–∞ –∫–æ–º–ø'—é—Ç–µ—Ä—ñ–≤ (–≤—É–∑–ª—ñ–≤, nodes), —è–∫—ñ —Å–ø—ñ–ª—å–Ω–æ –æ–±—Ä–æ–±–ª—è—é—Ç—å –≤–µ–ª–∏–∫—ñ –Ω–∞–±–æ—Ä–∏ –¥–∞–Ω–∏—Ö –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é Apache Spark, –∑ –≥–æ–ª–æ–≤–Ω–∏–º –≤—É–∑–ª–æ–º (master node), —è–∫–∏–π –∫–æ–æ—Ä–¥–∏–Ω—É—î —Ä–æ–±–æ—Ç—É –¥–µ–∫—ñ–ª—å–∫–æ—Ö —Ä–æ–±–æ—á–∏—Ö –≤—É–∑–ª—ñ–≤ (worker nodes).

–ì–æ–ª–æ–≤–Ω–∏–π –≤—É–∑–æ–ª: –ö–µ—Ä—É—î –∫–ª–∞—Å—Ç–µ—Ä–æ–º, –∫–æ–æ—Ä–¥–∏–Ω—É—î –∑–∞–≤–¥–∞–Ω–Ω—è, —Ç–∞ –ø–ª–∞–Ω—É—î –∑–∞–≤–¥–∞–Ω–Ω—è

–†–æ–±–æ—á—ñ –≤—É–∑–ª–∏: –≤–∏–∫–æ–Ω—É—é—Ç—å –∑–∞–¥–∞—á—ñ, –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—é—Ç—å –∑–∞ –æ–±—Ä–æ–±–∫—É –¥–∞–Ω–∏—Ö, –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å –¥–∞–Ω—ñ

## SparkSession

SparkSession --- —Ü–µ —Ç–æ—á–∫–∞ –≤—Ö–æ–¥—É –≤ PySpark, —â–æ –¥–æ–∑–≤–æ–ª—è—î –≤–∑–∞—î–º–æ–¥—ñ—è—Ç–∏ –∑ –æ—Å–Ω–æ–≤–Ω–∏–º–∏ –º–æ–∂–ª–∏–≤–æ—Å—Ç—è–º–∏ Apache Spark. –í—ñ–Ω –¥–æ–∑–≤–æ–ª—è—î –Ω–∞–º –≤–∏–∫–æ–Ω—É–≤–∞—Ç–∏ –∑–∞–ø–∏—Ç–∏, –æ–±—Ä–æ–±–ª—è—Ç–∏ –¥–∞–Ω—ñ —Ç–∞ –∫–µ—Ä—É–≤–∞—Ç–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä—ñ Spark.

```{python}
#| label: spark-session

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("MySparkApp") \
    .getOrCreate()
```

- `.builder` --- —Å—Ç–≤–æ—Ä—é—î –Ω–æ–≤–∏–π –æ–±'—î–∫—Ç SparkSession
- `.appName("MySparkApp")` --- –∑–∞–¥–∞—î —ñ–º'—è –ø—Ä–æ–≥—Ä–∞–º–∏
- `.getOrCreate()` --- –æ—Ç—Ä–∏–º—É—î —ñ—Å–Ω—É—é—á–∏–π SparkSession –∞–±–æ —Å—Ç–≤–æ—Ä—é—î –Ω–æ–≤–∏–π, —è–∫—â–æ –π–æ–≥–æ —â–µ –Ω–µ–º–∞—î

## PySpark DataFrames

- DataFrames: –¢–∞–±–ª–∏—á–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç (—Ä—è–¥–∫–∏/—Å—Ç–æ–≤–ø—Ü—ñ)
- –ü—ñ–¥—Ç—Ä–∏–º—É—î SQL-–ø–æ–¥—ñ–±–Ω—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó
- –ú–æ–∂–Ω–∞ –ø–æ—Ä—ñ–≤–Ω—è—Ç–∏ –∑ Pandas Dataframe –∞–±–æ SQL TABLE
- –°—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω—ñ –¥–∞–Ω—ñ

## PySpark DataFrames {.tiny}

```{python}
#| label: spark-dataframe

salaries = spark.read.csv("data/salaries.csv", header=True, inferSchema=True)

Markdown(tabulate(
  salaries.limit(5).collect(),
  headers=salaries.columns,
))
```

- `json()`
- `parquet()`
- `orc()`
- `jdbc()`

## –°—Ö–µ–º–∞ DataFrame

```{python}
#| label: spark-dataframe-schema

salaries.printSchema()
```

## –ë–∞–∑–æ–≤—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó –∑ DataFrame {.smaller}

```{python}
#| label: spark-dataframe-operations

row_count = salaries.count()
print(f"–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—è–¥–∫—ñ–≤: {row_count}")
```

```{python}
#| label: spark-dataframe-agg

Markdown(tabulate(
  salaries.groupBy("experience_level").agg({"salary_in_usd": "avg"}).collect(),
  headers=["–†—ñ–≤–µ–Ω—å –¥–æ—Å–≤—ñ–¥—É", "–°–µ—Ä–µ–¥–Ω—è –∑–∞—Ä–ø–ª–∞—Ç–∞"],
))
```

- `sum`
- `min`
- `max`

## –ö–ª—é—á–æ–≤—ñ —Ñ—É–Ω–∫—Ü—ñ—ó

- `select()`: –≤–∏–±—ñ—Ä —Å—Ç–æ–≤–ø—Ü—ñ–≤
- `filter()` –∞–±–æ `where()`: —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è —Ä—è–¥–∫—ñ–≤
- `groupBy()`: –≥—Ä—É–ø—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö
- `agg()`: –∞–≥—Ä–µ–≥–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
- `sort()` –∞–±–æ `orderBy()`: —Å–æ—Ä—Ç—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö
- `limit()`: –æ–±–º–µ–∂–µ–Ω–Ω—è –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —Ä—è–¥–∫—ñ–≤

## –ü—Ä–∏–∫–ª–∞–¥: –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è —Ç–∞ –∞–≥—Ä–µ–≥–∞—Ü—ñ—è {.smaller}

```{python}
#| label: spark-dataframe-filter-agg

# arrange

Markdown(tabulate(
  salaries.filter(salaries["experience_level"] == "SE").groupBy("job_title") \
        .agg({"salary_in_usd": "avg"}) \
        .orderBy("avg(salary_in_usd)", ascending=False).limit(5) \
        .collect(),
  headers=["–ü–æ—Å–∞–¥–∞", "–°–µ—Ä–µ–¥–Ω—è –∑–∞—Ä–ø–ª–∞—Ç–∞"],
))
```

## –í–∏–≤–µ–¥–µ–Ω–Ω—è —Å—Ö–µ–º–∏ —Ç–∞ —Ä—É—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Å—Ö–µ–º–∏

- `inferSchema`: –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Å—Ö–µ–º–∏
- `schema`: —Ä—É—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Å—Ö–µ–º–∏
- `printSchema()`: –≤–∏–≤–µ–¥–µ–Ω–Ω—è —Å—Ö–µ–º–∏ DataFrame

## –¢–∏–ø–∏ –¥–∞–Ω–∏—Ö

- `StringType`: —Ä—è–¥–æ–∫
- `IntegerType`: —Ü—ñ–ª–µ —á–∏—Å–ª–æ
- `FloatType`: —á–∏—Å–ª–æ –∑ –ø–ª–∞–≤–∞—é—á–æ—é –∫–æ–º–æ—é
- `LongType`: –¥–æ–≤–≥–µ —Ü—ñ–ª–µ —á–∏—Å–ª–æ
- `ArrayType`: –º–∞—Å–∏–≤

## –°–∏–Ω—Ç–∞–∫—Å–∏—Å —Ç–∏–ø—ñ–≤ –¥–∞–Ω–∏—Ö

```{python}
#| label: spark-dataframe-types
#| eval: false

from pyspark.sql.types import (StructType, # <1>
                              StructField, IntegerType, # <1>
                              StringType, ArrayType)  # <1>

schema = StructType([ # <2>
  StructField("id", IntegerType(), True), # <2>
  StructField("name", StringType(), True), # <2>
  StructField("scores", ArrayType(IntegerType()), True) # <2>
]) # <2>

df = spark.createDataFrame(data, schema=schema) # <3>
```
1. –Ü–º–ø–æ—Ä—Ç —Ç–∏–ø—ñ–≤ –¥–∞–Ω–∏—Ö
2. –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Å—Ö–µ–º–∏: `StructType` —Ç–∞ `StructField` –¥–ª—è –æ–ø–∏—Å—É —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –¥–∞–Ω–∏—Ö
3. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è DataFrame –∑ –¥–∞–Ω–∏–º–∏ —Ç–∞ —Å—Ö–µ–º–æ—é

# –ú–∞–Ω—ñ–ø—É–ª—è—Ü—ñ—è –¥–∞–Ω–∏–º–∏

## –ü—Ä–æ–ø—É—â–µ–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è

- `dropna()` –∞–±–æ `na.drop()`: –≤–∏–¥–∞–ª–µ–Ω–Ω—è —Ä—è–¥–∫—ñ–≤ –∑ –ø—Ä–æ–ø—É—â–µ–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏
- `fillna()` –∞–±–æ `na.fill()`: –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å

```{python}
#| label: spark-dataframe-missing
#| eval: false

df_cleaned = df.dropna() # <1>

df_cleaned = df.where(col("column_name").isNotNull()) # <2>

df_filled = df.na.fill({"column_name": "default_value"}) # <3>
df_filled = df.fillna(0) # <4>
```
1. –í–∏–¥–∞–ª–µ–Ω–Ω—è —Ä—è–¥–∫—ñ–≤ –∑ –ø—Ä–æ–ø—É—â–µ–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏
2. –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è —Ä—è–¥–∫—ñ–≤ –∑ –Ω–µ–Ω—É–ª—å–æ–≤–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏
3. –ó–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é —Å–ª–æ–≤–Ω–∏–∫–∞
4. –ó–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –∑–Ω–∞—á–µ–Ω–Ω—è

## –°—Ç–æ–≤–ø—Ü—ñ

- `withColumn()`: –¥–æ–¥–∞–≤–∞–Ω–Ω—è –∞–±–æ –∑–º—ñ–Ω–∞ —Å—Ç–æ–≤–ø—Ü—è
- `withColumnRenamed()`: –ø–µ—Ä–µ–π–º–µ–Ω—É–≤–∞–Ω–Ω—è —Å—Ç–æ–≤–ø—Ü—è
- `drop()`: –≤–∏–¥–∞–ª–µ–Ω–Ω—è —Å—Ç–æ–≤–ø—Ü—è
- `select()`: –≤–∏–±—ñ—Ä —Å—Ç–æ–≤–ø—Ü—ñ–≤
- `distinct()`: –≤–∏–¥–∞–ª–µ–Ω–Ω—è –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤

```{python}
#| label: spark-dataframe-columns
#| eval: false

df = df.withColumn("new_column", col("existing_column") * 2)
df = df.withColumnRenamed("old_name", "new_name")
df = df.drop("column_name")
df = df.select("column1", "column2")
```

## –†—è–¥–∫–∏

- `filter()`: —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è —Ä—è–¥–∫—ñ–≤
- `groupBy()`: –≥—Ä—É–ø—É–≤–∞–Ω–Ω—è —Ä—è–¥–∫—ñ–≤

```{python}
#| label: spark-dataframe-rows
#| eval: false

filtered_df = df.filter(df["column_name"] > 10)
grouped_df = df.groupBy("column_name").avg("other_column")
```

## –ü—Ä–æ—Å—É–Ω—É—Ç—ñ —Ñ—É–Ω–∫—Ü—ñ—ó

- `join()`
- `union()`
- `intersect()`
- `except()`
- `sample()`
- `distinct()`

## Join

–¢–∏–ø–∏ –æ–±'—î–¥–Ω–∞–Ω—å: `inner`, `cross`, `outer`, `full`, `fullouter`, `full_outer`, `left`, `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`, `semi`, `leftsemi`, `left_semi`, `anti`, `leftanti` —Ç–∞ `left_anti`.

```{python}
#| label: spark-dataframe-join
#| eval: false

df_joined = df1.join(df2, on="key", how="inner")
df_joined = df1.join(df2, on=["key1", "key2"], how="inner")

df_joined = df1.join(df2, df1.key == df2.key, "inner")
```

## Union

–û–±'—î–¥–Ω—É—î —Ä—è–¥–∫–∏ –∑ –¥–≤–æ—Ö —Ñ—Ä–µ–π–º—ñ–≤ –¥–∞–Ω–∏—Ö –∑ –æ–¥–Ω–∞–∫–æ–≤–æ—é —Å—Ö–µ–º–æ—é

```{python}
#| label: spark-dataframe-union
#| eval: false

df_union = df1.union(df2)
```

## Arrays —Ç–∞ Maps

- `array()`: —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–∞—Å–∏–≤—É
- `map()`: —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å–ª–æ–≤–Ω–∏–∫–∞

```{python}
#| label: spark-dataframe-arrays
#| eval: false

from pyspark.sql.functions import array, struct, lit

df = df.withColumn("scores", array(lit(85), lit(90), lit(78)))
```

```{python}
#| label: spark-dataframe-maps
#| eval: false

from pyspark.sql.types import StructField, StructType, StringType, MapType

schema = StructType([
    StructField("id", IntegerType(), True),
    StructField("name", StringType(), True),
    StructField("scores", MapType(StringType(), IntegerType()), True)
])
```

## –†–æ–±–æ—Ç–∞ –∑—ñ Structs

- Structs: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤–∫–ª–∞–¥–µ–Ω–∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä —É —Ä—è–¥–∫–∞—Ö

```{python}
#| label: spark-dataframe-structs
#| eval: false

df = df.withColumn("address", struct(col("city"), col("state")))
```

# –í–ª–∞—Å–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó

## UDF (User Defined Functions)

- UDF --- —Ü–µ —Ñ—É–Ω–∫—Ü—ñ—ó, —è–∫—ñ –≤–∏ –≤–∏–∑–Ω–∞—á–∞—î—Ç–µ —Å–∞–º–æ—Å—Ç—ñ–π–Ω–æ –¥–ª—è –≤–∏–∫–æ–Ω–∞–Ω–Ω—è —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω–∏—Ö –æ–±—á–∏—Å–ª–µ–Ω—å –Ω–∞ –¥–∞–Ω–∏—Ö —É DataFrame.

–ü–µ—Ä–µ–≤–∞–≥–∏ UDF:

- –ü–æ–≤—Ç–æ—Ä–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Ç–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è —Å–ø—ñ–ª—å–Ω–∏—Ö –∑–∞–≤–¥–∞–Ω—å
- –†–µ—î—Å—Ç—Ä—É—é—Ç—å—Å—è –±–µ–∑–ø–æ—Å–µ—Ä–µ–¥–Ω—å–æ —É Spark —ñ –º–æ–∂—É—Ç—å –±—É—Ç–∏ –¥–æ—Å—Ç—É–ø–Ω–∏–º–∏ –¥–ª—è —Å–ø—ñ–ª—å–Ω–æ–≥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
- PySpark DataFrames (–¥–ª—è –º–µ–Ω—à–∏—Ö –Ω–∞–±–æ—Ä—ñ–≤ –¥–∞–Ω–∏—Ö)
- Pandas UDF (–¥–ª—è –±—ñ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä—ñ–≤ –¥–∞–Ω–∏—Ö)

## UDF

–í—Å—ñ PySpark UDF –º–∞—é—Ç—å –±—É—Ç–∏ –∑–∞—Ä–µ—î—Å—Ç—Ä–æ–≤–∞–Ω—ñ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é —Ñ—É–Ω–∫—Ü—ñ—ó `udf()`.

```{python}
#| label: spark-dataframe-udf
#| eval: false

def to_uppercase(s): # <1>
    return s.upper() if s else None # <1>

to_uppercase_udf = udf(to_uppercase, StringType()) # <2>

df = df.withColumn("upper_name", to_uppercase_udf(df["name"])) # <2>
```

::: {.callout-note appearance="simple"}
UDF –¥–æ–∑–≤–æ–ª—è—é—Ç—å –∑–∞—Å—Ç–æ—Å–æ–≤—É–≤–∞—Ç–∏ –∫–∞—Å—Ç–æ–º–Ω—É –ª–æ–≥—ñ–∫—É Python –¥–æ —Ñ—Ä–µ–π–º—ñ–≤ –¥–∞–Ω–∏—Ö PySpark
:::

## pandas UDF

- –£—Å—É–≤–∞—î "–≤–∞–∂–∫—ñ" –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–¥—É —Ç–∞ –¥–∞–Ω–∏—Ö
- –ù–µ –ø–æ—Ç—Ä–µ–±—É—î —Ä–µ—î—Å—Ç—Ä–∞—Ü—ñ—ó –≤ SparkSession
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ Pandas –Ω–∞ –¥—É–∂–µ –≤–µ–ª–∏–∫–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–∏—Ö

```{python}
#| label: spark-dataframe-pandas-udf
#| eval: false

from pyspark.sql.functions import pandas_udf

@pandas_udf("float")
def fahrenheit_to_celsius_pandas(temp_f):
  return (temp_f - 32) * 5.0/9.0
```

## PySpark UDFs vs. Pandas UDFs

::: {.columns}
::: {.column}
**PySpark UDF**

- –ù–∞–π–∫—Ä–∞—â–µ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è –≤—ñ–¥–Ω–æ—Å–Ω–æ –Ω–µ–≤–µ–ª–∏–∫–∏—Ö –Ω–∞–±–æ—Ä—ñ–≤ –¥–∞–Ω–∏—Ö
- –ü—Ä–æ—Å—Ç–µ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è
- –ü–∞—Ä–∞–ª–µ–ª—å–Ω—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –Ω–∞ —Ä—ñ–≤–Ω—ñ —Å—Ç–æ–≤–ø—Ü—ñ–≤
- –ü–æ—Ç—Ä–µ–±—É—î —Ä–µ—î—Å—Ç—Ä–∞—Ü—ñ—ó –≤ SparkSession
:::
::: {.column}
**Pandas UDF**

- –í—ñ–¥–Ω–æ—Å–Ω–æ –≤–µ–ª–∏–∫—ñ –Ω–∞–±–æ—Ä–∏ –¥–∞–Ω–∏—Ö
- –°–∫–ª–∞–¥–Ω—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è, —Ç–∞–∫—ñ —è–∫ –º–∞—à–∏–Ω–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è
- –ü–∞—Ä–∞–ª–µ–ª—å–Ω—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –Ω–∞ —Ä—ñ–≤–Ω—ñ —Ä—è–¥–∫—ñ–≤
- –ù–µ –ø–æ—Ç—Ä–µ–±—É—î —Ä–µ—î—Å—Ç—Ä–∞—Ü—ñ—ó –≤ SparkSession
:::
:::

# RDD (Resilient Distributed Dataset)

## PySpark –ø–∞—Ä–∞–ª–µ–ª—ñ–∑–∞—Ü—ñ—è

- –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ —Ä–æ–∑–ø–∞—Ä–∞–ª–µ–ª—é–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö —ñ –æ–±—á–∏—Å–ª–µ–Ω—å –Ω–∞ –¥–µ–∫—ñ–ª—å–∫–æ—Ö –≤—É–∑–ª–∞—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞
- –†–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –≤–µ–ª–∏–∫–∏—Ö –Ω–∞–±–æ—Ä—ñ–≤ –¥–∞–Ω–∏—Ö –Ω–∞ –¥–µ–∫—ñ–ª—å–∫–æ—Ö –≤—É–∑–ª–∞—Ö
- –†–æ–±–æ—á—ñ –≤—É–∑–ª–∏ –æ–±—Ä–æ–±–ª—è—é—Ç—å –¥–∞–Ω—ñ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ, –æ–±'—î–¥–Ω—É—é—á–∏—Å—å –≤ –∫—ñ–Ω—Ü—ñ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∑–∞–≤–¥–∞–Ω–Ω—è
- –ú–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω–∞ —à–≤–∏–¥—à–∞ –æ–±—Ä–æ–±–∫–∞ (–≥—ñ–≥–∞–±–∞–π—Ç–∏ –∞–±–æ –Ω–∞–≤—ñ—Ç—å —Ç–µ—Ä–∞–±–∞–π—Ç–∏)

## RDD (Resilient Distributed Dataset)

RDD (—Å—Ç—ñ–π–∫—ñ —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω—ñ –Ω–∞–±–æ—Ä–∏ –¥–∞–Ω–∏—Ö) --- —Ü–µ –æ—Å–Ω–æ–≤–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–∏—Ö —É Spark, —â–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—î —Å–æ–±–æ—é —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–∏–π –Ω–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö, —è–∫–∏–π –º–æ–∂–µ –±—É—Ç–∏ –æ–±—Ä–æ–±–ª–µ–Ω–∏–π –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ –Ω–∞ –¥–µ–∫—ñ–ª—å–∫–æ—Ö –≤—É–∑–ª–∞—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞.

- –†–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–∞ –∫–æ–ª–µ–∫—Ü—ñ—è –¥–∞–Ω–∏—Ö –ø–æ –≤—Å—å–æ–º—É –∫–ª–∞—Å—Ç–µ—Ä—É –∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–º –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è–º –ø—ñ—Å–ª—è –∑–±–æ—ó–≤ –≤—É–∑–ª—ñ–≤
- –î–æ–±—Ä–µ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è –≤–µ–ª–∏–∫–∏—Ö –æ–±—Å—è–≥—ñ–≤ –¥–∞–Ω–∏—Ö
- –ù–µ–∑–º—ñ–Ω–Ω—ñ —ñ –º–æ–∂—É—Ç—å –±—É—Ç–∏ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω—ñ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é —Ç–∞–∫–∏—Ö –æ–ø–µ—Ä–∞—Ü—ñ–π, —è–∫ `map()` –∞–±–æ `filter()`, –∑ –¥—ñ—è–º–∏ –Ω–∞ –∫—à—Ç–∞–ª—Ç `collect()` –∞–±–æ `parallel()` –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —ñ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è RDD

## –°—Ç–≤–æ—Ä–µ–Ω–Ω—è RDD

```{python}
#| label: spark-rdd

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("RDDExample").getOrCreate()

census_df = spark.read.csv("data/salaries.csv", header=True, inferSchema=True)
census_rdd = census_df.rdd # <1>
data_collected = census_rdd.collect() # <2>

data_collected[:3]
```
1. –û—Ç—Ä–∏–º–∞–Ω–Ω—è RDD –∑ DataFrame
2. –í–∏–∫–ª–∏–∫ `collect()` –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –≤—Å—ñ—Ö –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ RDD

## –ü–æ–∫–∞–∑–∞—Ç–∏ RDD

```{python}
#| label: spark-rdd-show

for row in data_collected[:3]:
    print(row)
```

## RDD vs DataFrame {.smaller}

::: {.columns}
::: {.column}
**DataFrames**

- –í–∏—â—ñ–π —Ä—ñ–≤–µ–Ω—å: –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ –¥–ª—è –∑—Ä—É—á–Ω–æ—Å—Ç—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
- SQL-–ø–æ–¥—ñ–±–Ω—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó: –ü—Ä–∞—Ü—é–π—Ç–µ –∑ SQL-–ø–æ–¥—ñ–±–Ω–∏–º–∏ –∑–∞–ø–∏—Ç–∞–º–∏ —Ç–∞ –≤–∏–∫–æ–Ω—É–≤–∞—Ç–∏ —Å–∫–ª–∞–¥–Ω—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó
–∑ –º–µ–Ω—à–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –∫–æ–¥—É
- –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ —Å—Ö–µ–º—É: –ú—ñ—Å—Ç–∏—Ç—å —Å—Ç–æ–≤–ø—Ü—ñ —Ç–∞ —Ç–∏–ø–∏, —è–∫ —É SQL-—Ç–∞–±–ª–∏—Ü—ñ
:::
::: {.column}
**RDDs**

- –ù–∏–∑—å–∫–æ—Ä—ñ–≤–Ω–µ–≤–∏–π: –±—ñ–ª—å—à –≥–Ω—É—á–∫–∏–π, –∞–ª–µ –≤–∏–º–∞–≥–∞—î –±—ñ–ª—å—à–µ —Ä—è–¥–∫—ñ–≤ –∫–æ–¥—É –¥–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö –æ–ø–µ—Ä–∞—Ü—ñ–π
- –ë–µ–∑–ø–µ–∫–∞ —Ç–∏–ø—ñ–≤: –ó–±–µ—Ä—ñ–≥–∞—é—Ç—å —Ç–∏–ø–∏ –¥–∞–Ω–∏—Ö, –∞–ª–µ –Ω–µ –º–∞—é—Ç—å –ø–µ—Ä–µ–≤–∞–≥ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó DataFrames
- –ù–µ–º–∞—î —Å—Ö–µ–º–∏: –°–∫–ª–∞–¥–Ω—ñ—à–µ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –∑—ñ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏, —Ç–∞–∫–∏–º–∏ —è–∫ SQL –∞–±–æ —Ä–µ–ª—è—Ü—ñ–π–Ω—ñ –¥–∞–Ω—ñ
- –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
- –î—É–∂–µ –±–∞–≥–∞—Ç–æ—Å–ª—ñ–≤–Ω–∏–π —É –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—ñ –∑ DataFrames —ñ –ø–æ–≥–∞–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏
:::
:::

## –î–µ—è–∫—ñ —Ñ—É–Ω–∫—Ü—ñ—ó —Ç–∞ –º–µ—Ç–æ–¥–∏ RDD

- `map()`: –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è —Ñ—É–Ω–∫—Ü—ñ—ó –¥–æ –∫–æ–∂–Ω–æ–≥–æ –µ–ª–µ–º–µ–Ω—Ç–∞ RDD
- `collect()`: –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –≤—Å—ñ—Ö –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ RDD

# Spark SQL

## –ó–∞–≥–∞–ª—å–Ω–µ

- –ú–æ–¥—É–ª—å –≤ Apache Spark –¥–ª—è –æ–±—Ä–æ–±–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö
- –î–æ–∑–≤–æ–ª—è—î –∑–∞–ø—É—Å–∫–∞—Ç–∏ SQL-–∑–∞–ø–∏—Ç–∏ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ —ñ–∑ –∑–∞–≤–¥–∞–Ω–Ω—è–º–∏ –æ–±—Ä–æ–±–∫–∏ –¥–∞–Ω–∏—Ö
- –ë–µ–∑—à–æ–≤–Ω–µ –ø–æ—î–¥–Ω–∞–Ω–Ω—è Python —Ç–∞ SQL –≤ –æ–¥–Ω–æ–º—É –¥–æ–¥–∞—Ç–∫—É
- DataFrame Interfacing: –ó–∞–±–µ–∑–ø–µ—á—É—î –ø—Ä–æ–≥—Ä–∞–º–Ω–∏–π –¥–æ—Å—Ç—É–ø –¥–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö

## TempViews

- `createOrReplaceTempView()`: —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∞–±–æ –∑–∞–º—ñ–Ω–∞ —Ç–∏–º—á–∞—Å–æ–≤–æ—ó —Ç–∞–±–ª–∏—Ü—ñ

```{python}
#| label: spark-sql-table

spark = SparkSession.builder.appName("Spark SQL Example").getOrCreate()

data = [("Alice", "HR", 30), ("Bob", "IT", 40), ("Cathy", "HR", 28)]
columns = ["Name", "Department", "Age"]
df = spark.createDataFrame(data, schema=columns)

df.createOrReplaceTempView("people")

result = spark.sql("SELECT Name, Age FROM people WHERE Age > 30")
result.show()
```

## TempViews {.smaller}

- TempViews –∑–∞—Ö–∏—â–∞—é—Ç—å –æ—Å–Ω–æ–≤–Ω—ñ –¥–∞–Ω—ñ –ø—ñ–¥ —á–∞—Å –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏
- –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑ CSV –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –≤–∂–µ –≤—ñ–¥–æ–º—ñ –Ω–∞–º –º–µ—Ç–æ–¥–∏

```{python}
#| label: spark-sql-table-csv

df = spark.read.csv("data/salaries.csv", header=True, inferSchema=True)
df.createOrReplaceTempView("salaries")

spark.sql("SELECT experience_level, salary_in_usd FROM salaries").show(5)
```

## SQL —Ç–∞ DataFrame

```{python}
#| label: spark-sql-query

query_result = spark.sql("SELECT * FROM salaries WHERE job_title = 'Data Analyst'")

high_earners = query_result.withColumn("Bonus", query_result["salary_in_usd"] * 1.1) \
                            .filter(query_result["salary_in_usd"] > 100000) \
                            .select("experience_level", "job_title", "salary_in_usd", "Bonus")

high_earners.show(5)
```

## –ê–≥—Ä–µ–≥–∞—Ü—ñ—ó PySpark

```{python}
#| label: spark-sql-agg

spark.sql("""
SELECT experience_level, AVG(salary_in_usd) AS avg_salary
FROM salaries
GROUP BY experience_level
ORDER BY avg_salary DESC
""").show()
```

## –ö–æ–º–±—ñ–Ω—É–≤–∞–Ω–Ω—è SQL —Ç–∞ DataFrame

```{python}
#| label: spark-sql-combine

filtered_df = df.filter(df["experience_level"] == "SE") # <1>

filtered_df.createOrReplaceTempView("filtered_salaries") # <2>

spark.sql("""
  SELECT job_title, AVG(salary_in_usd) AS avg_salary
  FROM filtered_salaries
  GROUP BY job_title
  HAVING avg_salary > 300000
  ORDER BY avg_salary DESC
""").show(5)
```
1. –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö —É DataFrame
2. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–∏–º—á–∞—Å–æ–≤–æ—ó —Ç–∞–±–ª–∏—Ü—ñ –∑ –≤—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏

## –†–æ–±–æ—Ç–∞ –∑ —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–∏—Ö –≤ –∞–≥—Ä–µ–≥–∞—Ü—ñ—è—Ö

- `cast()`: –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–∏–ø—É –¥–∞–Ω–∏—Ö

```{python}
#| label: spark-sql-agg-types

data = [("IT", "3000"), ("IT", "4000"), ("Finance", "3500")]
columns = ["Department", "Salary"]
df = spark.createDataFrame(data, schema=columns)

df = df.withColumn("Salary", df["Salary"].cast("int"))
# Perform aggregation
df.groupBy("Department").sum("Salary").show()
```

## RDD –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü—ñ–π

```{python}
#| label: spark-rdd-agg

rdd = df.rdd.map(lambda row: (row["Department"], row["Salary"])) # <1>

rdd_aggregated = rdd.reduceByKey(lambda x, y: x + y) # <2>

print(rdd_aggregated.collect())
```
1. –û—Ç—Ä–∏–º–∞–Ω–Ω—è RDD –∑ DataFrame
2. –í–∏–∫–æ–Ω–∞–Ω–Ω—è –∞–≥—Ä–µ–≥–∞—Ü—ñ—ó –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é `reduceByKey()` --- –≥—Ä—É–ø—É—î –¥–∞–Ω—ñ –∑–∞ –∫–ª—é—á–∞–º–∏ —Ç–∞ –æ–±—á–∏—Å–ª—é—î —Å—É–º—É

## –ö—Ä–∞—â—ñ –ø—Ä–∞–∫—Ç–∏–∫–∏

- **–§—ñ–ª—å—Ç—Ä—É–π—Ç–µ –∑–∞–≤—á–∞—Å–Ω–æ**: –∑–º–µ–Ω—à—É–π—Ç–µ —Ä–æ–∑–º—ñ—Ä –¥–∞–Ω–∏—Ö –ø–µ—Ä–µ–¥ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è–º –∞–≥—Ä–µ–≥–∞—Ü—ñ—ó
- **–ü—Ä–∞—Ü—é–π—Ç–µ –∑ —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–∏—Ö**: –ø–µ—Ä–µ–∫–æ–Ω–∞–π—Ç–µ—Å—è, —â–æ –¥–∞–Ω—ñ —á–∏—Å—Ç—ñ —Ç–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≤–≤–µ–¥–µ–Ω—ñ
- **–£–Ω–∏–∫–∞–π—Ç–µ** –æ–ø–µ—Ä–∞—Ü—ñ–π, —è–∫—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å **–≤–µ—Å—å –Ω–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö**: –º—ñ–Ω—ñ–º—ñ–∑—É–π—Ç–µ —Ç–∞–∫—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó, —è–∫ `groupBy()`
- **–í–∏–±–µ—Ä—ñ—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å**: –Ω–∞–¥–∞–≤–∞–π—Ç–µ –ø–µ—Ä–µ–≤–∞–≥—É DataFrame –¥–ª—è –±—ñ–ª—å—à–æ—Å—Ç—ñ –∑–∞–≤–¥–∞–Ω—å —á–µ—Ä–µ–∑ —ó—Ö –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—é
- **–í—ñ–¥—Å—Ç–µ–∂—É–π—Ç–µ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å**: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ `explain()` –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –ø–ª–∞–Ω—É –≤–∏–∫–æ–Ω–∞–Ω–Ω—è —Ç–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó

## –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è

- Py–Üpark –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø—Ä–∞—Ü—é—î –∑ –≥—ñ–≥–∞–±–∞–π—Ç–∞–º–∏ —ñ —Ç–µ—Ä–∞–±–∞–π—Ç–∞–º–∏ –¥–∞–Ω–∏—Ö
- –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è PySpark --- —Ü–µ —à–≤–∏–¥–∫—ñ—Å—Ç—å —Ç–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –¥–∞–Ω–∏—Ö
- –†–æ–∑—É–º—ñ–Ω–Ω—è –≤–∏–∫–æ–Ω–∞–Ω–Ω—è PySpark –∑–∞–±–µ–∑–ø–µ—á—É—î —â–µ –±—ñ–ª—å—à—É –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ `broadcast()` –¥–ª—è —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –≤—Å—ñ–º –∫–ª–∞—Å—Ç–µ—Ä–æ–º: 
  - `broadcast()` --- –¥–ª—è —Ä–æ–∑–ø–æ–¥—ñ–ª—É –¥–∞–Ω–∏—Ö –Ω–∞ –≤—Å—ñ –≤—É–∑–ª–∏
  - `join()` --- –¥–ª—è –æ–±'—î–¥–Ω–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ —Ä—ñ–∑–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª

```{python}
#| label: spark-sql-broadcast
#| eval: false

joinded_df = df1.join(broadcast(small_df2), on="key", how="inner")
```

## –ü–ª–∞–Ω –≤–∏–∫–æ–Ω–∞–Ω–Ω—è

- `explain()`: –≤–∏–≤–µ–¥–µ–Ω–Ω—è –ø–ª–∞–Ω—É –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∑–∞–ø–∏—Ç—É

```{python}
#| label: spark-sql-agg-explain

df.groupBy("Department").agg({"Salary": "avg"}).explain()
```

::: footer
[explain](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.explain.html)
:::

## –ö–µ—à—É–≤–∞–Ω–Ω—è —Ç–∞ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è {.tiny}

- `cache()`: –∫–µ—à—É–≤–∞–Ω–Ω—è DataFrame —É –ø–∞–º'—è—Ç—ñ
- `persist()`: –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è DataFrame –Ω–∞ –¥–∏—Å–∫—É –∞–±–æ –≤ –ø–∞–º'—è—Ç—ñ

```{python}
#| label: spark-sql-cache

df = spark.read.csv("data/salaries.csv", header=True, inferSchema=True)

df.cache() # <1>

df.filter(df["experience_level"] == "SE").select("job_title", "salary_in_usd").show(5) # <2>
df.groupBy("job_title").agg({"salary_in_usd": "avg"}).select("job_title", "avg(salary_in_usd)").show(5) # <3>
```

## –ö–µ—à—É–≤–∞–Ω–Ω—è —Ç–∞ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è {.tiny}

- `MEMORY_ONLY`: –∫–µ—à—É–≤–∞–Ω–Ω—è –≤ –ø–∞–º'—è—Ç—ñ
- `MEMORY_AND_DISK`: –∫–µ—à—É–≤–∞–Ω–Ω—è –≤ –ø–∞–º'—è—Ç—ñ —Ç–∞ –Ω–∞ –¥–∏—Å–∫—É
- `DISK_ONLY`: –∫–µ—à—É–≤–∞–Ω–Ω—è –Ω–∞ –¥–∏—Å–∫—É
- ...

```{python}
#| label: spark-sql-persist

from pyspark import StorageLevel # <1>

df.persist(StorageLevel.MEMORY_ONLY) # <2>

result = df.groupBy("job_title").agg({"salary_in_usd": "avg"}) # <3>
result.show(5)

df.unpersist() # <4>
```
1. –Ü–º–ø–æ—Ä—Ç `StorageLevel` –¥–ª—è —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –∫–µ—à—É–≤–∞–Ω–Ω—è–º
2. –ó–±–µ—Ä—ñ–≥–∞–Ω–Ω—è DataFrame –≤ –ø–∞–º'—è—Ç—ñ —Ç–∞ –Ω–∞ –¥–∏—Å–∫—É
3. –í–∏–∫–æ–Ω–∞–Ω–Ω—è –∑–∞–ø–∏—Ç—É
4. –í–∏–≤—ñ–ª—å–Ω–µ–Ω–Ω—è –∫–µ—à—É

::: footer
[StorageLevel](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.StorageLevel.html)
:::

## –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è

- –ß–∏–º –±—ñ–ª—å—à–µ –¥–∞–Ω–∏—Ö –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è, —Ç–∏–º –ø–æ–≤—ñ–ª—å–Ω—ñ—à–∞ —Ä–æ–±–æ—Ç–∞: –≤—ñ–¥–¥–∞–≤–∞–π—Ç–µ –ø–µ—Ä–µ–≤–∞–≥—É —Ç–∞–∫–∏–º —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º, —è–∫
`map()` –Ω–∞–¥ `groupby()`.
- Broadcast Joins: Broadcast –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –≤—Å—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è, –Ω–∞–≤—ñ—Ç—å –Ω–∞ –Ω–µ–≤–µ–ª–∏–∫–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–∏—Ö
- –£–Ω–∏–∫–∞–π—Ç–µ –ø–æ–≤—Ç–æ—Ä—é–≤–∞–Ω–∏—Ö –¥—ñ–π: –ü–æ–≤—Ç–æ—Ä—é–≤–∞–Ω—ñ –¥—ñ—ó –Ω–∞–¥ –æ–¥–Ω–∏–º–∏ —ñ —Ç–∏–º–∏ –∂ –¥–∞–Ω–∏–º–∏ –∑–∞–±–∏—Ä–∞—é—Ç—å —á–∞—Å —Ç–∞ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è,
–±–µ–∑ –∂–æ–¥–Ω–æ—ó –∫–æ—Ä–∏—Å—Ç—ñ

# –î—è–∫—É—é –∑–∞ —É–≤–∞–≥—É! {.unnumbered .unlisted background-iframe=".01_files/libs/colored-particles/index.html"}

<br> <br>

{{< iconify solar book-bold >}} [–ú–∞—Ç–µ—Ä—ñ–∞–ª–∏ –∫—É—Ä—Å—É](https://aranaur.rbind.io/lectures/mm_big_data/)

{{< iconify mdi envelope >}} ihor.miroshnychenko\@knu.ua

{{< iconify ic baseline-telegram >}} [Data Mirosh](https://t.me/araprof)

{{< iconify mdi linkedin >}} [\@ihormiroshnychenko](https://www.linkedin.com/in/ihormiroshnychenko/)

{{< iconify mdi github >}} [\@aranaur](https://github.com/Aranaur)

{{< iconify ion home >}} [aranaur.rbind.io](https://aranaur.rbind.io)
