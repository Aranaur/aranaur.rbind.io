---
title: "–û—Å–Ω–æ–≤–∏ –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö –∑ PySpark"
subtitle: "–ú–æ–¥–µ–ª—ñ —Ç–∞ –º–µ—Ç–æ–¥–∏ –æ–±—Ä–æ–±–∫–∏ –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö"
author: "–Ü–≥–æ—Ä –ú—ñ—Ä–æ—à–Ω–∏—á–µ–Ω–∫–æ"
institute: –ö–ù–£ —ñ–º–µ–Ω—ñ –¢–∞—Ä–∞—Å–∞ –®–µ–≤—á–µ–Ω–∫–∞, –§–Ü–¢
from: markdown+emoji
title-slide-attributes:
    data-background-iframe: .02_files/libs/colored-particles/index.html
language: _language-ua.yml
footer: <a href="https://aranaur.rbind.io/lectures/mm_big_data/">üîó–ú–æ–¥–µ–ª—ñ —Ç–∞ –º–µ—Ç–æ–¥–∏ –æ–±—Ä–æ–±–∫–∏ –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö</a>
format:
  revealjs: 
    code-line-numbers: false
    # center: true
    navigation-mode: vertical
    transition: fade
    background-transition: fade
    # controls-layout: bottom-right
    chalkboard: true
    logo: fit.png
    slide-number: true
    toc: true
    toc-depth: 1
    mouse-wheel: true
    width: 1350  
    height: 759.375
    highlight-style: github
    # fig-width: 9
    # fig-height: 5
    fig-format: svg
    theme: [default, custom.scss]
    mermaid:
      theme: forest
  # gfm:
  #   mermaid-format: png
preload-iframes: true
engine: jupyter
jupyter: python3
execute: 
  echo: true
  warning: false
editor_options: 
  chunk_output_type: console

revealjs-plugins:
  - verticator
---

```{python}
#| label: setup
#| include: false

# Import libraries
from scipy.stats import (
    binom
)
import numpy as numpy
from seaborn import distplot
from matplotlib import pyplot
import seaborn
import random

import sys

# Import libraries
# import numpy as numpy
import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns

from IPython.display import Markdown
from tabulate import tabulate

# Define colors
red_pink   = "#e64173"
turquoise  = "#20B2AA"
orange     = "#FFA500"
red        = "#fb6107"
blue       = "#181485"
navy       = "#150E37FF"
green      = "#8bb174"
yellow     = "#D8BD44"
purple     = "#6A5ACD"
slate      = "#314f4f"
```

# –í—Å—Ç—É–ø –¥–æ –∞–Ω–∞–ª—ñ–∑—É –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é Spark

## –©–æ —Ç–∞–∫–µ –≤–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ?

> –í–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ --- –Ω–∞–±–æ—Ä–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó (—è–∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–æ—ó, —Ç–∞–∫ —ñ –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–æ—ó) –Ω–∞—Å—Ç—ñ–ª—å–∫–∏ –≤–µ–ª–∏–∫–∏—Ö —Ä–æ–∑–º—ñ—Ä—ñ–≤, —â–æ —Ç—Ä–∞–¥–∏—Ü—ñ–π–Ω—ñ —Å–ø–æ—Å–æ–±–∏ —Ç–∞ –ø—ñ–¥—Ö–æ–¥–∏ (–∑–¥–µ–±—ñ–ª—å—à–æ–≥–æ –∑–∞—Å–Ω–æ–≤–∞–Ω—ñ –Ω–∞ —Ä—ñ—à–µ–Ω–Ω—è—Ö –∫–ª–∞—Å—É –±—ñ–∑–Ω–µ—Å–æ–≤–æ—ó –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏ —Ç–∞ —Å–∏—Å—Ç–µ–º–∞—Ö —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –±–∞–∑–∞–º–∏ –¥–∞–Ω–∏—Ö) –Ω–µ –º–æ–∂—É—Ç—å –±—É—Ç–∏ –∑–∞—Å—Ç–æ—Å–æ–≤–∞–Ω—ñ –¥–æ –Ω–∏—Ö - Wikipedia

## –¢—Ä–∏ ¬´V¬ª –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö

- **–û–±—Å—è–≥** (Volume) --- –æ–±—Å—è–≥ –¥–∞–Ω–∏—Ö, —è–∫—ñ –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ —Ç–∞ –æ–±—Ä–æ–±–ª—è—Ç–∏. –¶–µ –º–æ–∂–µ –±—É—Ç–∏ –≤–µ–ª–∏—á–µ–∑–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö, —â–æ –ø–µ—Ä–µ–≤–∏—â—É—î –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ —Ç—Ä–∞–¥–∏—Ü—ñ–π–Ω–∏—Ö —Å–∏—Å—Ç–µ–º –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è.
- **–®–≤–∏–¥–∫—ñ—Å—Ç—å** (Velocity) --- —à–≤–∏–¥–∫—ñ—Å—Ç—å, –∑ —è–∫–æ—é –¥–∞–Ω—ñ –≥–µ–Ω–µ—Ä—É—é—Ç—å—Å—è —Ç–∞ –æ–±—Ä–æ–±–ª—è—é—Ç—å—Å—è. –¶–µ –º–æ–∂–µ –±—É—Ç–∏ –ø–æ—Ç—ñ–∫ –¥–∞–Ω–∏—Ö —É —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ –∞–±–æ –¥–∞–Ω—ñ, —è–∫—ñ –Ω–∞–¥—Ö–æ–¥—è—Ç—å –∑ —Ä—ñ–∑–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª.
- **–†—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å** (Variety) --- —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å —Ç–∏–ø—ñ–≤ –¥–∞–Ω–∏—Ö, —è–∫—ñ –ø–æ—Ç—Ä—ñ–±–Ω–æ –æ–±—Ä–æ–±–ª—è—Ç–∏. –¶–µ –º–æ–∂—É—Ç—å –±—É—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω—ñ, –Ω–∞–ø—ñ–≤—Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω—ñ —Ç–∞ –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω—ñ –¥–∞–Ω—ñ –∑ —Ä—ñ–∑–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª.

## –ö–æ–Ω—Ü–µ–ø—Ü—ñ—ó —Ç–∞ —Ç–µ—Ä–º—ñ–Ω–æ–ª–æ–≥—ñ—è –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö

- **–ö–ª–∞—Å—Ç–µ—Ä–Ω—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è**: –û–±'—î–¥–Ω–∞–Ω–Ω—è —Ä–µ—Å—É—Ä—Å—ñ–≤ –¥–µ–∫—ñ–ª—å–∫–æ—Ö –º–∞—à–∏–Ω
- **–ü–∞—Ä–∞–ª–µ–ª—å–Ω—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è**: –û–¥–Ω–æ—á–∞—Å–Ω—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –Ω–∞ –æ–¥–Ω–æ–º—É –∫–æ–º–ø'—é—Ç–µ—Ä—ñ
- **–†–æ–∑–ø–æ–¥—ñ–ª–µ–Ω—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è**: –°—É–∫—É–ø–Ω—ñ—Å—Ç—å –≤—É–∑–ª—ñ–≤ (–º–µ—Ä–µ–∂–µ–≤–∏—Ö –∫–æ–º–ø'—é—Ç–µ—Ä—ñ–≤), —è–∫—ñ –ø—Ä–∞—Ü—é—é—Ç—å –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ
- **–ü–∞–∫–µ—Ç–Ω–∞ –æ–±—Ä–æ–±–∫–∞**: –†–æ–∑–±–∏—Ç—Ç—è –∑–∞–≤–¥–∞–Ω–Ω—è –Ω–∞ –Ω–µ–≤–µ–ª–∏–∫—ñ —á–∞—Å—Ç–∏–Ω–∏ —ñ –∑–∞–ø—É—Å–∫ —ó—Ö –Ω–∞ –æ–∫—Ä–µ–º–∏—Ö –º–∞—à–∏–Ω–∞—Ö
- **–û–±—Ä–æ–±–∫–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ**: –ù–µ–≥–∞–π–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –¥–∞–Ω–∏—Ö

## –°–∏—Å—Ç–µ–º–∏ –æ–±—Ä–æ–±–∫–∏ –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö {.smaller}

- **Hadoop/MapReduce**: –ú–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω–∏–π —Ç–∞ –≤—ñ–¥–º–æ–≤–æ—Å—Ç—ñ–π–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ Java
  + –í—ñ–¥–∫—Ä–∏—Ç–∏–π –≤–∏—Ö—ñ–¥–Ω–∏–π –∫–æ–¥
  + –ü–∞–∫–µ—Ç–Ω–∞ –æ–±—Ä–æ–±–∫–∞
- **Apache Spark**: –£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∞ —Ç–∞ —à–≤–∏–¥–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–Ω–∞ –æ–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω–∞ —Å–∏—Å—Ç–µ–º–∞
  + –í—ñ–¥–∫—Ä–∏—Ç–∏–π –≤–∏—Ö—ñ–¥–Ω–∏–π –∫–æ–¥
  + –ü–∞–∫–µ—Ç–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –¥–∞–Ω–∏—Ö —Ç–∞ –æ–±—Ä–æ–±–∫–∞ –¥–∞–Ω–∏—Ö —É —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ

::: {.callout-note}
Apache Spark –Ω–∞ —Å—å–æ–≥–æ–¥–Ω—ñ—à–Ω—ñ–π –¥–µ–Ω—å —î –∫—Ä–∞—â–∏–º –∑–∞ Hadoop/MapReduce —É –±—ñ–ª—å—à–æ—Å—Ç—ñ –≤–∏–ø–∞–¥–∫—ñ–≤, –æ—Å–∫—ñ–ª—å–∫–∏ –≤—ñ–Ω —à–≤–∏–¥—à–∏–π, –ø—Ä–æ—Å—Ç—ñ—à–∏–π —É –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—ñ —Ç–∞ –ø—ñ–¥—Ç—Ä–∏–º—É—î —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞ –¥–∞–Ω–∏—Ö.
:::

## –û—Å–æ–±–ª–∏–≤–æ—Å—Ç—ñ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫—É Apache Spark

- –§—Ä–µ–π–º–≤–æ—Ä–∫ —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–Ω–∏—Ö –æ–±—á–∏—Å–ª–µ–Ω—å
- –ï—Ñ–µ–∫—Ç–∏–≤–Ω—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –≤ –ø–∞–º'—è—Ç—ñ –¥–ª—è –≤–µ–ª–∏–∫–∏—Ö –º–∞—Å–∏–≤—ñ–≤ –¥–∞–Ω–∏—Ö
- –ë–ª–∏—Å–∫–∞–≤–∏—á–Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –æ–±—Ä–æ–±–∫–∏ –¥–∞–Ω–∏—Ö
- –ó–∞–±–µ–∑–ø–µ—á—É—î –ø—ñ–¥—Ç—Ä–∏–º–∫—É Java, Scala, Python, R —Ç–∞ SQL

## –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ Apache Spark

```{dot}
//| echo: false

digraph SparkArchitecture {
    rankdir=BT
    node [shape=box style=filled fontcolor=white width=2.5 height=1]

    Core [label="Apache Spark Core\nRDD API", fillcolor="#72A8EB", fontcolor=black]

    SQL [label="Spark SQL", fillcolor="#1A3A7A"]
    ML [label="MLlib\nMachine Learning", fillcolor="#1A3A7A"]
    GraphX [label="GraphX", fillcolor="#1A3A7A"]
    Streaming [label="Spark Streaming", fillcolor="#1A3A7A"]

    Core -> SQL
    Core -> ML
    Core -> GraphX
    Core -> Streaming
}
```

## –†–µ–∂–∏–º–∏ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è Spark

- **–õ–æ–∫–∞–ª—å–Ω–∏–π —Ä–µ–∂–∏–º**: –û–¥–Ω–∞ –º–∞—à–∏–Ω–∞, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, –≤–∞—à –Ω–æ—É—Ç–±—É–∫
  + –õ–æ–∫–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å –∑—Ä—É—á–Ω–∞ –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è, –Ω–∞–ª–∞–≥–æ–¥–∂–µ–Ω–Ω—è —Ç–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó
- **–ö–ª–∞—Å—Ç–µ—Ä–Ω–∏–π —Ä–µ–∂–∏–º**: –ù–∞–±—ñ—Ä –∑–∞–∑–¥–∞–ª–µ–≥—ñ–¥—å –≤–∏–∑–Ω–∞—á–µ–Ω–∏—Ö –º–∞—à–∏–Ω
  + –î–æ–±—Ä–µ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è —Ä–µ–∞–ª—å–Ω–∏—Ö —Å—Ü–µ–Ω–∞—Ä—ñ—ó–≤, –∫–æ–ª–∏ –ø–æ—Ç—Ä—ñ–±–Ω–æ –æ–±—Ä–æ–±–ª—è—Ç–∏ –≤–µ–ª–∏–∫—ñ –æ–±—Å—è–≥–∏ –¥–∞–Ω–∏—Ö
- –†–æ–±–æ—á–∏–π –ø—Ä–æ—Ü–µ—Å: –õ–æ–∫–∞–ª—å–Ω–æ $\rightarrow$ –∫–ª–∞—Å—Ç–µ—Ä–∏
- –ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑–º—ñ–Ω—é–≤–∞—Ç–∏ –∫–æ–¥

# PySpark: Spark –∑ Python {.unnumbered .unlisted}

## –û–≥–ª—è–¥ PySpark

- Apache Spark –Ω–∞–ø–∏—Å–∞–Ω–æ –Ω–∞ Scala
- –î–ª—è –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ Python –∑—ñ Spark, —Å–ø—ñ–ª—å–Ω–æ—Ç–∞ Apache Spark –≤–∏–ø—É—Å—Ç–∏–ª–∞ PySpark
- –ü–æ–¥—ñ–±–Ω–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å —Ç–∞ –ø–æ—Ç—É–∂–Ω—ñ—Å—Ç—å –æ–±—á–∏—Å–ª–µ–Ω—å –¥–æ Scala
- API PySpark —Å—Ö–æ–∂—ñ –Ω–∞ Pandas —Ç–∞ Scikit-learn

## –©–æ —Ç–∞–∫–µ –æ–±–æ–ª–æ–Ω–∫–∞ Spark?

- –Ü–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–µ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ –¥–ª—è –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∑–∞–≤–¥–∞–Ω—å Spark
- –î–æ–ø–æ–º–∞–≥–∞—î –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ —ñ–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ—Ç–æ—Ç–∏–ø—É–≤–∞–Ω–Ω—è
- –û–±–æ–ª–æ–Ω–∫–∏ Spark –¥–æ–∑–≤–æ–ª—è—é—Ç—å –≤–∑–∞—î–º–æ–¥—ñ—è—Ç–∏ –∑ –¥–∞–Ω–∏–º–∏ –Ω–∞ –¥–∏—Å–∫—É –∞–±–æ –≤ –ø–∞–º'—è—Ç—ñ
- –¢—Ä–∏ —Ä—ñ–∑–Ω—ñ –æ–±–æ–ª–æ–Ω–∫–∏ Spark:
  + Spark-–æ–±–æ–ª–æ–Ω–∫–∞ –¥–ª—è Scala
  + PySpark-–æ–±–æ–ª–æ–Ω–∫–∞ –¥–ª—è Python
  + SparkR –¥–ª—è R

## PySpark shell

- –û–±–æ–ª–æ–Ω–∫–∞ PySpark --- —Ü–µ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –∫–æ–º–∞–Ω–¥–Ω–æ–≥–æ —Ä—è–¥–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤—ñ Python
- –û–±–æ–ª–æ–Ω–∫–∞ PySpark –¥–æ–∑–≤–æ–ª—è—î –∞–Ω–∞–ª—ñ—Ç–∏–∫–∞–º –¥–∞–Ω–∏—Ö –≤–∑–∞—î–º–æ–¥—ñ—è—Ç–∏ –∑—ñ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º–∏ –¥–∞–Ω–∏—Ö Spark
- PySpark shell –ø—ñ–¥—Ç—Ä–∏–º—É—î –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞

## –†–æ–∑—É–º—ñ–Ω–Ω—è SparkContext

- SparkContext --- —Ü–µ —Ç–æ—á–∫–∞ –≤—Ö–æ–¥—É —É —Å–≤—ñ—Ç Spark
- –¢–æ—á–∫–∞ –≤—Ö–æ–¥—É --- —Ü–µ —Å–ø–æ—Å—ñ–± –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ Spark
- –¢–æ—á–∫–∞ –≤—Ö–æ–¥—É --- —Ü–µ —è–∫ –∫–ª—é—á –¥–æ –±—É–¥–∏–Ω–∫—É
- PySpark –º–∞—î SparkContext –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º, —è–∫–∏–π –Ω–∞–∑–∏–≤–∞—î—Ç—å—Å—è `sc`

## –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ SparkContext {.tiny}

- **–í–µ—Ä—Å—ñ—è**: –©–æ–± –æ—Ç—Ä–∏–º–∞—Ç–∏ –≤–µ—Ä—Å—ñ—é SparkContext

```{.yaml filename="Terminal"}
pyspark --version
```

```{yaml}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.5.5
      /_/

Using Python version 3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023 05:45:37)
Spark context Web UI available at http://AranaurPC:4040
Spark context available as 'sc' (master = local[*], app id = local-1745489304985).
SparkSession available as 'spark'.
```

```{python}
from pyspark import SparkContext

sc = SparkContext(master = 'local[*]')
print(sc.version)
```

- **–í–µ—Ä—Å—ñ—è Python**:

```{python}
print(sc.pythonVer)
```

- **–ú–∞–π—Å—Ç–µ—Ä**: URL –∫–ª–∞—Å—Ç–µ—Ä–∞ –∞–±–æ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ä—è–¥–∫–∞ –¥–ª—è –∑–∞–ø—É—Å–∫—É —É –ª–æ–∫–∞–ª—å–Ω–æ–º—É —Ä–µ–∂–∏–º—ñ SparkContext

```{python}
print(sc.master)
```



::: footer
[üîóPySpark RDD Cheat Sheet](https://media.datacamp.com/legacy/image/upload/v1676303379/Marketing/Blog/PySpark_RDD_Cheat_Sheet.pdf)
:::

## –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö —É PySpark

- –ú–µ—Ç–æ–¥ SparkContext `parallelize()`

```{python}
rdd = sc.parallelize([1,2,3,4,5])
```

- –ú–µ—Ç–æ–¥ `textFile()` SparkContext

```{python}
rdd = sc.textFile("data/data.txt")
```

# –§—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—å–Ω–µ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è —É Python {.unnumbered .unlisted}

## –©–æ —Ç–∞–∫–µ –∞–Ω–æ–Ω—ñ–º–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó –≤ Python?

- –õ—è–º–±–¥–∞-—Ñ—É–Ω–∫—Ü—ñ—ó --- –∞–Ω–æ–Ω—ñ–º–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó –≤ Python
- –î—É–∂–µ –ø–æ—Ç—É–∂–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è, —è–∫–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è —É Python. –î–æ—Å–∏—Ç—å –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ –∑ `map()` —Ç–∞ `filter()`
- –õ—è–º–±–¥–∞-—Ñ—É–Ω–∫—Ü—ñ—ó —Å—Ç–≤–æ—Ä—é—é—Ç—å —Ñ—É–Ω–∫—Ü—ñ—ó –¥–ª—è –ø–æ–¥–∞–ª—å—à–æ–≥–æ –≤–∏–∫–ª–∏–∫—É –ø–æ–¥—ñ–±–Ω–æ –¥–æ `def`
- –ü–æ–≤–µ—Ä—Ç–∞—î —Ñ—É–Ω–∫—Ü—ñ—ó –±–µ–∑ —ñ–º–µ–Ω—ñ (—Ç–æ–±—Ç–æ –∞–Ω–æ–Ω—ñ–º–Ω—ñ)

## –°–∏–Ω—Ç–∞–∫—Å–∏—Å –ª—è–º–±–¥–∞-—Ñ—É–Ω–∫—Ü—ñ—ó

- –ó–∞–≥–∞–ª—å–Ω–∏–π –≤–∏–≥–ª—è–¥ –ª—è–º–±–¥–∞-—Ñ—É–Ω–∫—Ü—ñ—ó –º–∞—î –≤–∏–≥–ª—è–¥

```{python}
#| eval: false
lambda arguments: expression
```

- –ü—Ä–∏–∫–ª–∞–¥ –ª—è–º–±–¥–∞-—Ñ—É–Ω–∫—Ü—ñ—ó

```{python}
double = lambda x: x * 2
print(double(3))
```

## –†—ñ–∑–Ω–∏—Ü—è –º—ñ–∂ —Ñ—É–Ω–∫—Ü—ñ—è–º–∏ `def` —Ç–∞ `lambda`

- –ö–æ–¥ –Ω–∞ Python –¥–ª—è —ñ–ª—é—Å—Ç—Ä–∞—Ü—ñ—ó –∫—É–±–∞ —á–∏—Å–ª–∞

```{python}
def cube(x):
  return x ** 3

g = lambda x: x ** 3

print(g(10))
print(cube(10))
```

- –ù–µ–º–∞—î –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ –ø–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è –¥–ª—è –ª—è–º–±–¥–∞-—Ñ—É–Ω–∫—Ü—ñ—ó
- –ú–æ–∂–Ω–∞ —Ä–æ–∑–º—ñ—â—É–≤–∞—Ç–∏ –ª—è–º–±–¥–∞-—Ñ—É–Ω–∫—Ü—ñ—é –±—É–¥—å-–¥–µ

## `map()`

- `map()` –∑–∞—Å—Ç–æ—Å–æ–≤—É—î —Ñ—É–Ω–∫—Ü—ñ—é –¥–æ –≤—Å—ñ—Ö –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ —É –≤—Ö—ñ–¥–Ω–æ–º—É —Å–ø–∏—Å–∫—É
- –ó–∞–≥–∞–ª—å–Ω–∏–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å `map()`

```{python}
#| eval: false

map(function, list)
```

- –ü—Ä–∏–∫–ª–∞–¥ `map()`

```{python}
items = [1, 2, 3, 4]
new_items = list(map(lambda x: x + 2 , items))
print(new_items)
```

## `filter()`

- —Ñ—É–Ω–∫—Ü—ñ—è `filter()` –æ—Ç—Ä–∏–º—É—î —Ñ—É–Ω–∫—Ü—ñ—é —Ç–∞ —Å–ø–∏—Å–æ–∫ —ñ –ø–æ–≤–µ—Ä—Ç–∞—î –Ω–æ–≤–∏–π —Å–ø–∏—Å–æ–∫, –¥–ª—è —è–∫–æ–≥–æ —Ñ—É–Ω–∫—Ü—ñ—è
–ø–æ–≤–µ—Ä—Ç–∞—î –∑–Ω–∞—á–µ–Ω–Ω—è `true`
- –ó–∞–≥–∞–ª—å–Ω–∏–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å `filter()`

```{python}
#| eval: false

filter(function, list)
```

- –ü—Ä–∏–∫–ª–∞–¥ `filter()`

```{python}
items = [1, 2, 3, 4]
new_items = list(filter(lambda x: (x%2 != 0), items))

print(new_items)
```

# –ü—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è –≤ PySpark RDD

## –©–æ —Ç–∞–∫–µ RDD?

- RDD (Resilient Distributed Datasets) --- —Ü–µ –æ—Å–Ω–æ–≤–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–∏—Ö —É Spark

```{dot}
//| echo: false
//| 
digraph RDDDistribution {
    node [shape=box style=filled fontname="Helvetica"]

    DataFile [label="–§–∞–π–ª –¥–∞–Ω–∏—Ö –Ω–∞ –¥–∏—Å–∫—É", style="filled,bold", fillcolor=white, color=black]
    Driver [label="–î—Ä–∞–π–≤–µ—Ä Spark —Å—Ç–≤–æ—Ä—é—î RDD\n –π —Ä–æ–∑–ø–æ–¥—ñ–ª—è—î –º—ñ–∂ –≤—É–∑–ª–∞–º–∏", style="filled,bold", fillcolor=white, color=black]

    subgraph cluster_0 {
        label="–ö–ª–∞—Å—Ç–µ—Ä"
        style="rounded,filled"
        color=gray
        fillcolor="#f0f8ff"
        
        Node1 [label="–†–æ–∑–¥—ñ–ª –≤—É–∑–ª–∞\nRDD 4", fillcolor=lightblue, style=filled]
        Node2 [label="–†–æ–∑–¥—ñ–ª –≤—É–∑–ª–∞\nRDD 3", fillcolor=lightblue, style=filled]
        Node3 [label="–†–æ–∑–¥—ñ–ª –≤—É–∑–ª–∞\nRDD 2", fillcolor=lightblue, style=filled]
        Node4 [label="–†–æ–∑–¥—ñ–ª –≤—É–∑–ª–∞\nRDD 1", fillcolor=lightblue, style=filled]
    }

    DataFile -> Driver
    Driver -> Node1
    Driver -> Node2
    Driver -> Node3
    Driver -> Node4
}
```

## –î–µ–∫–æ–º–ø–æ–∑–∏—Ü—ñ—è RDD

- –°—Ç—ñ–π–∫—ñ —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω—ñ –Ω–∞–±–æ—Ä–∏ –¥–∞–Ω–∏—Ö
  + **–í—ñ–¥–º–æ–≤–æ—Å—Ç—ñ–π–∫—ñ—Å—Ç—å**: –ó–¥–∞—Ç–Ω—ñ—Å—Ç—å –ø—Ä–æ—Ç–∏—Å—Ç–æ—è—Ç–∏ –∑–±–æ—è–º
  + **–†–æ–∑–ø–æ–¥—ñ–ª–µ–Ω—ñ**: –†–æ–∑–ø–æ–¥—ñ–ª–µ–Ω—ñ –º—ñ–∂ –¥–µ–∫—ñ–ª—å–∫–æ–º–∞ –º–∞—à–∏–Ω–∞–º–∏
  + **–ù–∞–±–æ—Ä–∏ –¥–∞–Ω–∏—Ö**: –ö–æ–ª–µ–∫—Ü—ñ—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–∏—Ö –¥–∞–Ω–∏—Ö, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, –º–∞—Å–∏–≤—ñ–≤, —Ç–∞–±–ª–∏—Ü—å, –∫–æ—Ä—Ç–µ–∂—ñ–≤ —Ç–æ—â–æ.

## –°—Ç–≤–æ—Ä–µ–Ω–Ω—è RDD. –Ø–∫ —Ü–µ –∑—Ä–æ–±–∏—Ç–∏?

- –†–æ–∑–ø–∞—Ä–∞–ª–µ–ª—é–≤–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–æ—ó –∫–æ–ª–µ–∫—Ü—ñ—ó –æ–±'—î–∫—Ç—ñ–≤
- –ó–æ–≤–Ω—ñ—à–Ω—ñ –Ω–∞–±–æ—Ä–∏ –¥–∞–Ω–∏—Ö:
  + –§–∞–π–ª–∏ –≤ HDFS
  + –û–±'—î–∫—Ç–∏ –≤ Amazon S3 bucket
  + —Ä—è–¥–∫–∏ –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É —Ñ–∞–π–ª—ñ
- –ó —ñ—Å–Ω—É—é—á–∏—Ö RDD

## –†–æ–∑–ø–∞—Ä–∞–ª–µ–ª–µ–Ω–∏–π –∑–±—ñ—Ä (—Ä–æ–∑–ø–∞—Ä–∞–ª–µ–ª—é–≤–∞–Ω–Ω—è)

- `parallelize()` –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è RDD –∑—ñ —Å–ø–∏—Å–∫—ñ–≤ –Ω–∞ Python

```{python}
numRDD = sc.parallelize([1,2,3,4])

helloRDD = sc.parallelize("Hello world")

type(helloRDD)
```

## –Ü–∑ –∑–æ–≤–Ω—ñ—à–Ω—ñ—Ö –Ω–∞–±–æ—Ä—ñ–≤ –¥–∞–Ω–∏—Ö

- `textFile()` –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è RDD —ñ–∑ –∑–æ–≤–Ω—ñ—à–Ω—ñ—Ö –Ω–∞–±–æ—Ä—ñ–≤ –¥–∞–Ω–∏—Ö

```{python}
fileRDD = sc.textFile("data/data.txt")

type(fileRDD)
```

## –†–æ–∑—É–º—ñ–Ω–Ω—è –ø–∞—Ä—Ç–∏—Ü—ñ–π —É PySpark

- –ü–∞—Ä—Ç–∏—Ü—ñ—è (Partition) --- —Ü–µ –ª–æ–≥—ñ—á–Ω–∏–π –ø–æ–¥—ñ–ª –≤–µ–ª–∏–∫–æ–≥–æ —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–æ–≥–æ –Ω–∞–±–æ—Ä—É –¥–∞–Ω–∏—Ö

- –º–µ—Ç–æ–¥ `parallelize()`

```{python}
numRDD = sc.parallelize(range(10), numSlices = 6)
```

- –º–µ—Ç–æ–¥ `textFile()`

```{python}
#| eval: false
fileRDD = sc.textFile("README.md", numSlices = 6)
```

–ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–∞—Ä—Ç–∏—Ü—ñ–π –≤ RDD –º–æ–∂–Ω–∞ –¥—ñ–∑–Ω–∞—Ç–∏—Å—è –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –º–µ—Ç–æ–¥—É `getNumPartitions()`

# RDD operations in PySpark {.unnumbered .unlisted}

## –û–≥–ª—è–¥ PySpark operations {.smaller}

```{dot}
//| echo: false

digraph SparkOperationsEmoji {
    node [shape=box style=filled fontname="Helvetica"]

    Transformations [label="üêõ ‚û°Ô∏è üêõ ‚û°Ô∏è üêõ ‚û°Ô∏è ü¶ã Transformations", fillcolor="#c0f7a9"]
    Actions [label="üñ®Ô∏è Actions", fillcolor="#b3e5fc"]
    Operations [label="Spark üß† Operations", fillcolor="#ffe066", style="filled,bold", color=black]

    Transformations -> Operations
    Actions -> Operations
}
```

- –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å—Ç–≤–æ—Ä—é—é—Ç—å –Ω–æ–≤—ñ RDD
- –î—ñ—ó –≤–∏–∫–æ–Ω—É—é—Ç—å –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –Ω–∞ RDD

## RDD Transformations {.tiny}

- –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó –≤–∏–∫–æ–Ω—É—é—Ç—å –ª—ñ–Ω–∏–≤—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è
- –ë–∞–∑–æ–≤—ñ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è RDD
  + `map()`
  + `filter()`
  + `flatMap()`
  + `union()`

```{dot}
//| echo: false

digraph RDDPipeline {
    rankdir=LR;
    node [shape=box, style=filled, fillcolor=lightgoldenrod];

    Storage [shape=cylinder, fillcolor=lightgreen, label="–°—Ö–æ–≤–∏—â–µ"];
    RDD1 [label="RDD1"];
    RDD2 [label="RDD2"];
    RDD3 [label="RDD3"];
    Result [shape=ellipse, fillcolor=white, fontcolor=blue, label="–†–µ–∑—É–ª—å—Ç–∞—Ç"];

    Storage -> RDD1 [label="Rdd —Å—Ç–≤–æ—Ä—é—î—Ç—å—Å—è —à–ª—è—Ö–æ–º\n—á–∏—Ç–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö\n–∑—ñ —Å—Ö–æ–≤–∏—â–∞"];
    RDD1 -> RDD2 [label="transformation"];
    RDD2 -> RDD3 [label="transformation"];
    RDD3 -> Result [label="action"];
}
```

## `map()` {.tiny}

- –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è `map()` –∑–∞—Å—Ç–æ—Å–æ–≤—É—î —Ñ—É–Ω–∫—Ü—ñ—é –¥–æ –≤—Å—ñ—Ö –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ —É RDD

```{dot}
//| echo: false

digraph MapTransformation {
    rankdir=LR;
    node [shape=ellipse, style=filled, fillcolor=deepskyblue, fontcolor=white];

    // Input nodes
    A1 [label="1"];
    A2 [label="2"];
    A3 [label="3"];
    A4 [label="4"];

    // Map operation
    MAP [shape=circle, label="map\nx * x", fillcolor=dodgerblue];

    // Output nodes
    B1 [label="1"];
    B2 [label="4"];
    B3 [label="9"];
    B4 [label="16"];

    // Connections
    A1 -> MAP -> B1;
    A2 -> MAP -> B2;
    A3 -> MAP -> B3;
    A4 -> MAP -> B4;
}
```

```{python}
RDD = sc.parallelize([1,2,3,4])
RDD_map = RDD.map(lambda x: x * x)
```

## `filter()` {.tiny}

- `filter()` –ø–æ–≤–µ—Ä—Ç–∞—î –Ω–æ–≤–∏–π RDD –∑ –µ–ª–µ–º–µ–Ω—Ç–∞–º–∏, —è–∫—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—é—Ç—å —É–º–æ–≤—ñ

```{dot}
//| echo: false

digraph FilterTransformation {
    rankdir=LR;
    node [shape=ellipse, style=filled, fillcolor=deepskyblue, fontcolor=white];

    // Input
    A1 [label="1"];
    A2 [label="2"];
    A3 [label="3"];
    A4 [label="4"];

    // Operation
    FILTER [shape=box, label="filter\nx : x > 2", fillcolor=dodgerblue];

    // Output
    B3 [label="3"];
    B4 [label="4"];

    // Connections
    A1 -> FILTER;
    A2 -> FILTER;
    A3 -> FILTER -> B3;
    A4 -> FILTER -> B4;
}
```

```{python}
RDD = sc.parallelize([1,2,3,4])
RDD_filter = RDD.filter(lambda x: x > 2)
```

## `flatMap()` {.tiny}

- `flatMap()` –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î RDD —É –Ω–æ–≤–∏–π RDD, –¥–µ –∫–æ–∂–µ–Ω –µ–ª–µ–º–µ–Ω—Ç –º–æ–∂–µ –±—É—Ç–∏ —Ä–æ–∑–≥–æ—Ä–Ω—É—Ç–∏–π —É –∫—ñ–ª—å–∫–∞ –µ–ª–µ–º–µ–Ω—Ç—ñ–≤

```{dot}
//| echo: false

digraph FlatMapTransformation {
    rankdir=TD;
    node [shape=box, style=filled, fillcolor=deepskyblue, fontcolor=white];

    // Input
    A1 [label="['Hello world', 'How are you?']"];
    // Operation
    FLATMAP [shape=box, label="flatMap\nx : x.split(' ')", fillcolor=green];

    // Output
    B1 [label="['Hello', 'world', 'How', 'are', 'you?']"];
    // Connections
    A1 -> FLATMAP -> B1;
}
```

```{python}
RDD = sc.parallelize(["hello world", "how are you"])
RDD_flatmap = RDD.flatMap(lambda x: x.split(" "))
```

## `union()` {.tiny}

- `union()` –æ–±'—î–¥–Ω—É—î –¥–≤–∞ RDD –≤ –æ–¥–∏–Ω

```{dot}
//| echo: false

digraph RDD_Pipeline {
    node [fontname="Helvetica", style=filled, fontcolor=white]

    inputRDD [label="inputRDD", shape=box, fillcolor="#2980b9"]
    errorsRDD [label="errorsRDD", shape=box, fillcolor="#2980b9"]
    warningsRDD [label="warningsRDD", shape=box, fillcolor="#2980b9"]
    badlinesRDD [label="badlinesRDD", shape=box, fillcolor="#2980b9"]

    filter1 [label="Filter", shape=rect, fillcolor="#a93226"]
    filter2 [label="Filter", shape=rect, fillcolor="#a93226"]
    union [label="Union", shape=rect, fillcolor="#a93226"]

    inputRDD -> filter1 -> errorsRDD
    inputRDD -> filter2 -> warningsRDD
    errorsRDD -> union -> badlinesRDD
    warningsRDD -> union
}
```

```{python}
#| eval: false
inputRDD = sc.textFile("logs.txt")
errorRDD = inputRDD.filter(lambda x: "error" in x.split())
warningsRDD = inputRDD.filter(lambda x: "warnings" in x.split())
combinedRDD = errorRDD.union(warningsRDD)
```


## RDD Actions

–¶–µ –æ–ø–µ—Ä–∞—Ü—ñ—ó, —è–∫—ñ –ø–æ–≤–µ—Ä—Ç–∞—é—Ç—å –∑–Ω–∞—á–µ–Ω–Ω—è –ø—ñ—Å–ª—è –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –æ–±—á–∏—Å–ª–µ–Ω—å –Ω–∞ RDD

- –ë–∞–∑–æ–≤—ñ –¥—ñ—ó –∑ RDD
  + `collect()`
  + `take(N)`
  + `first()`
  + `count()`

## `collect()` —Ç–∞ `take()`

- `collect()` –ø–æ–≤–µ—Ä—Ç–∞—î –≤—Å—ñ –µ–ª–µ–º–µ–Ω—Ç–∏ –Ω–∞–±–æ—Ä—É –¥–∞–Ω–∏—Ö —É –≤–∏–≥–ª—è–¥—ñ –º–∞—Å–∏–≤—É
- `take(N)` –ø–æ–≤–µ—Ä—Ç–∞—î –º–∞—Å–∏–≤ –ø–µ—Ä—à–∏—Ö `N` –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ –Ω–∞–±–æ—Ä—É –¥–∞–Ω–∏—Ö

```{python}
RDD_map.collect()
```

```{python}
RDD_map.take(2)
```

## `first()` —Ç–∞ `count()`

- `first()` –≤–∏–≤–æ–¥–∏—Ç—å –ø–µ—Ä—à–∏–π –µ–ª–µ–º–µ–Ω—Ç RDD

```{python}
RDD_map.first()
```

- `count()` –ø–æ–≤–µ—Ä—Ç–∞—î –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ —É RDD

```{python}
RDD_flatmap.count()
```

# –†–æ–±–æ—Ç–∞ –∑ Pair RDD –≤ PySpark {.unnumbered .unlisted}

## –í—Å—Ç—É–ø –¥–æ Pair RDD —É PySpark

- –†–µ–∞–ª—å–Ω—ñ –Ω–∞–±–æ—Ä–∏ –¥–∞–Ω–∏—Ö –∑–∞–∑–≤–∏—á–∞–π —î –ø–∞—Ä–∞–º–∏ –∫–ª—é—á/–∑–Ω–∞—á–µ–Ω–Ω—è
- –ö–æ–∂–µ–Ω —Ä—è–¥–æ–∫ —î –∫–ª—é—á–µ–º —ñ —Å–ø—ñ–≤–≤—ñ–¥–Ω–æ—Å–∏—Ç—å—Å—è –∑ –æ–¥–Ω–∏–º –∞–±–æ –¥–µ–∫—ñ–ª—å–∫–æ–º–∞ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏
- Pair RDD --- —Ü–µ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–∏—Ö –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ —Ç–∞–∫–∏–º–∏ –Ω–∞–±–æ—Ä–∞–º–∏ –¥–∞–Ω–∏—Ö
- Pair RDD: –∫–ª—é—á --- —Ü–µ —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä, –∞ –∑–Ω–∞—á–µ–Ω–Ω—è --- —Ü–µ –¥–∞–Ω—ñ

## –°—Ç–≤–æ—Ä–µ–Ω–Ω—è Pair RDD {.smaller}

- –î–≤–∞ –Ω–∞–π–ø–æ—à–∏—Ä–µ–Ω—ñ—à—ñ —Å–ø–æ—Å–æ–±–∏ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–∞—Ä–Ω–∏—Ö RDD
  + –ó—ñ —Å–ø–∏—Å–∫—É –∫–æ—Ä—Ç–µ–∂—É –∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–Ω—è
  + –ó—ñ –∑–≤–∏—á–∞–π–Ω–æ–≥–æ RDD
- –û—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö —É —Ñ–æ—Ä–º—ñ –∫–ª—é—á/–∑–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è –ø–∞—Ä–Ω–æ–≥–æ RDD

```{python}
my_tuple = [('Sam', 23), ('Mary', 34), ('Peter', 25)]
pairRDD_tuple = sc.parallelize(my_tuple)
```

\

```{python}
my_list = ['Sam 23', 'Mary 34', 'Peter 25']
regularRDD = sc.parallelize(my_list)
pairRDD_RDD = regularRDD.map(lambda s: (s.split(' ')[0], s.split(' ')[1]))
```

## –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è Pair RDD

- –í—Å—ñ —Ä–µ–≥—É–ª—è—Ä–Ω—ñ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø—Ä–∞—Ü—é—é—Ç—å –Ω–∞ –ø–∞—Ä—ñ RDD
- –ü–æ—Ç—Ä—ñ–±–Ω–æ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç–∏ —Ñ—É–Ω–∫—Ü—ñ—ó, —è–∫—ñ –æ–ø–µ—Ä—É—é—Ç—å –ø–∞—Ä–∞–º–∏ –∑–Ω–∞—á–µ–Ω—å –∫–ª—é—á—ñ–≤, –∞ –Ω–µ –æ–∫—Ä–µ–º–∏–º–∏ –µ–ª–µ–º–µ–Ω—Ç–∞–º–∏
- –ü—Ä–∏–∫–ª–∞–¥–∏ –ø–∞—Ä–Ω–∏—Ö –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω—å RDD
  + `reduceByKey(func)`: –û–±'—î–¥–Ω—É—î –∑–Ω–∞—á–µ–Ω–Ω—è –∑ –æ–¥–Ω–∞–∫–æ–≤–∏–º –∫–ª—é—á–µ–º
  + `groupByKey()`: –ó–≥—Ä—É–ø—É–≤–∞—Ç–∏ –∑–Ω–∞—á–µ–Ω–Ω—è –∑ –æ–¥–Ω–∞–∫–æ–≤–∏–º –∫–ª—é—á–µ–º
  + `sortByKey()`: –ü–æ–≤–µ—Ä–Ω—É—Ç–∏ RDD, –≤—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω–∏–π –∑–∞ –∫–ª—é—á–µ–º
  + `join()`: –û–±'—î–¥–Ω–∞—Ç–∏ –¥–≤—ñ –ø–∞—Ä–∏ RDD –Ω–∞ –æ—Å–Ω–æ–≤—ñ —ó—Ö –∫–ª—é—á–∞

## `reduceByKey(func)`

- `reduceByKey(func)` –æ–±'—î–¥–Ω—É—î –∑–Ω–∞—á–µ–Ω–Ω—è –∑ –æ–¥–Ω–∞–∫–æ–≤–∏–º –∫–ª—é—á–µ–º
- –í—ñ–Ω –≤–∏–∫–æ–Ω—É—î –ø–∞—Ä–∞–ª–µ–ª—å–Ω—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–ª—é—á–∞ –≤ –Ω–∞–±–æ—Ä—ñ –¥–∞–Ω–∏—Ö
- –¶–µ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è, –∞ –Ω–µ –¥—ñ—è

```{python}
regularRDD = sc.parallelize([("Messi", 23), ("Ronaldo", 34),
                              ("Neymar", 22), ("Messi", 24)])

pairRDD_reducebykey = regularRDD.reduceByKey(lambda x,y : x + y)
pairRDD_reducebykey.collect()
```

## `sortByKey()`

- `sortByKey()` –ø–æ–≤–µ—Ä—Ç–∞—î RDD, –≤—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω–∏–π –∑–∞ –∫–ª—é—á–µ–º

```{python}
pairRDD_reducebykey_rev = pairRDD_reducebykey.map(lambda x: (x[1], x[0]))

pairRDD_reducebykey_rev.sortByKey(ascending=False).collect()
```

## `groupByKey()`

- `groupByKey()` –∑–≥—Ä—É–ø–æ–≤—É—î –∑–Ω–∞—á–µ–Ω–Ω—è –∑ –æ–¥–Ω–∞–∫–æ–≤–∏–º –∫–ª—é—á–µ–º

```{python}
airports = [("US", "JFK"),("UK", "LHR"),("FR", "CDG"),("US", "SFO")]

regularRDD = sc.parallelize(airports)
pairRDD_group = regularRDD.groupByKey().collect()

for cont, air in pairRDD_group:
  print(cont, list(air))
```

## `join()`

- `join()` –æ–±'—î–¥–Ω—É—î –¥–≤—ñ –ø–∞—Ä–∏ RDD –Ω–∞ –æ—Å–Ω–æ–≤—ñ —ó—Ö –∫–ª—é—á–∞

```{python}
RDD1 = sc.parallelize([("Messi", 34),("Ronaldo", 32),("Neymar", 24)])
RDD2 = sc.parallelize([("Ronaldo", 80),("Neymar", 120),("Messi", 100)])

RDD1.join(RDD2).collect()
```

# –î–æ–¥–∞—Ç–∫–æ–≤—ñ –¥—ñ—ó (actions) {.unnumbered .unlisted}

## `reduce()`

- `reduce()` –æ–±'—î–¥–Ω—É—î –≤—Å—ñ –µ–ª–µ–º–µ–Ω—Ç–∏ RDD –≤ –æ–¥–∏–Ω
- –§—É–Ω–∫—Ü—ñ—è –ø–æ–≤–∏–Ω–Ω–∞ –±—É—Ç–∏ –∫–æ–º—É—Ç–∞—Ç–∏–≤–Ω–æ—é (–∑–º—ñ–Ω–∞ –ø–æ—Ä—è–¥–∫—É –æ–ø–µ—Ä–∞–Ω–¥—ñ–≤ –Ω–µ –∑–º—ñ–Ω—é—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç) —Ç–∞ –∞—Å–æ—Ü—ñ–∞—Ç–∏–≤–Ω–æ—é

```{python}
x = [1,3,4,6]
RDD = sc.parallelize(x)
RDD.reduce(lambda x, y : x + y)
```

## `saveAsTextFile()`

- `saveAsTextFile()` –∑–±–µ—Ä—ñ–≥–∞—î RDD —É —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É —Ñ–∞–π–ª—ñ

```{python}
#| eval: false

RDD.saveAsTextFile("tempFile")
```

- `coalesce()` –º–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è RDD —É –≤–∏–≥–ª—è–¥—ñ –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª—É

```{python}
#| eval: false

RDD.coalesce(1).saveAsTextFile("tempFile")
```

## Action –æ–ø–µ—Ä–∞—Ü—ñ—ó –Ω–∞–¥ –ø–∞—Ä–∞–º–∏ RDD

- Action RDD, –¥–æ—Å—Ç—É–ø–Ω—ñ –¥–ª—è –ø–∞—Ä–Ω–∏—Ö RDD —É PySpark
- –ü–∞—Ä–Ω—ñ –¥—ñ—ó RDD –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å –¥–∞–Ω—ñ –∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–Ω—è
- –ö—ñ–ª—å–∫–∞ –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –¥—ñ–π –ø–∞—Ä–Ω–æ–≥–æ RDD –≤–∫–ª—é—á–∞—é—Ç—å
  + `countByKey()`
  + `collectAsMap()`

## `countByKey()`

- `countByKey()` –¥–æ—Å—Ç—É–ø–Ω–∏–π —Ç—ñ–ª—å–∫–∏ –¥–ª—è —Ç–∏–ø—É Pair RDD
- `countByKey()` –ø—ñ–¥—Ä–∞—Ö–æ–≤—É—î –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–ª—é—á–∞

```{python}
rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])

for kee, val in rdd.countByKey().items():
  print(kee, val)
```

## `collectAsMap()`

- `collectAsMap()` –ø–æ–≤–µ—Ä–Ω—É—Ç–∏ –ø–∞—Ä–∏ –∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–Ω—è –≤ RDD —É –≤–∏–≥–ª—è–¥—ñ —Å–ª–æ–≤–Ω–∏–∫–∞

```{python}
sc.parallelize([(1, 2), (3, 4)]).collectAsMap()
```

## PySpark SQL —Ç–∞ DataFrames

## PySpark DataFrames

- PySpark SQL --- —Ü–µ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ Spark –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö. –í–æ–Ω–∞ –Ω–∞–¥–∞—î –±—ñ–ª—å—à–µ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ
—Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–∏—Ö —Ç–∞ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è
- PySpark DataFrame --- –Ω–µ–∑–º—ñ–Ω–Ω–∞ —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–∞ –∫–æ–ª–µ–∫—Ü—ñ—è –¥–∞–Ω–∏—Ö –∑ —ñ–º–µ–Ω–æ–≤–∞–Ω–∏–º–∏ —Å—Ç–æ–≤–ø—Ü—è–º–∏
- –ü—Ä–∏–∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –æ–±—Ä–æ–±–∫–∏ —è–∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏—Ö (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, —Ä–µ–ª—è—Ü—ñ–π–Ω–∞ –±–∞–∑–∞ –¥–∞–Ω–∏—Ö), —Ç–∞–∫ —ñ –Ω–∞–ø—ñ–≤—Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, JSON)
- API —Ñ—Ä–µ–π–º—ñ–≤ –¥–∞–Ω–∏—Ö –¥–æ—Å—Ç—É–ø–Ω–∏–π —É Python, R, Scala —Ç–∞ Java
- DataFrames –≤ PySpark –ø—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å —è–∫ SQL –∑–∞–ø–∏—Ç–∏ ( `SELECT * from table` ), —Ç–∞–∫ —ñ –≤–∏—Ä–∞–∑–∏ –º–µ—Ç–æ–¥–∏ ( `df.select()` )

## SparkSession --- —Ç–æ—á–∫–∞ –≤—Ö–æ–¥—É –¥–ª—è API DataFrame

- SparkContext —î –æ—Å–Ω–æ–≤–Ω–æ—é —Ç–æ—á–∫–æ—é –≤—Ö–æ–¥—É –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è RDD
- SparkSession –∑–∞–±–µ–∑–ø–µ—á—É—î —î–¥–∏–Ω—É —Ç–æ—á–∫—É –≤—Ö–æ–¥—É –¥–ª—è –≤–∑–∞—î–º–æ–¥—ñ—ó –∑ —Ñ—Ä–µ–π–º–∞–º–∏ –¥–∞–Ω–∏—Ö Spark
- SparkSession –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è DataFrame, —Ä–µ—î—Å—Ç—Ä–∞—Ü—ñ—ó DataFrame, –≤–∏–∫–æ–Ω–∞–Ω–Ω—è SQL –∑–∞–ø–∏—Ç—ñ–≤
- SparkSession –¥–æ—Å—Ç—É–ø–Ω–∏–π –≤ –æ–±–æ–ª–æ–Ω—Ü—ñ PySpark —è–∫ spark

## –°—Ç–≤–æ—Ä–µ–Ω–Ω—è DataFrame

- –î–≤–∞ —Ä—ñ–∑–Ω—ñ –º–µ—Ç–æ–¥–∏ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è DataFrame –≤ PySpark
  + –ó —ñ—Å–Ω—É—é—á–∏—Ö RDD –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –º–µ—Ç–æ–¥—É `createDataFrame()` SparkSession
  + –ó —Ä—ñ–∑–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª –¥–∞–Ω–∏—Ö (CSV, JSON, TXT) –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –º–µ—Ç–æ–¥—É `read()` SparkSession
- –°—Ö–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—é—î –¥–∞–Ω—ñ —ñ –¥–æ–ø–æ–º–∞–≥–∞—î DataFrames –æ–ø—Ç–∏–º—ñ–∑—É–≤–∞—Ç–∏ –∑–∞–ø–∏—Ç–∏
- –°—Ö–µ–º–∞ –Ω–∞–¥–∞—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –Ω–∞–∑–≤—É —Å—Ç–æ–≤–ø—Ü—è, —Ç–∏–ø –¥–∞–Ω–∏—Ö —É —Å—Ç–æ–≤–ø—Ü—ñ, –ø–æ—Ä–æ–∂–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è —Ç–æ—â–æ.

## –°—Ç–≤–æ—Ä–µ–Ω–Ω—è DataFrame

```{python}
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("DataFrame").getOrCreate()

iphones_RDD = sc.parallelize([
  ("XS", 2018, 5.65, 2.79, 6.24),
  ("XR", 2018, 5.94, 2.98, 6.84),
  ("X10", 2017, 5.65, 2.79, 6.13),
  ("8Plus", 2017, 6.23, 3.07, 7.12)
])

names = ['Model', 'Year', 'Height', 'Width', 'Weight']

iphones_df = spark.createDataFrame(iphones_RDD, schema=names)
type(iphones_df)
```

## DataFrame –∑ CSV/JSON/TXT

```{python}
#| eval: false
df_csv = spark.read.csv("people.csv", header=True, inferSchema=True)

df_json = spark.read.json("people.json")

df_txt = spark.read.txt("people.txt")
```

- –®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É —Ç–∞ –¥–≤–∞ –Ω–µ–æ–±–æ–≤'—è–∑–∫–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
- –î–≤–∞ –Ω–µ–æ–±–æ–≤'—è–∑–∫–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
  + `header=True`
  + `inferSchema=True`

# –í–∑–∞—î–º–æ–¥—ñ—è –∑ DataFrame {.unnumbered .unlisted}

## –û–ø–µ—Ä–∞—Ç–æ—Ä–∏ DataFrame —É PySpark {.tiny}

- –û–ø–µ—Ä–∞—Ü—ñ—ó –∑ DataFrame: –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–∞ –¥—ñ—ó

:::: {.columns}

::: {.column width="50%"}
- –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ñ—Ä–µ–π–º—É –¥–∞–Ω–∏—Ö:
  + `select()`
  + `filter()`
  + `groupby()`
  + `orderby()`
  + `dropDuplicates()`
  + `withColumnRenamed()`
:::

::: {.column width="50%"}
- –î—ñ—ó –∑ —Ñ—Ä–µ–π–º–æ–º –¥–∞–Ω–∏—Ö:
  + `head()`
  + `show()`
  + `count()`
  + `columns`
  + `describe()`
:::

::::

::: {.callout-note}
`printSchema()` —î –º–µ—Ç–æ–¥–æ–º –¥–ª—è –±—É–¥—å-—è–∫–æ–≥–æ –Ω–∞–±–æ—Ä—É –¥–∞–Ω–∏—Ö/—Ñ—Ä–µ–π–º—É –¥–∞–Ω–∏—Ö Spark
:::

## `select()` —Ç–∞ `show()` {.smaller}

- `select()` –≤–∏–±–∏—Ä–∞—î —Å—Ç–æ–≤–ø—Ü—ñ –∑ DataFrame
- `show()` –≤–∏–≤–æ–¥–∏—Ç—å –ø–µ—Ä—à—ñ —Ä—è–¥–∫–∏ DataFrame

```{python}
from pyspark.sql import SparkSession
from datetime import date as sysdate

spark = SparkSession.builder.appName("DataFrame").getOrCreate()

people = spark.read.csv("data/people.csv", header=True, inferSchema=True)

people.select('name').show(5)
```

## `filter()`

- `filter()` –≤–∏–±–∏—Ä–∞—î —Ä—è–¥–∫–∏ –∑ DataFrame, —è–∫—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—é—Ç—å —É–º–æ–≤—ñ

```{python}
people.filter(people.sex == 'female').show(5)
```

## `groupby()` —Ç–∞ `count()`

```{python}
people.groupby('sex').count().show()
```

## `orderby()`

```{python}
people.orderBy('date of birth').show(5)
```

## `dropDuplicates()`

```{python}
print(people.select('name').count())

print(people.select('name').dropDuplicates().count())
```

## `withColumnRenamed()`

- `withColumnRenamed()` –ø–µ—Ä–µ–π–º–µ–Ω–æ–≤—É—î —Å—Ç–æ–≤–ø–µ—Ü—å —É DataFrame

```{python}
people = people.withColumnRenamed("date of birth", "dob")
people.show(5)
```

## `printSchema()`

```{python}
people.printSchema()
```

## `columns`

- `columns` –ø–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–≤–ø—Ü—ñ–≤ —É DataFrame

```{python}
people.columns
```

## `describe()`

- `describe()` –ø–æ–≤–µ—Ä—Ç–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è —á–∏—Å–ª–æ–≤–∏—Ö —Å—Ç–æ–≤–ø—Ü—ñ–≤ —É DataFrame

```{python}
people.describe().show()
```

# PySpark SQL

## DataFrame API vs SQL –∑–∞–ø–∏—Ç–∏

- –£ PySpark –≤–∏ –º–æ–∂–µ—Ç–µ –≤–∑–∞—î–º–æ–¥—ñ—è—Ç–∏ –∑ SparkSQL —á–µ—Ä–µ–∑ DataFrame API —Ç–∞ SQL –∑–∞–ø–∏—Ç–∏
- DataFrame API –Ω–∞–¥–∞—î –ø—Ä–æ–≥—Ä–∞–º–Ω—É –º–æ–≤—É, —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—É –¥–ª—è –¥–æ–º–µ–Ω—É (DSL) –¥–ª—è –¥–∞–Ω–∏—Ö
- –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–∞ –¥—ñ—ó DataFrame –ª–µ–≥—à–µ –∫–æ–Ω—Å—Ç—Ä—É—é–≤–∞—Ç–∏ –ø—Ä–æ–≥—Ä–∞–º–Ω–æ
- SQL-–∑–∞–ø–∏—Ç–∏ –º–æ–∂—É—Ç—å –±—É—Ç–∏ —Å—Ç–∏—Å–ª–∏–º–∏, –ø—Ä–æ—Å—Ç—ñ—à–∏–º–∏ –¥–ª—è —Ä–æ–∑—É–º—ñ–Ω–Ω—è —Ç–∞ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–Ω—è
- –û–ø–µ—Ä–∞—Ü—ñ—ó –Ω–∞–¥ DataFrame —Ç–∞–∫–æ–∂ –º–æ–∂–Ω–∞ –≤–∏–∫–æ–Ω—É–≤–∞—Ç–∏ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é SQL –∑–∞–ø–∏—Ç—ñ–≤

## SQL –∑–∞–ø–∏—Ç–∏

- –ú–µ—Ç–æ–¥ SparkSession `sql()` –≤–∏–∫–æ–Ω—É—î SQL-–∑–∞–ø–∏—Ç
- –ú–µ—Ç–æ–¥ `sql()` –æ—Ç—Ä–∏–º—É—î SQL-–∑–∞–ø–∏—Ç —è–∫ –∞—Ä–≥—É–º–µ–Ω—Ç —ñ –ø–æ–≤–µ—Ä—Ç–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É –≤–∏–≥–ª—è–¥—ñ DataFrame

```{python}
people.createOrReplaceTempView("people")
df_sql = spark.sql("SELECT * FROM people LIMIT 5")
df_sql.show()
```

## SQL-–∑–∞–ø–∏—Ç

```{python}
query = '''SELECT name FROM people LIMIT 5'''
df_sql = spark.sql(query)
df_sql.show()
```

## –ì—Ä—É–ø—É–≤–∞–Ω–Ω—è —Ç–∞ –∞–≥—Ä–µ–≥–∞—Ü—ñ—è

```{python}
query = '''SELECT sex, COUNT(*) as count FROM people GROUP BY sex'''
df_sql = spark.sql(query)
df_sql.show()
```

## –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è

```{python}
query = '''SELECT * FROM people WHERE dob > '2000-01-01' '''
df_sql = spark.sql(query)
df_sql.show(5)
```

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö —É PySpark {.unnumbered .unlisted}

## –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö —É PySpark

–ü–æ–±—É–¥–æ–≤–∞ –≥—Ä–∞—Ñ—ñ–∫—ñ–≤ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é —Ñ—Ä–µ–π–º—ñ–≤ –¥–∞–Ω–∏—Ö PySpark –≤–∏–∫–æ–Ω—É—î—Ç—å—Å—è —Ç—Ä—å–æ–º–∞ –º–µ—Ç–æ–¥–∞–º–∏

- –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ `pyspark_dist_explore`
- `toPandas()`
- –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ `HandySpark`

## `pyspark_dist_explore`

- `pyspark_dist_explore` --- —Ü–µ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –¥–∞–Ω–∏—Ö —É PySpark
- –ù–∞—Ä–∞–∑—ñ –¥–æ—Å—Ç—É–ø–Ω—ñ —Ç—Ä–∏ —Ñ—É–Ω–∫—Ü—ñ—ó: `hist()` , `distplot()` —Ç–∞ `pandas_histogram()`

```{python}
#| output-location: slide
from pyspark_dist_explore import hist
import matplotlib.pyplot as plt

df = spark.read.csv("data/salaries.csv", header=True, inferSchema=True)

fig, ax = plt.subplots()
hist(ax, df.select('salary_in_usd'), bins = 20);
```

## `toPandas()`

```{python}
df_pandas = df.toPandas()
df_pandas.hist(column='salary_in_usd', bins=20);
```

::: {.callout-note}
–Ø–∫—â–æ –≤–∏ –º–∞—î—Ç–µ –≤–µ–ª–∏–∫—ñ –æ–±—Å—è–≥–∏ –¥–∞–Ω–∏—Ö, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ `toPandas()` –Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—î—Ç—å—Å—è
:::

## Pandas vs PySpark

- Pandas DataFrames --- —Ü–µ –æ–¥–Ω–æ—Å–µ—Ä–≤–µ—Ä–Ω—ñ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –≤ –ø–∞–º'—è—Ç—ñ, —â–æ –±–∞–∑—É—é—Ç—å—Å—è –Ω–∞ –æ–¥–Ω–æ–º—É —Å–µ—Ä–≤–µ—Ä—ñ, –∞ –æ–ø–µ—Ä–∞—Ü—ñ—ó –Ω–∞–¥
PySpark –≤–∏–∫–æ–Ω—É—é—Ç—å—Å—è –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ
- –†–µ–∑—É–ª—å—Ç–∞—Ç –≥–µ–Ω–µ—Ä—É—î—Ç—å—Å—è, –∫–æ–ª–∏ –º–∏ –∑–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ –±—É–¥—å-—è–∫—É –æ–ø–µ—Ä–∞—Ü—ñ—é –≤ Pandas, –≤ —Ç–æ–π —á–∞—Å —è–∫ –æ–ø–µ—Ä–∞—Ü—ñ—ó –≤ PySpark
DataFrame —î –ª—ñ–Ω–∏–≤–∏–º–∏
- Pandas DataFrame —î –∑–º—ñ–Ω—é–≤–∞–Ω–∏–º, –∞ PySpark DataFrame —î –Ω–µ–∑–º—ñ–Ω–Ω–∏–º
- Pandas API –ø—ñ–¥—Ç—Ä–∏–º—É—î –±—ñ–ª—å—à–µ –æ–ø–µ—Ä–∞—Ü—ñ–π, –Ω—ñ–∂ PySpark Dataframe API

# PySpark MLlib

## –û–≥–ª—è–¥ PySpark MLlib

- MLlib - —Ü–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç Apache Spark –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
- –î–æ —Å–∫–ª–∞–¥—É MLlib –≤—Ö–æ–¥—è—Ç—å —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏:
  + –ê–ª–≥–æ—Ä–∏—Ç–º–∏ ML: —Å–ø—ñ–ª—å–Ω–∞ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è, –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è —Ç–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è
  + –§—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—ñ–∑–∞—Ü—ñ—è: –≤–∏–ª—É—á–µ–Ω–Ω—è –æ–∑–Ω–∞–∫, –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è, –∑–º–µ–Ω—à–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ —Ç–∞ –≤–∏–±—ñ—Ä–∫–∞
  + –ö–æ–Ω–≤–µ—î—Ä–∏: —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ –¥–ª—è –ø–æ–±—É–¥–æ–≤–∏, –æ—Ü—ñ–Ω–∫–∏ —Ç–∞ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –∫–æ–Ω–≤–µ—î—Ä—ñ–≤ ML

## –ß–æ–º—É PySpark MLlib?

- Scikit-learn - –ø–æ–ø—É–ª—è—Ä–Ω–∞ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ Python –¥–ª—è —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É –¥–∞–Ω–∏—Ö —Ç–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
- –ê–ª–≥–æ—Ä–∏—Ç–º–∏ Scikit-learn –ø—Ä–∞—Ü—é—é—Ç—å —Ç—ñ–ª—å–∫–∏ –¥–ª—è –Ω–µ–≤–µ–ª–∏–∫–∏—Ö –Ω–∞–±–æ—Ä—ñ–≤ –¥–∞–Ω–∏—Ö –Ω–∞ –æ–¥–Ω—ñ–π –º–∞—à–∏–Ω—ñ
- –ê–ª–≥–æ—Ä–∏—Ç–º–∏ MLlib Spark –ø—Ä–∏–∑–Ω–∞—á–µ–Ω—ñ –¥–ª—è –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏ –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä—ñ
- –ü—ñ–¥—Ç—Ä–∏–º—É—î —Ç–∞–∫—ñ –º–æ–≤–∏, —è–∫ Scala, Java —Ç–∞ R
- –ù–∞–¥–∞—î –≤–∏—Å–æ–∫–æ—Ä—ñ–≤–Ω–µ–≤–∏–π API –¥–ª—è –ø–æ–±—É–¥–æ–≤–∏ –∫–æ–Ω–≤–µ—î—Ä—ñ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è

## –ê–ª–≥–æ—Ä–∏—Ç–º–∏ PySpark MLlib

- –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è (–±—ñ–Ω–∞—Ä–Ω–∞ —Ç–∞ –±–∞–≥–∞—Ç–æ–∫–ª–∞—Å–æ–≤–∞) —Ç–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è: –õLinear SVMs, logistic regression,
decision trees, random forests, gradient-boosted trees, naive Bayes, linear least squares,
Lasso, ridge regression, isotonic regression
- –ö–æ–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω–∞ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è: –ê–ª–≥–æ—Ä–∏—Ç–º –∑–º—ñ–Ω–Ω–∏—Ö –Ω–∞–π–º–µ–Ω—à–∏—Ö –∫–≤–∞–¥—Ä–∞—Ç—ñ–≤ (Alternating least squares, ALS)
- –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è: K-means, Gaussian mixture, Bisecting K-means and Streaming K-Means

## –¢—Ä–∏ ¬´–°¬ª –º–∞—à–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è –≤ PySpark MLlib

- Collaborative filtering (—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π–Ω—ñ —Å–∏—Å—Ç–µ–º–∏): –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π
- Classification: –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è, –¥–æ —è–∫–æ—ó –∑ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –Ω–∞–ª–µ–∂–∏—Ç—å –Ω–æ–≤–µ —Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è
- Clustering: –ì—Ä—É–ø—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Å—Ö–æ–∂–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫

## PySpark MLlib imports

- Collaborative filtering

```{python}
from pyspark.mllib.recommendation import ALS
```

- Classification

```{python}
from pyspark.mllib.classification import LogisticRegressionWithLBFGS
```

- Clustering

```{python}
from pyspark.mllib.clustering import KMeans
```

# –í—Å—Ç—É–ø –¥–æ Collaborative filtering {.unnumbered .unlisted}

## Collaborative filtering

- –ö–æ–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω–∞ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è --- —Ü–µ –ø–æ—à—É–∫ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤, —è–∫—ñ –º–∞—é—Ç—å —Å–ø—ñ–ª—å–Ω—ñ —ñ–Ω—Ç–µ—Ä–µ—Å–∏
- –ö–æ–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω–∞ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∑–∞–∑–≤–∏—á–∞–π –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π–Ω–∏—Ö —Å–∏—Å—Ç–µ–º
- –ü—ñ–¥—Ö–æ–¥–∏ –¥–æ —Å–ø—ñ–ª—å–Ω–æ—ó —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó:
  + User-User: –ó–Ω–∞—Ö–æ–¥–∏—Ç—å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤, —Å—Ö–æ–∂–∏—Ö –Ω–∞ —Ü—ñ–ª—å–æ–≤–æ–≥–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
  + Item-Item: –ó–Ω–∞—Ö–æ–¥–∏—Ç—å —ñ —Ä–µ–∫–æ–º–µ–Ω–¥—É—î –µ–ª–µ–º–µ–Ω—Ç–∏, —Å—Ö–æ–∂—ñ –Ω–∞ –µ–ª–µ–º–µ–Ω—Ç–∏ –∑ —Ü—ñ–ª—å–æ–≤–∏–º –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º

## –ö–ª–∞—Å `Rating` —É `pyspark.mllib.recommendations`

- –ö–ª–∞—Å `Rating` --- —Ü–µ –æ–±–≥–æ—Ä—Ç–∫–∞ –Ω–∞–≤–∫–æ–ª–æ –∫–æ—Ä—Ç–µ–∂—É (–∫–æ—Ä–∏—Å—Ç—É–≤–∞—á, —Ç–æ–≤–∞—Ä —ñ —Ä–µ–π—Ç–∏–Ω–≥)
- –ö–æ—Ä–∏—Å–Ω–∏–π –¥–ª—è —Ä–æ–∑–±–æ—Ä—É RDD —Ç–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ—Ä—Ç–µ–∂—É –∑ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞, —Ç–æ–≤–∞—Ä—É —Ç–∞ —Ä–µ–π—Ç–∏–Ω–≥—É

```{python}
from pyspark.mllib.recommendation import Rating

r = Rating(user = 1, product = 2, rating = 5.0)
print(r)
```

## `randomSplit()`

- –ú–µ—Ç–æ–¥ `randomSplit()` PySpark –≤–∏–ø–∞–¥–∫–æ–≤–∏–º —á–∏–Ω–æ–º —Ä–æ–∑–±–∏–≤–∞—î –¥–∞–Ω—ñ —ñ–∑ –∑–∞–¥–∞–Ω–∏–º–∏ –≤–∞–≥–∞–º–∏ —ñ –ø–æ–≤–µ—Ä—Ç–∞—î –¥–µ–∫—ñ–ª—å–∫–∞ RDD

```{python}
data = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
training, test=data.randomSplit([0.6, 0.4])
training.collect()
test.collect()
```

## Alternating Least Squares (ALS)

- ALS —É `spark.mllib` –∑–∞–±–µ–∑–ø–µ—á—É—î –∫–æ–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω—É —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—é
- `ALS.train(ratings, rank, iterations)`

```{python}
r1 = Rating(1, 1, 1.0)
r2 = Rating(1, 2, 2.0)
r3 = Rating(2, 1, 2.0)
ratings = sc.parallelize([r1, r2, r3])
ratings.collect()
```

```{python}
model = ALS.train(ratings, rank=10, iterations=10)
```

## `predictAll()`

- –ú–µ—Ç–æ–¥ `predictAll()` –ø–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤–∞–Ω–∏—Ö –æ—Ü—ñ–Ω–æ–∫ –¥–ª—è –≤—Ö—ñ–¥–Ω–æ—ó –ø–∞—Ä–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á —ñ —Ç–æ–≤–∞—Ä
- –ú–µ—Ç–æ–¥ –æ—Ç—Ä–∏–º—É—î RDD –±–µ–∑ –æ—Ü—ñ–Ω–æ–∫, —â–æ–± –∑–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —Ä–µ–π—Ç–∏–Ω–≥–∏

```{python}
unrated_RDD = sc.parallelize([(1, 2), (1, 1)])

predictions = model.predictAll(unrated_RDD)
predictions.collect()
```

## –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ {.smaller}

:::: {.columns}

::: {.column width="50%"}

```{python}
rates = ratings.map(lambda x: ((x[0], x[1]), x[2]))
rates.collect()
```

\

```{python}
preds = predictions.map(lambda x: ((x[0], x[1]), x[2]))
preds.collect()
```

\

```{python}
rates_preds = rates.join(preds)
rates_preds.collect()
```
:::

::: {.column width="50%"}

```{python}
MSE = rates_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()
print("Mean Squared Error = " + str(MSE))
```
:::

::::

# –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è {.unnumbered .unlisted}

## –†–æ–±–æ—Ç–∞ –∑ –≤–µ–∫—Ç–æ—Ä–∞–º–∏ {.smaller}

- PySpark MLlib –º—ñ—Å—Ç–∏—Ç—å —Å–ø–µ—Ü—ñ–∞–ª—å–Ω—ñ —Ç–∏–ø–∏ –¥–∞–Ω–∏—Ö `Vectors` —Ç–∞ `LabelledPoint`
- –î–≤–∞ —Ç–∏–ø–∏ –≤–µ–∫—Ç–æ—Ä—ñ–≤
  + Dense Vector: –∑–±–µ—Ä—ñ–≥–∞—î –≤—Å—ñ —Å–≤–æ—ó –∑–∞–ø–∏—Å–∏ –≤ –º–∞—Å–∏–≤—ñ —á–∏—Å–µ–ª –∑ –ø–ª–∞–≤–∞—é—á–æ—é –∫–æ–º–æ—é
  + Sparse Vector: –∑–±–µ—Ä—ñ–≥–∞—î –ª–∏—à–µ –Ω–µ–Ω—É–ª—å–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∞ —ó—Ö–Ω—ñ —ñ–Ω–¥–µ–∫—Å–∏

```{python}
from pyspark.mllib.linalg import Vectors, SparseVector, DenseVector

denseVec = Vectors.dense([1.0, 2.0, 3.0])
sparseVec = Vectors.sparse(3, {0: 1.0, 2: 3.0})

print(denseVec)
print(sparseVec)
```

## `LabeledPoint`

- `LabeledPoint` --- —Ü–µ –æ–±–≥–æ—Ä—Ç–∫–∞ –¥–ª—è –≤—Ö—ñ–¥–Ω–∏—Ö –æ–∑–Ω–∞–∫ —ñ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤–∞–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è
- –î–ª—è –±—ñ–Ω–∞—Ä–Ω–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –ª–æ–≥—ñ—Å—Ç–∏—á–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó –º—ñ—Ç–∫–æ—é —î –∞–±–æ 0 (–≤—ñ–¥'—î–º–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è), –∞–±–æ 1 (–¥–æ–¥–∞—Ç–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è)

```{python}
from pyspark.mllib.regression import LabeledPoint

positive = LabeledPoint(1.0, [1.0, 0.0, 3.0])
negative = LabeledPoint(0.0, [2.0, 1.0, 1.0])
print(positive)
print(negative)
```

## `HashingTF`

- –ê–ª–≥–æ—Ä–∏—Ç–º `HashingTF()` –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è –∑—ñ—Å—Ç–∞–≤–ª–µ–Ω–Ω—è –∑–Ω–∞—á–µ–Ω–Ω—è –æ–∑–Ω–∞–∫–∏ –∑ —ñ–Ω–¥–µ–∫—Å–∞–º–∏ —É –≤–µ–∫—Ç–æ—Ä—ñ –æ–∑–Ω–∞–∫

```{python}
from pyspark.mllib.feature import HashingTF

sentence = "hello hello world"
words = sentence.split()
tf = HashingTF(10000)
tf.transform(words)
```

## –õ–æ–≥—ñ—Å—Ç–∏—á–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º `LogisticRegressionWithLBFGS`

```{python}
data = [
  LabeledPoint(0.0, [0.0, 1.0]),
  LabeledPoint(1.0, [1.0, 0.0]),
]

RDD = sc.parallelize(data)

model = LogisticRegressionWithLBFGS.train(RDD, iterations=10)
model.predict([0.0, 1.0])
model.predict([1.0, 0.0])
```

# –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è {.unnumbered .unlisted}

## –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è

–ë—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ PySpark MLlib –Ω–∞—Ä–∞–∑—ñ –ø—ñ–¥—Ç—Ä–∏–º—É—î –Ω–∞—Å—Ç—É–ø–Ω—ñ –º–æ–¥–µ–ª—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó

- K-means
- Gaussian mixture
- Power iteration clustering (PIC)
- Bisecting k-means
- Streaming k-means

## K-means

```{python}
RDD = sc.textFile("data/wine_data.csv") \
  .filter(lambda line: not line.startswith("class_label")) \
  .map(lambda x: x.split(",")) \
  .map(lambda x: [float(x[0]), float(x[1]), float(x[2])])

RDD.take(5)
```

## –ù–∞–≤—á–∞–Ω–Ω—è K-means

```{python}
from pyspark.mllib.clustering import KMeans

model = KMeans.train(RDD, k = 3, maxIterations = 10)
model.clusterCenters
```

## –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ

```{python}
from math import sqrt

def error(point):
  center = model.centers[model.predict(point)]
  return sqrt(sum([x**2 for x in (point - center)]))

WSSSE = RDD.map(lambda point: error(point)).reduce(lambda x, y: x + y)
print("Within Set Sum of Squared Error = " + str(WSSSE))
```

## –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

```{python}
#| output-location: slide
from pyspark.sql import SparkSession
import pandas as pd
import matplotlib.pyplot as plt

spark = SparkSession.builder.appName("KMeans").getOrCreate()

wine_data_df = spark.createDataFrame(RDD, schema=["col0", "col1", "col2"])
wine_data_df_pandas = wine_data_df.toPandas()

cluster_centers_pandas = pd.DataFrame(
    [center[1:] for center in model.clusterCenters],  # –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ col0
    columns=["col1", "col2"]
)

plt.scatter(wine_data_df_pandas["col1"], wine_data_df_pandas["col2"], c=wine_data_df_pandas["col0"])
plt.scatter(cluster_centers_pandas["col1"], cluster_centers_pandas["col2"], c="red", marker="x")
plt.title("KMeans Clustering")
plt.show()
```


# –î—è–∫—É—é –∑–∞ —É–≤–∞–≥—É! {.unnumbered .unlisted background-iframe=".02_files/libs/colored-particles/index.html"}

```{python}
#| echo: false
sc.stop()
```

<br> <br>

{{< iconify solar book-bold >}} [–ú–∞—Ç–µ—Ä—ñ–∞–ª–∏ –∫—É—Ä—Å—É](https://aranaur.rbind.io/lectures/mm_big_data/)

{{< iconify mdi envelope >}} ihor.miroshnychenko\@knu.ua

{{< iconify ic baseline-telegram >}} [Data Mirosh](https://t.me/araprof)

{{< iconify mdi linkedin >}} [\@ihormiroshnychenko](https://www.linkedin.com/in/ihormiroshnychenko/)

{{< iconify mdi github >}} [\@aranaur](https://github.com/Aranaur)

{{< iconify ion home >}} [aranaur.rbind.io](https://aranaur.rbind.io)
