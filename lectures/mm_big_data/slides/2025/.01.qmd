---
title: "Not so big data"
subtitle: "–ú–æ–¥–µ–ª—ñ —Ç–∞ –º–µ—Ç–æ–¥–∏ –æ–±—Ä–æ–±–∫–∏ –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö"
author: "–Ü–≥–æ—Ä –ú—ñ—Ä–æ—à–Ω–∏—á–µ–Ω–∫–æ"
institute: –ö–ù–£ —ñ–º–µ–Ω—ñ –¢–∞—Ä–∞—Å–∞ –®–µ–≤—á–µ–Ω–∫–∞, –§–Ü–¢
from: markdown+emoji
title-slide-attributes:
    data-background-iframe: .01_files/libs/colored-particles/index.html
language: _language-ua.yml
footer: <a href="https://aranaur.rbind.io/lectures/mm_big_data/">üîó–ú–æ–¥–µ–ª—ñ —Ç–∞ –º–µ—Ç–æ–¥–∏ –æ–±—Ä–æ–±–∫–∏ –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö</a>
format:
  revealjs: 
    code-line-numbers: false
    # center: true
    navigation-mode: vertical
    transition: fade
    background-transition: fade
    # controls-layout: bottom-right
    chalkboard: true
    logo: fit.png
    slide-number: true
    toc: true
    toc-depth: 1
    mouse-wheel: true
    width: 1350  
    height: 759.375
    highlight-style: github
    # fig-width: 9
    # fig-height: 5
    fig-format: svg
    theme: [default, custom.scss]
    mermaid:
      theme: forest
  # gfm:
  #   mermaid-format: png
preload-iframes: true
jupyter: python3
execute: 
  echo: true
  warning: false
  # cache: true
editor_options: 
  chunk_output_type: console

revealjs-plugins:
  - verticator
---


```{python}
#| label: setup
#| include: false

# Import libraries
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns
import random
import pandas as pd

import sys

from IPython.display import Markdown
from tabulate import tabulate

# Define colors
red_pink   = "#e64173"
turquoise  = "#20B2AA"
orange     = "#FFA500"
red        = "#fb6107"
blue       = "#181485"
navy       = "#150E37FF"
green      = "#8bb174"
yellow     = "#D8BD44"
purple     = "#6A5ACD"
slate      = "#314f4f"
```

# Polars vs. Pandas

## Benchmarking

::: {.columns}
::: {.column}
- [Benchmark](https://h2oai.github.io/db-benchmark/){preview-link="true"} –≤—ñ–¥ H2O.ai

![](data/h20.png)
:::
::: {.column}
- [Benchmark](https://duckdblabs.github.io/db-benchmark/){preview-link="true"} –≤—ñ–¥ DuckDB Labs

![](data/duckdb.png)
:::
:::

## –î–∞–Ω—ñ {.tiny}

–°–ø–æ—á–∞—Ç–∫—É –º–∏ –æ—Ç—Ä–∏–º–∞—î–º–æ –¥–µ—è–∫—ñ –¥–∞–Ω—ñ –ø—Ä–æ –∑–∞—Ç—Ä–∏–º–∫—É —Ä–µ–π—Å—ñ–≤.

```{python}
#| eval: false

from pathlib import Path
from zipfile import ZipFile
import requests

data_dir = Path("../data") # replace this with a directory of your choice
dest = data_dir / "flights.csv.zip"

if not dest.exists():
    r = requests.get(
        "https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2022_1.zip",
        verify=False,
        stream=True,
    )

    data_dir.mkdir(exist_ok=True)
    with dest.open("wb") as f:
        for chunk in r.iter_content(chunk_size=102400):
            if chunk:
                f.write(chunk)

    with ZipFile(dest) as zf:
        zf.extract(zf.filelist[0].filename, path=data_dir)

extracted = data_dir / "On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_1.csv"
```

## –î–∞–Ω—ñ –∑ Google Drive {.tiny}

```{python}
#| eval: false
import gdown
import io
import polars as pl
import pandas as pd
import time

file_id = "1UApDVdpgyeeZbJTvgCIjMjumGv_tWeQi"
url = f"https://drive.google.com/uc?id={file_id}"

response = gdown.download(url, quiet=True, fuzzy=True)
```

```{python}
#| include: false
import gdown
import io
import polars as pl
import pandas as pd
import time

response = "On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_1.csv"
```

::: {.panel-tabset}
## Polars

```{python}
pl.Config.set_tbl_rows(5)

start = time.time()
with open(response, 'rb') as f:
    df_pl = pl.read_csv(f, truncate_ragged_lines=True)
end = time.time()
print(f"Time taken: {end - start:.2f} seconds")

df_pl
```
## Pandas
```{python}
pd.options.display.max_rows = 5

start = time.time()
with open(response, 'rb') as f:
    df_pd = pd.read_csv(f, on_bad_lines='skip')
end = time.time()
print(f"Time taken: {end - start:.2f} seconds")

df_pd
```

:::

## –†—è–¥–∫–∏ –∑–∞ –Ω–æ–º–µ—Ä–æ–º, —Å—Ç–æ–≤–ø—Ü—ñ –∑–∞ –Ω–∞–∑–≤–æ—é

::: {.panel-tabset}
## Polars (recommended)

Using `head` and `tail`:

```{python}
%%timeit

df_pl.select(["Dest", "Tail_Number"]).head(16).tail(4)
```

Or using `gather`:

```{python}
%%timeit
df_pl.select(pl.col(["Dest", "Tail_Number"]).gather(list(range(12, 16))))
```

## Polars (square bracket)

```{python}
%%timeit
df_pl[12:16, ["Dest", "Tail_Number"]]
```

## Pandas

```{python}
%%timeit
df_pd.loc[12:15, ["Dest", "Tail_Number"]]
```

:::

## –†—è–¥–∫–∏ –∑–∞ —ñ–Ω–¥–µ–∫—Å–æ–º —Ä—è–¥–∫–∞, —Å—Ç–æ–≤–ø—Ü—ñ –∑–∞ –Ω–∞–∑–≤–æ—é

–û—Å–∫—ñ–ª—å–∫–∏ –≤ Polars –Ω–µ–º–∞—î —Ç–∞–∫–æ–≥–æ –ø–æ–Ω—è—Ç—Ç—è —è–∫ —ñ–Ω–¥–µ–∫—Å, –º–∏ –ø—Ä–æ—Å—Ç–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ `.filter`:

::: {.panel-tabset}
## Polars

```{python}
%%timeit
(
    df_pl
    .filter(pl.col("IATA_CODE_Reporting_Airline").is_in(['AA', 'DL']))
    .select(["IATA_CODE_Reporting_Airline", "Dest", "Tail_Number"])
)
```

## Pandas

```{python}
%%timeit
(
    df_pd
    .set_index("IATA_CODE_Reporting_Airline")
    .loc[['AA', 'DL'], ["Dest", "Tail_Number"]]
)
```

:::

## –†—è–¥–∫–∏ –∑–∞ –Ω–æ–º–µ—Ä–∞–º–∏, —Å—Ç–æ–≤–ø—Ü—ñ –∑–∞ –Ω–æ–º–µ—Ä–∞–º–∏

::: {.panel-tabset}
## Polars

```{python}
df_pl[[0, 1, 3], [0, 1]]
```

## Pandas

```{python}
df_pd.iloc[[0, 1, 3], [0, 1]]
```

:::

# –ü–∞–π–ø–∏

## –ü–∞–π–ø–∏

- `thing.min().abs().str()` –∑–∞–º—ñ—Å—Ç—å `str(abs(min(thing)))`
- –º–µ—Ç–æ–¥–∏ `assign` —Ç–∞ `pipe`

## –û—Ç—Ä–∏–º–∞—Ç–∏ –Ω–∞–∑–≤–∏ –º—ñ—Å—Ç {.tiny}

–£ –Ω–∞–±–æ—Ä—ñ –¥–∞–Ω–∏—Ö —î –¥–≤–∞ —Å—Ç–æ–≤–ø—Ü—ñ, —è–∫—ñ –º–∞—é—Ç—å –≤–∏–≥–ª—è–¥ `$city, $state`. –î–∞–≤–∞–π—Ç–µ –≤–∏–∑–Ω–∞—á–∏–º–æ —Ñ—É–Ω–∫—Ü—ñ—é, —è–∫–∞ –≤–∏–¥–∞–ª—è—î —á–∞—Å—Ç–∏–Ω—É –∑—ñ –∑–Ω–∞—á–µ–Ω–Ω—è–º —à—Ç–∞—Ç—É –∑ —Ü–∏—Ö —Å—Ç–æ–≤–ø—Ü—ñ–≤.

::: {.panel-tabset}
## Polars

``` {python}
def extract_city_name_pl() -> pl.Expr:
    """
    Chicago, IL -> Chicago for OriginCityName and DestCityName
    """
    cols = ["OriginCityName", "DestCityName"]
    return pl.col(cols).str.split(",").list.get(0)
```

## Pandas

``` {python}
def extract_city_name_pd(df: pd.DataFrame) -> pl.DataFrame:
    """
    Chicago, IL -> Chicago for OriginCityName and DestCityName
    """
    cols = ["OriginCityName", "DestCityName"]
    return df.assign(**{col: df[col].str.split(",", regex=False).str[0] for col in cols})
```

:::

## –û—Ç—Ä–∏–º–∞—Ç–∏ –Ω–∞–∑–≤–∏ –º—ñ—Å—Ç {.tiny}

–ö—ñ–ª—å–∫–∞ –ø—É–Ω–∫—Ç—ñ–≤, –Ω–∞ —è–∫—ñ —Å–ª—ñ–¥ –∑–≤–µ—Ä–Ω—É—Ç–∏ —É–≤–∞–≥—É:

1. –ù–∞—à–∞ —Ñ—É–Ω–∫—Ü—ñ—è Pandas –¥–æ–¥–∞—î —Å—Ç–æ–≤–ø—Ü—ñ –¥–æ —Ñ—Ä–µ–π–º—É –¥–∞–Ω–∏—Ö, —Ç–æ–¥—ñ —è–∫ –Ω–∞—à–∞ —Ñ—É–Ω–∫—Ü—ñ—è Polars –ø—Ä–æ—Å—Ç–æ –≥–µ–Ω–µ—Ä—É—î *–≤–∏—Ä–∞–∑*.
–í–∏ –ø–æ–±–∞—á–∏—Ç–µ, —â–æ —á–∞—Å—Ç–æ –ø—Ä–æ—Å—Ç—ñ—à–µ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç–∏ `Expr`, –Ω—ñ–∂ —Ñ—Ä–µ–π–º–∏ –¥–∞–Ω–∏—Ö, —Ç–æ–º—É —â–æ 
    i. –í–æ–Ω–∏ –ø—Ä–∞—Ü—é—é—Ç—å —è–∫ –∑ `DataFrame`, —Ç–∞–∫ —ñ –∑ `LazyFrame`, —ñ –≤–æ–Ω–∏ –Ω–µ –ø—Ä–∏–≤'—è–∑–∞–Ω—ñ –¥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö –¥–∞–Ω–∏—Ö.
    ii. Polars –ø—Ä–∞—Ü—é—î –∫—Ä–∞—â–µ, —è–∫—â–æ –≤–∏ –ø–æ–º—ñ—â–∞—î—Ç–µ –≤—Å–µ –≤ –æ–¥–∏–Ω –≤–∏–∫–ª–∏–∫ `.select` –∞–±–æ `.with_columns`, –∞ –Ω–µ –≤–∏–∫–ª–∏–∫–∞—î—Ç–µ `.select` –∫—ñ–ª—å–∫–∞ —Ä–∞–∑—ñ–≤. –Ø–∫—â–æ –≤–∏ –ø–µ—Ä–µ–¥–∞—î—Ç–µ –≤–∏—Ä–∞–∑–∏, —Ç–æ —Ü–µ–π –ø–∞—Ç–µ—Ä–Ω —Å—Ç–∞—î –ø—Ä–æ—Å—Ç–∏–º.
2.  Polars —î —à–≤–∏–¥–∫–∏–º —ñ –∑—Ä—É—á–Ω–∏–º –¥–ª—è –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –æ–¥–Ω–∏—Ö —ñ —Ç–∏—Ö –∂–µ –¥—ñ–π –∑ –¥–µ–∫—ñ–ª—å–∫–æ–º–∞ —Å—Ç–æ–≤–ø—á–∏–∫–∞–º–∏. –ú–∏ –º–æ–∂–µ–º–æ –ø–µ—Ä–µ–¥–∞—Ç–∏ —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–≤–ø—Ü—ñ–≤ –≤ `pl.col`
    –∞ –ø–æ—Ç—ñ–º –≤–∏–∫–ª–∏–∫–∞—Ç–∏ –º–µ—Ç–æ–¥ –Ω–∞ —Ü—å–æ–º—É `pl.col` —Ç–∞–∫, –Ω—ñ–±–∏ —Ü–µ –æ–¥–∏–Ω —Å—Ç–æ–≤–ø–µ—Ü—å. –ö–æ–ª–∏ –≤–∏—Ä–∞–∑ –±—É–¥–µ –≤–∏–∫–æ–Ω–∞–Ω–æ, –π–æ–≥–æ –±—É–¥–µ —Ä–æ–∑–ø–∞—Ä–∞–ª–µ–ª–µ–Ω–æ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é Polars.

    –¢–∏–º —á–∞—Å–æ–º —É Pandas –Ω–∞–º –¥–æ–≤–æ–¥–∏—Ç—å—Å—è —Ü–∏–∫–ª—ñ—á–Ω–æ –ø–µ—Ä–µ–±–∏—Ä–∞—Ç–∏ —Å—Ç–æ–≤–ø—Ü—ñ, —â–æ–± —Å—Ç–≤–æ—Ä–∏—Ç–∏ —Å–ª–æ–≤–Ω–∏–∫ kwargs  –¥–ª—è `.assign`. –¶–µ –Ω–µ —Ä–æ–∑–ø–∞—Ä–∞–ª–µ–ª—é—î—Ç—å—Å—è. (–ú–∏ –º–æ–≥–ª–∏ –± –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ `.apply` –∑ `axis=0` –∑–∞–º—ñ—Å—Ç—å —Ü—å–æ–≥–æ, –∞–ª–µ —Ü–µ –≤—Å–µ –æ–¥–Ω–æ –≤—ñ–¥–±—É–≤–∞—Ç–∏–º–µ—Ç—å—Å—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ).
3.  –í–∏–∫–ª–∏–∫ `.str.split` —É Polars —Å—Ç–≤–æ—Ä—é—î —Å—Ç–æ–≤–ø—á–∏–∫, –¥–µ –∫–æ–∂–µ–Ω –µ–ª–µ–º–µ–Ω—Ç —î —Å–ø–∏—Å–∫–æ–º. –¢–∞–∫–∏–π —Ç–∏–ø –¥–∞–Ω–∏—Ö –¥—Ä–∞—Ç—É—î –≤ Pandas —Ç–æ–º—É —â–æ –∑ –Ω–∏–º–∏ –ø–æ–≤—ñ–ª—å–Ω–æ —ñ –Ω–µ–∑—Ä—É—á–Ω–æ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ - –∑–≤–µ—Ä–Ω—ñ—Ç—å —É–≤–∞–≥—É, —â–æ –Ω–∞–π–∑—Ä—É—á–Ω—ñ—à–∏–π —Å–ø–æ—Å—ñ–± –æ—Ç—Ä–∏–º–∞—Ç–∏ –ø–µ—Ä—à–∏–π –µ–ª–µ–º–µ–Ω—Ç —Å—Ç–æ–≤–ø—Ü—è –∑—ñ —Å–ø–∏—Å–∫–æ–º —É Pandas - —Ü–µ –≤–∏–∫–ª–∏–∫–∞—Ç–∏ `.str[0]`, –Ω–∞–≤—ñ—Ç—å —è–∫—â–æ —Ü–µ —Å–ø–∏—Å–æ–∫, –∞ –Ω–µ —Ä—è–¥–æ–∫ ü§î.
    
    –Ø –Ω–µ –≤–ø–µ–≤–Ω–µ–Ω–∏–π, —â–æ —Ü–µ –≤–∑–∞–≥–∞–ª—ñ –º–∞—î –ø—Ä–∞—Ü—é–≤–∞—Ç–∏. –ù–∞ –ø—Ä–æ—Ç–∏–≤–∞–≥—É —Ü—å–æ–º—É, Polars –Ω–∞—Å–ø—Ä–∞–≤–¥—ñ –º–∞—î –ø–µ—Ä—à–æ–∫–ª–∞—Å–Ω—É –ø—ñ–¥—Ç—Ä–∏–º–∫—É —Å—Ç–æ–≤–ø—Ü—ñ–≤ –∑—ñ —Å–ø–∏—Å–∫–∞–º–∏, —ñ –≤–æ–Ω–∏ –ø—Ä–∞—Ü—é—é—Ç—å —à–≤–∏–¥–∫–æ, **—è–∫—â–æ –≤–æ–Ω–∏ –Ω–µ –º–∞—é—Ç—å –∑–º—ñ—à–∞–Ω–∏—Ö —Ç–∏–ø—ñ–≤.**

## –û—Ç—Ä–∏–º–∞—Ç–∏ –Ω–∞–∑–≤–∏ –º—ñ—Å—Ç {.tiny}

::: {.panel-tabset}
## Polars

``` {python}
def time_col_pl(col: str) -> pl.Expr:
    col_expr = pl.col(col)
    return (
        pl.when(col_expr == "2400")
        .then(pl.lit("0000"))
        .otherwise(col_expr)
        .str.strptime(pl.Time, "%H%M", strict=True)
        .alias(col)
    )


def time_to_datetime_pl(columns: list[str]) -> list[pl.Expr]:
    """
    Combine all time items into datetimes.

    2014-01-01,0914 -> 2014-01-01 09:14:00
    """
    date_col = pl.col("FlightDate")
    return [
        date_col
        .dt.combine(time_col_pl(col))
        .alias(col)
        for col in columns
    ]
```

## Pandas

``` {python}
def time_col_pd(col: str, df: pd.DataFrame) -> pd.Series:
    timepart = df[col].replace("2400", "0000")
    return pd.to_datetime(df["FlightDate"] + ' ' +
                            timepart.str.slice(0, 2) + ':' +
                            timepart.str.slice(2, 4),
                            errors='coerce')

def time_to_datetime_pd(df: pd.DataFrame, columns: list[str]) -> pd.DataFrame:
    '''
    Combine all time items into datetimes.

    2014-01-01,0914 -> 2014-01-01 09:14:00
    '''
    return df.assign(**{col: time_col_pd(col, df) for col in columns})
```

:::

## –ó–∞–≥–∞–ª—å–Ω—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è {.tiny}

```{python}
category_cols = [
    "Dest",
    "Tail_Number",
    "IATA_CODE_Reporting_Airline",
    "CancellationCode",
]
time_cols = ["DepTime", "ArrTime", "CRSArrTime", "CRSDepTime"]
cols = (
    category_cols
    + time_cols
    + [
        "FlightDate",
        "Flight_Number_Reporting_Airline",
        "OriginCityName",
        "DestCityName",
        "Origin",
        "DepDelay",
    ]
)

extracted = "On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_1.csv"
```

## –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ñ—É–Ω–∫—Ü—ñ—ó {.tiny}

::: {.panel-tabset}
## Polars

``` {python}
dtypes_pl = (
    {col: pl.Categorical for col in category_cols}
    | {"FlightDate": pl.Date}
    | {col: pl.Utf8 for col in time_cols}
)
df_pl = (
    pl.scan_csv(extracted, schema_overrides=dtypes_pl, null_values="")
    .select(cols)
    .with_columns([extract_city_name_pl(), *time_to_datetime_pl(time_cols)])
    .collect()
)
df_pl.head()
```

## Pandas

``` {python}
dtypes_pd = (
    {col: pd.CategoricalDtype() for col in category_cols}
    | {col: pd.StringDtype() for col in time_cols}
)
df_pd = (
    pd.read_csv(extracted, dtype=dtypes_pd, usecols=cols, na_values="")
    .pipe(extract_city_name_pd)
    .pipe(time_to_datetime_pd, time_cols)
    .assign(FlightDate=lambda df: pd.to_datetime(df["FlightDate"]))
)
df_pd[cols].head()
```

:::

## –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ñ—É–Ω–∫—Ü—ñ—ó

–í—ñ–¥–º—ñ–Ω–Ω–æ—Å—Ç—ñ –º—ñ–∂ –¥–≤–æ–º–∞ –ø—ñ–¥—Ö–æ–¥–∞–º–∏:

1. –û—Å–∫—ñ–ª—å–∫–∏ `scan_csv` —î –ª—ñ–Ω–∏–≤–∏–º, –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è `scan_csv` –∑ –Ω–∞—Å—Ç—É–ø–Ω–∏–º `.select` –¥–ª—è –≤–∏–±–æ—Ä—É –ø—ñ–¥–º–Ω–æ–∂–∏–Ω–∏ —Å—Ç–æ–≤–ø—Ü—ñ–≤ –µ–∫–≤—ñ–≤–∞–ª–µ–Ω—Ç–Ω–æ `usecols` —É `pd.read_csv`. –û—Å—å —á–æ–º—É —Å–∞–º `pl.scan_csv` –Ω–µ –º–∞—î –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –¥–ª—è –≤–∏–±–æ—Ä—É –ø—ñ–¥–º–Ω–æ–∂–∏–Ω–∏ —Å—Ç–æ–≤–ø—Ü—ñ–≤ –¥–ª—è —á–∏—Ç–∞–Ω–Ω—è.
2. –£ Polars —î –º–µ—Ç–æ–¥ `.pipe`, –∞–ª–µ –º–∏ –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –π–æ–≥–æ —É —Ü—å–æ–º—É –≤–∏–ø–∞–¥–∫—É, –æ—Å–∫—ñ–ª—å–∫–∏ –ø—Ä–æ—Å—Ç—ñ—à–µ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –∑ –≤–∏—Ä–∞–∑–∞–º–∏.

## –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è {.tiny}

::: {.panel-tabset}
## Polars

``` {python}
#| output-location: column
# filter for the busiest airlines
filter_expr = pl.col("IATA_CODE_Reporting_Airline").is_in(
    pl.col("IATA_CODE_Reporting_Airline")
    .value_counts(sort=True)
    .struct.field("IATA_CODE_Reporting_Airline")
    .head(5)
)
(
    df_pl
    .drop_nulls(subset=["DepTime", "IATA_CODE_Reporting_Airline"])
    .filter(filter_expr)
    .sort("DepTime")
    .group_by_dynamic(
        "DepTime",
        every="1h",
        group_by="IATA_CODE_Reporting_Airline")
    .agg(pl.col("Flight_Number_Reporting_Airline").count())
    .pivot(
        index="DepTime",
        on="IATA_CODE_Reporting_Airline",
        values="Flight_Number_Reporting_Airline",
    )
    .sort("DepTime")
    # fill every missing hour with 0 so the plot looks better
    .upsample(time_column="DepTime", every="1h")
    .fill_null(0)
    .select([pl.col("DepTime"), pl.col(pl.UInt32).rolling_sum(24)])
    .to_pandas()
    .set_index("DepTime")
    .rename_axis("Flights per Day", axis=1)
    .plot()
)
```

## Pandas

``` {python}
#| output-location: column
(
    df_pd
    .dropna(subset=["DepTime", "IATA_CODE_Reporting_Airline"])
    # filter for the busiest airlines
    .loc[
        lambda x: x["IATA_CODE_Reporting_Airline"].isin(
            x["IATA_CODE_Reporting_Airline"].value_counts().index[:5]
        )
    ]
    .assign(
        IATA_CODE_Reporting_Airline=lambda x: x[
            "IATA_CODE_Reporting_Airline"
        ].cat.remove_unused_categories()  #  annoying pandas behaviour
    )
    .set_index("DepTime")
    # TimeGrouper to resample & groupby at once
    .groupby(["IATA_CODE_Reporting_Airline", pd.Grouper(freq="h")])[
        "Flight_Number_Reporting_Airline"
    ]
    .count()
    # the .pivot takes care of this in the Polars code.
    .unstack(0)
    .fillna(0)
    .rolling(24)
    .sum()
    .rename_axis("Flights per Day", axis=1)
    .plot()
)
```
:::

## –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è {.smaller}

–í—ñ–¥–º—ñ–Ω–Ω–æ—Å—Ç—ñ –º—ñ–∂ Polars —ñ Pandas:

1. –î–ª—è –≥—Ä—É–ø—É–≤–∞–Ω–Ω—è –∑–∞ —á–∞—Å–æ–≤–∏–º –≤—ñ–∫–Ω–æ–º —Ç–∞ —ñ–Ω—à–∏–º –∑–Ω–∞—á–µ–Ω–Ω—è–º –º–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ `.groupby_dynamic`. –£ Pandas –º–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ .groupby –∑ –ø–æ–º—ñ—á–Ω–∏–∫–æ–º `pd.Grouper`.
2. –ó–∞–º—ñ—Å—Ç—å `.rolling(n).sum()` —É Polars –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è `.rolling_sum(n)`.
3. –Ø–∫—â–æ –≤–∏ –±–∞—á–∏—Ç–µ –∫–æ–¥ Pandas, —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î `.unstack`, —Ç–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–π –∫–æ–¥ Polars, –π–º–æ–≤—ñ—Ä–Ω–æ, –ø–æ—Ç—Ä–µ–±—É—î `.pivot`.
4. –£ Polars `.value_counts` –ø–æ–≤–µ—Ä—Ç–∞—î —Å—Ç–æ–≤–ø–µ—Ü—å `pl.Struct`, —â–æ –º—ñ—Å—Ç–∏—Ç—å –∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–Ω–∞—á–µ–Ω—å. –£ Pandas –≤–æ–Ω–∞ –ø–æ–≤–µ—Ä—Ç–∞—î —Ä—è–¥, –¥–µ –µ–ª–µ–º–µ–Ω—Ç–∞–º–∏ —î –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–Ω–∞—á–µ–Ω—å, –∞ —ñ–Ω–¥–µ–∫—Å –º—ñ—Å—Ç–∏—Ç—å —Å–∞–º—ñ –∑–Ω–∞—á–µ–Ω–Ω—è.
5. –£ Polars –Ω–∞–º –ø–æ—Ç—Ä—ñ–±–Ω–æ –≤–∏–±—Ä–∞—Ç–∏ –≤—Å—ñ —Å—Ç–æ–≤–ø—Ü—ñ UInt32 –≤ –æ–¥–Ω—ñ–π —Ç–æ—á—Ü—ñ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é pl.col(pl.UInt32). –£ Pandas —Å–ø–æ—Å—ñ–± —Ä–æ–±–æ—Ç–∏ `.rolling` –æ–∑–Ω–∞—á–∞—î, —â–æ –Ω–∞–º –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ –≤–∏–±–∏—Ä–∞—Ç–∏ —Ü—ñ —Å—Ç–æ–≤–ø—Ü—ñ —è–≤–Ω–æ, –∞–ª–µ —è–∫—â–æ –º–∏ —Ü–µ –∑—Ä–æ–±–∏–º–æ, —Ü–µ –±—É–¥–µ –≤–∏–≥–ª—è–¥–∞—Ç–∏ —è–∫ `df.select_dtypes("uint32")`.

## –õ—ñ—Ç–∞–∫–∏ –∑ –∫—ñ–ª—å–∫–æ–º–∞ —â–æ–¥–µ–Ω–Ω–∏–º–∏ —Ä–µ–π—Å–∞–º–∏ {.tiny .scrollable}

::: {.panel-tabset}
## Polars

``` {python}
#| output-location: column
flights_pl = (
    df_pl.select(
        pl.col([
            "FlightDate",
            "Tail_Number",
            "DepTime",
            "DepDelay"
        ])
    )
    .drop_nulls()
    .sort("DepTime")
    .filter(pl.col("DepDelay") < 500)
    .with_columns(
        pl.col("DepTime")
        .rank()
        .over(["FlightDate", "Tail_Number"])
        .alias("turn")
    )
)

fig, ax = plt.subplots(figsize=(10, 5))
sns.boxplot(x="turn", y="DepDelay", data=flights_pl.to_pandas(), ax=ax)
ax.set_ylim(-50, 50)
```

## Pandas

``` {python}
#| output-location: column
flights_pd = (
    df_pd[[
        "FlightDate",
        "Tail_Number",
        "DepTime",
        "DepDelay"
    ]]
    .dropna()
    .sort_values('DepTime')
    .loc[lambda x: x["DepDelay"] < 500]
    .assign(turn = lambda x:
        x.groupby(["FlightDate", "Tail_Number"])
        ["DepTime"].transform('rank')
        .astype(int)
    )
)
fig, ax = plt.subplots(figsize=(10, 5))
sns.boxplot(x="turn", y="DepDelay", data=flights_pd, ax=ax)
ax.set_ylim(-50, 50)
```

:::

## –õ—ñ—Ç–∞–∫–∏ –∑ –∫—ñ–ª—å–∫–æ–º–∞ —â–æ–¥–µ–Ω–Ω–∏–º–∏ —Ä–µ–π—Å–∞–º–∏

–¢—É—Ç —î –æ–¥–Ω–∞ –Ω–æ–≤–∏–Ω–∫–∞: [–≤—ñ–∫–æ–Ω–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó](https://pola-rs.github.io/polars-book/user-guide/dsl/window_functions.html).

–ö–æ–ª–∏ –∫–æ–¥ Pandas –º–∞—î –≤–∏–≥–ª—è–¥:

```python
.groupby("country")["population"].transform("sum")
```
—Ç–æ –µ–∫–≤—ñ–≤–∞–ª–µ–Ω—Ç–Ω–∏–π –∫–æ–¥ Polars –º–∞—Ç–∏–º–µ –≤–∏–≥–ª—è–¥:

```python
pl.col("population").sum().over("country")
```

## –ó–∞—Ç—Ä–∏–º–∫–∞ –ø–æ –≥–æ–¥–∏–Ω–∞—Ö –¥–æ–±–∏ {.tiny}

::: {.panel-tabset}
## Polars

``` {python}
#| output-location: column
plt.figure(figsize=(10, 5))
(
    df_pl.select(
        pl.col(
            ["FlightDate", "Tail_Number", "DepTime", "DepDelay"],
        )
    )
    .drop_nulls()
    .filter(pl.col("DepDelay").is_between(5, 600, closed="none"))
    .with_columns(pl.col("DepTime").dt.hour().alias("hour"))
    .to_pandas()
    .pipe((sns.boxplot, "data"), x="hour", y="DepDelay")
)
```

## Pandas

``` {python}
#| output-location: column
plt.figure(figsize=(10, 5))
(
    df_pd[["FlightDate", "Tail_Number", "DepTime", "DepDelay"]]
    .dropna()
    .loc[lambda df: df["DepDelay"].between(5, 600, inclusive="neither")]
    .assign(hour=lambda df: df["DepTime"].dt.hour)
    .pipe((sns.boxplot, "data"), x="hour", y="DepDelay")
)
```

:::

## –ü—ñ–¥—Å—É–º–æ–∫

![](data/pandas_chain_tweet.png){fig-align="center"}

# –ü—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å

## –®—ñ—Å—Ç—å –¥–æ—Å–∏—Ç—å –æ—á–µ–≤–∏–¥–Ω–∏—Ö –ø—Ä–∞–≤–∏–ª –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ {.tiny}

[Polars –¥—É–∂–µ —à–≤–∏–¥–∫–∏–π](https://www.pola.rs/benchmarks.html).

- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ –ª—ñ–Ω–∏–≤–∏–π API.
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ `Expr` —ñ –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ `.apply`, —è–∫—â–æ —Ü–µ –¥—ñ–π—Å–Ω–æ –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ.
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ –Ω–∞–π–º–µ–Ω—à—ñ –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ —á–∏—Å–ª–æ–≤—ñ —Ç–∏–ø–∏ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, —è–∫—â–æ —É –≤–∞—Å —î —Ü—ñ–ª–µ —á–∏—Å–ª–æ –≤—ñ–¥ 0 –¥–æ 255, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ `pl.UInt8`, –∞ –Ω–µ `pl.Int64`). –¶–µ –∑–∞–æ—â–∞–¥–∏—Ç—å —ñ —á–∞—Å, —ñ –º—ñ—Å—Ü–µ.
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–µ —Å—Ö–æ–≤–∏—â–µ (—è–∫—â–æ –≤–∏ –∑–±–µ—Ä—ñ–≥–∞—î—Ç–µ –¥–∞–Ω—ñ —É —Ñ–∞–π–ª–∞—Ö, Parquet - —Ö–æ—Ä–æ—à–∏–π –≤–∏–±—ñ—Ä).
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—ó –¥–ª—è —Ä—è–¥–∫—ñ–≤, —â–æ –ø–æ–≤—Ç–æ—Ä—é—é—Ç—å—Å—è (–∞–ª–µ –∑–∞—É–≤–∞–∂—Ç–µ, —â–æ —Ü–µ –º–æ–∂–µ –±—É—Ç–∏ –Ω–µ–¥–æ—Ü—ñ–ª—å–Ω–æ, —è–∫—â–æ –ø–æ–≤—Ç–æ—Ä—é–≤–∞–Ω—ñ—Å—Ç—å –Ω–µ–≤–µ–ª–∏–∫–∞).
- –í–∏–±–∏—Ä–∞–π—Ç–µ –ª–∏—à–µ —Ç—ñ —Å—Ç–æ–≤–ø—Ü—ñ, —è–∫—ñ –≤–∞–º –ø–æ—Ç—Ä—ñ–±–Ω—ñ.

## Polars —à–≤–∏–¥—à–µ —Å–ø—Ä–∞–≤–ª—è—î—Ç—å—Å—è –∑ –Ω—É–¥–Ω–∏–º–∏ –∑–∞–≤–¥–∞–Ω–Ω—è–º–∏ {.tiny}

[–î–∞–Ω—ñ](https://www.kaggle.com/datasets/yagunnersya/fifa-21-messy-raw-dataset-for-cleaning-exploring?resource=download) –∑ Kaggle.

–ö—Ä—ñ–º —Ç–æ–≥–æ, –¥–∞–Ω—ñ –∑–∞–Ω–∞–¥—Ç–æ –º–∞–ª—ñ, —Ç–æ–º—É —è –æ–±'—î–¥–Ω–∞–≤ —ó—Ö 20 —Ä–∞–∑—ñ–≤.

```{python}
#| label: messy

fifa = pd.read_csv("data/fifa21_raw_data.csv")
fifa = pd.concat([fifa] * 20, ignore_index=True)

fifa.to_csv("data/fifa21_raw_big.csv", index=False)
fifa
```

## –®–∞–±–ª–æ–Ω —Å—Ç–æ–≤–ø—Ü—ñ–≤ {.tiny .scrollable}

```{python}
import pandas as pd
import polars as pl
import numpy as np
import math
str_cols = [
    "Name",
    "LongName",
    "playerUrl",
    "photoUrl",
]
initial_category_cols_pl = [
    "Nationality",
    "Preferred Foot",
    "Best Position",
    "A/W",
    "D/W"
]
category_cols = [*initial_category_cols_pl, "Club"]
date_cols = [
    "Joined",
    "Loan Date End"
]
# –≤—Å—ñ –≤–æ–Ω–∏ –ø–æ—á–∏–Ω–∞—é—Ç—å—Å—è –∑ —Å–∏–º–≤–æ–ª—É —î–≤—Ä–æ —ñ –∑–∞–∫—ñ–Ω—á—É—é—Ç—å—Å—è –Ω–∞ 0, M –∞–±–æ K
money_cols = [
    "Value",
    "Wage",
    "Release Clause"
]
star_cols = [
    "W/F",
    "SM",
    "IR",
]
# Contract col - –¥—ñ–∞–ø–∞–∑–æ–Ω —Ä–æ–∫—ñ–≤ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—É
# Positions - —Å–ø–∏—Å–æ–∫ –ø–æ—Å–∞–¥
# Height - —É —Å–º
# Weight –≤ –∫–≥
# Hits - –Ω–æ–º–µ—Ä–∏ –∑ K —Ç–∞ M 
messy_cols = [
    "Contract",
    "Positions",
    "Height",
    "Weight",
    "Hits"
]
initially_str_cols = str_cols + date_cols + money_cols + star_cols + messy_cols
initially_str_cols_pl = [*initially_str_cols, "Club"]
u32_cols = [
    "ID",
    "Total Stats"
]
u8_cols = [
    'Age',
    '‚ÜìOVA',
    'POT',
    'BOV',
    'Crossing',
    'Finishing',
    'Heading Accuracy',
    'Short Passing',
    'Volleys',
    'Dribbling',
    'Curve',
    'FK Accuracy',
    'Long Passing',
    'Ball Control',
    'Acceleration',
    'Sprint Speed',
    'Agility',
    'Reactions',
    'Balance',
    'Shot Power',
    'Jumping',
    'Stamina',
    'Strength',
    'Long Shots',
    'Aggression',
    'Interceptions',
    'Positioning',
    'Vision',
    'Penalties',
    'Composure',
    'Marking',
    'Standing Tackle',
    'Sliding Tackle',
    'GK Diving',
    'GK Handling',
    'GK Kicking',
    'GK Positioning',
    'GK Reflexes',
    'PAC',
    'SHO',
    'PAS',
    'DRI',
    'DEF',
    'PHY'
]

u16_cols = [
    'Attacking',
    'Skill',
    'Movement',
    'Power',
    'Mentality',
    'Defending',
    'Goalkeeping',
    'Total Stats',
    'Base Stats'
]
```

## Dtypes

::: {.panel-tabset}
## Polars
``` {python}
# –Ω–µ –º–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ UInt8/16 –≤ scan_csv
dtypes_pl = (
    {col: pl.Utf8 for col in initially_str_cols_pl}
    | {col: pl.Categorical for col in initial_category_cols_pl}
    | {col: pl.UInt32 for col in [*u32_cols, *u16_cols, *u8_cols]}
)
```
## Pandas
``` {python}
dtypes_pd = (
    {col: pd.StringDtype() for col in initially_str_cols}
    | {col: pd.CategoricalDtype() for col in category_cols}
    | {col: "uint32" for col in u32_cols}
    | {col: "uint8" for col in u8_cols}
    | {col: "uint16" for col in u16_cols}
)
```
:::

## –û—á–∏—Å—Ç–∫–∞ {.tiny .scrollable}

[`pl.when`](https://pola-rs.github.io/polars/py-polars/html/reference/expressions/api/polars.when.html#polars.when) –¥–ª—è —Ç–µ—Ä–Ω–∞—Ä–Ω–∏—Ö –≤–∏—Ä–∞–∑—ñ–≤.

::: {.panel-tabset}
## Polars

``` {python}
def parse_date_pl(col: pl.Expr) -> pl.Expr:
    return col.str.strptime(pl.Date, format="%b %d, %Y")

def parse_suffixed_num_pl(col: pl.Expr) -> pl.Expr:
    suffix = col.str.slice(-1, 1)
    suffix_value = (
        pl.when(suffix == "K")
        .then(1_000)
        .when(suffix == "M")
        .then(1_000_000)
        .otherwise(1)
        .cast(pl.UInt32)
    )
    without_suffix = (
        col
        .str.replace("K", "", literal=True)
        .str.replace("M", "", literal=True)
        .cast(pl.Float32)
    )
    original_name = col.meta.output_name()
    return (suffix_value * without_suffix).alias(original_name)

def parse_money_pl(col: pl.Expr) -> pl.Expr:
    return parse_suffixed_num_pl(col.str.slice(1)).cast(pl.UInt32)

def parse_star_pl(col: pl.Expr) -> pl.Expr:
    return col.str.slice(0, 1).cast(pl.UInt8)

def feet_to_cm_pl(col: pl.Expr) -> pl.Expr:
    feet_inches_split = col.str.split_exact("'", 1)
    total_inches = (
        (feet_inches_split.struct.field("field_0").cast(pl.UInt8, strict=False) * 12)
        + feet_inches_split.struct.field("field_1").str.strip_chars_end('"').cast(pl.UInt8, strict=False)
    )
    return (total_inches * 2.54).round(0).cast(pl.UInt8)

def parse_height_pl(col: pl.Expr) -> pl.Expr:
    is_cm = col.str.ends_with("cm")
    return (
        pl.when(is_cm)
        .then(col.str.slice(0, 3).cast(pl.UInt8, strict=False))
        .otherwise(feet_to_cm_pl(col))
    )

def parse_weight_pl(col: pl.Expr) -> pl.Expr:
    is_kg = col.str.ends_with("kg")
    without_unit = col.str.extract(r"(\d+)").cast(pl.UInt8)
    return (
        pl.when(is_kg)
        .then(without_unit)
        .otherwise((without_unit * 0.453592).round(0).cast(pl.UInt8))
    )

def parse_contract_pl(col: pl.Expr) -> list[pl.Expr]:
    contains_tilde = col.str.contains(" ~ ", literal=True)
    loan_str = " On Loan"
    loan_col = col.str.ends_with(loan_str)
    split = (
        pl.when(contains_tilde)
        .then(col)
        .otherwise(None)
        .str.split_exact(" ~ ", 1)
    )
    start = split.struct.field("field_0").cast(pl.UInt16).alias("contract_start")
    end = split.struct.field("field_1").cast(pl.UInt16).alias("contract_end")
    free_agent = (col == "Free").alias("free_agent").fill_null(False)
    loan_date = (
        pl.when(loan_col)
        .then(col)
        .otherwise(None)
        .str.split_exact(" On Loan", 1)
        .struct.field("field_0")
        .alias("loan_date_start")
    )
    return [start, end, free_agent, parse_date_pl(loan_date)]
```

## Pandas

``` {python}
def parse_date_pd(col: pd.Series) -> pd.Series:
    return pd.to_datetime(col, format="%b %d, %Y")

def parse_suffixed_num_pd(col: pd.Series) -> pd.Series:
    suffix_value = (
        col
        .str[-1]
        .map({"K": 1_000, "M": 1_000_000})
        .fillna(1)
        .astype("uint32")
    )
    without_suffix = (
        col
        .str.replace("K", "", regex=False)
        .str.replace("M", "", regex=False)
        .astype("float")
    )
    return suffix_value * without_suffix

def parse_money_pd(col: pd.Series) -> pd.Series:
    return parse_suffixed_num_pd(col.str[1:]).astype("uint32")

def parse_star_pd(col: pd.Series) -> pd.Series:
    return col.str[0].astype("uint8")

def feet_to_cm_pd(col: pd.Series) -> pd.Series:
    feet_inches_split = col.str.split("'", expand=True)
    total_inches = (
        feet_inches_split[0].astype("uint8").mul(12)
        + feet_inches_split[1].str[:-1].astype("uint8")
    )
    return total_inches.mul(2.54).round().astype("uint8")

def parse_height_pd(col: pd.Series) -> pd.Series:
    is_cm = col.str.endswith("cm")
    cm_values = col.loc[is_cm].str[:-2].astype("uint8")
    inches_as_cm = feet_to_cm_pd(col.loc[~is_cm])
    return pd.concat([cm_values, inches_as_cm])

def parse_weight_pd(col: pd.Series) -> pd.Series:
    is_kg = col.str.endswith("kg")
    without_unit = col.where(is_kg, col.str[:-3]).mask(is_kg, col.str[:-2]).astype("uint8")
    return without_unit.where(is_kg, without_unit.mul(0.453592).round().astype("uint8"))

def parse_contract_pd(df: pd.DataFrame) -> pd.DataFrame:
    contract_col = df["Contract"]
    contains_tilde = contract_col.str.contains(" ~ ", regex=False)
    split = (
        contract_col.loc[contains_tilde].str.split(" ~ ", expand=True).astype(pd.UInt16Dtype())
    )
    split.columns = ["contract_start", "contract_end"]
    not_tilde = contract_col.loc[~contains_tilde]
    free_agent = (contract_col == "Free").rename("free_agent").fillna(False)
    loan_date = parse_date_pd(not_tilde.loc[~free_agent].str[:-8]).rename("loan_date_start")
    return pd.concat([df.drop("Contract", axis=1), split, free_agent, loan_date], axis=1)
```

:::

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ {.tiny .scrollable}

–£ —Ü—å–æ–º—É –ø—Ä–∏–∫–ª–∞–¥—ñ **Polar –≤ —Ä–∞–∑–∏ —à–≤–∏–¥—à–µ** –Ω—ñ–∂ Pandas

::: {.panel-tabset}
## Polars

``` {python}
%%time
new_cols_pl = ([
    pl.col("Club").str.strip_chars().cast(pl.Categorical),
    parse_suffixed_num_pl(pl.col("Hits")).cast(pl.UInt32),
    pl.col("Positions").str.split(","),
    parse_height_pl(pl.col("Height")),
    parse_weight_pl(pl.col("Weight")),
]
+ [parse_date_pl(pl.col(col)) for col in date_cols]
+ [parse_money_pl(pl.col(col)) for col in money_cols]
+ [parse_star_pl(pl.col(col)) for col in star_cols]
+ parse_contract_pl(pl.col("Contract"))
+ [pl.col(col).cast(pl.UInt16) for col in u16_cols]
+ [pl.col(col).cast(pl.UInt8) for col in u8_cols]
)
fifa_pl = (
    pl.scan_csv("data/fifa21_raw_big.csv", schema_overrides=dtypes_pl)
    .with_columns(new_cols_pl)
    .drop("Contract")
    .rename({"‚ÜìOVA": "OVA"})
    .collect()
)
```

## Pandas

``` {python}
%%time
fifa_pd = (
    pd.read_csv("data/fifa21_raw_big.csv", dtype=dtypes_pd)
    .assign(Club=lambda df: df["Club"].cat.rename_categories(lambda c: c.strip()),
        **{col: lambda df: parse_date_pd(df[col]) for col in date_cols},
        **{col: lambda df: parse_money_pd(df[col]) for col in money_cols},
        **{col: lambda df: parse_star_pd(df[col]) for col in star_cols},
        Hits=lambda df: parse_suffixed_num_pd(df["Hits"]).astype(pd.UInt32Dtype()),
        Positions=lambda df: df["Positions"].str.split(","),
        Height=lambda df: parse_height_pd(df["Height"]),
        Weight=lambda df: parse_weight_pd(df["Weight"])
    )
    .pipe(parse_contract_pd)
    .rename(columns={"‚ÜìOVA": "OVA"})
)
```
:::

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ {.tiny}

–†–µ–∑—É–ª—å—Ç–∞—Ç–∏:

::: {.panel-tabset}
## Polars

``` {python}
fifa_pl.head(3)
```

## Pandas

``` {python}
fifa_pd.head(3)
```
:::

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ

–£ —Ü—å–æ–º—É —Å—Ü–µ–Ω–∞—Ä—ñ—ó –ø–µ—Ä–µ–≤–∞–≥–∞ Polars —É —à–≤–∏–¥–∫–æ—Å—Ç—ñ, –π–º–æ–≤—ñ—Ä–Ω–æ, –∑–≤–æ–¥–∏—Ç—å—Å—è –¥–æ —Ç—Ä—å–æ—Ö —Ä–µ—á–µ–π:

- –í—ñ–Ω –Ω–∞–±–∞–≥–∞—Ç–æ —à–≤–∏–¥—à–µ —á–∏—Ç–∞—î CSV-—Ñ–∞–π–ª–∏.
- –í—ñ–Ω –Ω–∞–±–∞–≥–∞—Ç–æ —à–≤–∏–¥—à–µ –æ–±—Ä–æ–±–ª—è—î —Ä—è–¥–∫–∏.
- –í—ñ–Ω –º–æ–∂–µ –≤–∏–±–∏—Ä–∞—Ç–∏/–ø—Ä–∏–∑–Ω–∞—á–∞—Ç–∏ —Å—Ç–æ–≤–ø—Ü—ñ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ.

## NumPy –º–æ–∂–µ —ñ–Ω–æ–¥—ñ –ø—Ä–∏—Å–∫–æ—Ä—é–≤–∞—Ç–∏ —Ä–æ–±–æ—Ç—É Polars

Polars –¥–æ–±—Ä–µ –ª–∞–¥–Ω–∞—î –∑ —É—Ç–∏–ª—ñ—Ç–∞–º–∏ NumPy, –Ω–∞–≤—ñ—Ç—å —É –ª—ñ–Ω–∏–≤–æ–º—É —Ä–µ–∂–∏–º—ñ (—â–æ —Ü—ñ–∫–∞–≤–æ, –æ—Å–∫—ñ–ª—å–∫–∏ NumPy –Ω–µ –º–∞—î –ª—ñ–Ω–∏–≤–æ–≥–æ API).

–î–∞–Ω—ñ:

```{python}
airports = pl.scan_csv("data/airports.csv").drop_nulls().unique(subset=["AIRPORT"])
pairs = airports.join(airports, how="cross").filter(
    (pl.col("AIRPORT") != pl.col("AIRPORT_right"))
    & (pl.col("LATITUDE") != pl.col("LATITUDE_right"))
    & (pl.col("LONGITUDE") != pl.col("LONGITUDE_right"))
)
```

## –†–æ–∑—Ä–∞—Ö—É–≤–∞—Ç–∏ –≤—ñ–¥—Å—Ç–∞–Ω—å –ø–æ –≤–µ–ª–∏–∫–æ–º—É –∫–æ–ª—É

$$
d = 2r \cdot \arccos(\sin(\varphi_1) \cdot \sin(\varphi_2) + \cos(\varphi_1) \cdot \cos(\varphi_2) \cdot \cos(\theta_1 - \theta_2))
$$

::: footer
[Great-circle distance](https://en.wikipedia.org/wiki/Great-circle_distance#:~:text=The%20great%2Dcircle%20distance%2C%20orthodromic,the%20surface%20of%20the%20sphere.)
:::

::: {.panel-tabset}
## Polars

``` {python}
def deg2rad_pl(degrees: pl.Expr) -> pl.Expr:
    return degrees * math.pi / 180

def gcd_pl(lat1: pl.Expr, lng1: pl.Expr, lat2: pl.Expr, lng2: pl.Expr):
    œï1 = deg2rad_pl(90 - lat1)
    œï2 = deg2rad_pl(90 - lat2)

    Œ∏1 = deg2rad_pl(lng1)
    Œ∏2 = deg2rad_pl(lng2)

    cos = œï1.sin() * œï2.sin() * (Œ∏1 - Œ∏2).cos() + œï1.cos() * œï2.cos()
    arc = cos.arccos()
    return arc * 6373
```

## NumPy

``` {python}
def gcd_np(lat1, lng1, lat2, lng2):
    œï1 = np.deg2rad(90 - lat1)
    œï2 = np.deg2rad(90 - lat2)

    Œ∏1 = np.deg2rad(lng1)
    Œ∏2 = np.deg2rad(lng2)

    cos = np.sin(œï1) * np.sin(œï2) * np.cos(Œ∏1 - Œ∏2) + np.cos(œï1) * np.cos(œï2)
    arc = np.arccos(cos)
    return arc * 6373
```
:::

## –†–æ–∑—Ä–∞—Ö—É–≤–∞—Ç–∏ –≤—ñ–¥—Å—Ç–∞–Ω—å –ø–æ –≤–µ–ª–∏–∫–æ–º—É –∫–æ–ª—É

::: {.panel-tabset}
## Polars

``` {python}
%%timeit
pairs.select(
    gcd_pl(
        pl.col("LATITUDE"),
        pl.col("LONGITUDE"),
        pl.col("LATITUDE_right"),
        pl.col("LONGITUDE_right")
    )
).collect()
```

## NumPy

``` {python}
%%timeit
pairs.select(
    gcd_np(
        pl.col("LATITUDE"),
        pl.col("LONGITUDE"),
        pl.col("LATITUDE_right"),
        pl.col("LONGITUDE_right")
    )
).collect()
```
:::

## Polars —ñ–Ω–∫–æ–ª–∏ –ø–æ–≤—ñ–ª—å—à–∏–π –∑–∞ Pandas {.tiny .scrollable}

```{python}
def create_frame(n, n_groups):
    return pl.DataFrame(
        {"name": np.random.randint(0, n_groups, size=n), "value2": np.random.randn(n)}
    )

def pandas_transform(df: pd.DataFrame) -> pd.DataFrame:
    g = df.groupby("name")["value2"]
    v = df["value2"]
    return (v - g.transform("mean")) / g.transform("std")


def polars_transform() -> pl.Expr:
    v = pl.col("value2")
    return (v - v.mean().over("name")) / v.std().over("name")

rand_df_pl = create_frame(50_000_000, 50_000)
rand_df_pd = rand_df_pl.to_pandas()
```

::: {.panel-tabset}
## Polars
``` {python}
%timeit rand_df_pl.select(polars_transform())
```
## Pandas
``` {python}
%timeit pandas_transform(rand_df_pd)
```
:::

# –û—Ö–∞–π–Ω—ñ –¥–∞–Ω—ñ

## –û—Ö–∞–π–Ω—ñ –¥–∞–Ω—ñ

–Ü—Å–Ω—É—î [—Ü—ñ–ª–∞ —Å—Ç–∞—Ç—Ç—è](https://vita.had.co.nz/papers/tidy-data.pdf) –•–µ–¥–ª—ñ –í—ñ–∫—Ö–µ–º (Hadley Wickham) –ø—Ä–æ —á–∏—Å—Ç—ñ –¥–∞–Ω—ñ, –∞–ª–µ –≤–æ–Ω–∞ —É —Ñ–æ—Ä–º–∞—Ç—ñ PDF, —Ç–æ–º—É –≤–∏, –º–∞–±—É—Ç—å, –Ω–µ –±—É–¥–µ—Ç–µ —ó—ó —á–∏—Ç–∞—Ç–∏. –û—Å—å –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —á–∏—Å—Ç–∏—Ö –¥–∞–Ω–∏—Ö, –Ω–∞–≤–µ–¥–µ–Ω–µ –≤ —Ü—ñ–π —Å—Ç–∞—Ç—Ç—ñ:

1. –ö–æ–∂–Ω–∞ –∑–º—ñ–Ω–Ω–∞ —É—Ç–≤–æ—Ä—é—î —Å—Ç–æ–≤–ø–µ—Ü—å.
2. –ö–æ–∂–Ω–µ —Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è —Ñ–æ—Ä–º—É—î —Ä—è–¥–æ–∫.
3. –ö–æ–∂–µ–Ω —Ç–∏–ø –æ–¥–∏–Ω–∏—Ü—ñ —Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è —Ñ–æ—Ä–º—É—î —Ç–∞–±–ª–∏—Ü—é.

## –î–∞–Ω—ñ NBA {.tiny .scrollable}

```{python}
#| output-location: column
from pathlib import Path
import polars as pl
import pandas as pd

pl.Config.set_tbl_rows(5)
pd.options.display.max_rows = 5

nba_dir = Path("data/nba/")

column_names = {
    "Date": "date",
    "Visitor/Neutral": "away_team",
    "PTS": "away_points",
    "Home/Neutral": "home_team",
    "PTS.1": "home_points",
}

if not nba_dir.exists():
    nba_dir.mkdir()
    for month in (
        "october",
        "november",
        "december",
        "january",
        "february",
        "march",
        "april",
        "may",
        "june",
    ):
        # –ù–∞ –ø—Ä–∞–∫—Ç–∏—Ü—ñ –º–∏ –± –∑—Ä–æ–±–∏–ª–∏ –±—ñ–ª—å—à–µ –æ—á–∏—â–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö —Ç—É—Ç, —ñ –∑–±–µ—Ä–µ–≥–ª–∏ –± —É –ø–∞—Ä–∫–µ—Ç, –∞ –Ω–µ CSV.
        # –ê–ª–µ –º–∏ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –±—Ä—É–¥–Ω—ñ –¥–∞–Ω—ñ —Ç—É—Ç, —â–æ–± –ø–æ—Ç—ñ–º –æ—á–∏—Å—Ç–∏—Ç–∏ —ó—Ö –¥–ª—è –ø–µ–¥–∞–≥–æ–≥—ñ—á–Ω–∏—Ö —Ü—ñ–ª–µ–π.
        url = f"http://www.basketball-reference.com/leagues/NBA_2016_games-{month}.html"
        tables = pd.read_html(url)
        raw = (
            pl.from_pandas(tables[0].query("Date != 'Playoffs'"))
            .rename(column_names)
            .select(column_names.values())
        )
        raw.write_csv(nba_dir / f"{month}.csv")

nba_glob = nba_dir / "*.csv"
pl.scan_csv(nba_glob).head().collect()
```

## –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–∏—Ö {.tiny}

::: {.panel-tabset}
## Polars

``` {python}
games_pl = (
    pl.scan_csv(nba_glob)
    .with_columns(
        pl.col("date").str.strptime(pl.Date, "%a, %b %d, %Y"),
    )
    .sort("date")
    .with_row_index("game_id")
)
games_pl.head().collect()
```

## Pandas

``` {python}
games_pd = (
    pl.read_csv(nba_glob)
    .to_pandas()
    .dropna(how="all")
    .assign(date=lambda x: pd.to_datetime(x["date"], format="%a, %b %d, %Y"))
    .sort_values("date")
    .reset_index(drop=True)
    .set_index("date", append=True)
    .rename_axis(["game_id", "date"])
    .sort_index()
)
games_pd.head()
```
:::

## Pivot {.tiny}

–ü—Ä–∏–ø—É—Å—Ç–∏–º–æ, —É –≤–∞—Å —î —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–∞–Ω–∏—Ö, —è–∫–∏–π –≤–∏–≥–ª—è–¥–∞—î —Ç–∞–∫:

``` {python}
#| code-fold: true
from datetime import date
prices = pl.DataFrame({
    "date": [*[date(2020, 1, 1)]*4, *[date(2020, 1, 2)]*4, *[date(2020, 1, 3)]*4],
    "ticker": [*["AAPL", "TSLA", "MSFT", "NFLX"]*3],
    "price": [100, 200, 300, 400, 110, 220, 330, 420, 105, 210, 315, 440],
})
prices
```

–Ü –≤ Polars, —ñ –≤ Pandas –≤–∏ –º–æ–∂–µ—Ç–µ –≤–∏–∫–ª–∏–∫–∞—Ç–∏ `df.pivot`, —â–æ–± –æ—Ç—Ä–∏–º–∞—Ç–∏ —Ñ—Ä–µ–π–º –¥–∞–Ω–∏—Ö, —è–∫–∏–π –≤–∏–≥–ª—è–¥–∞—î —Ç–∞–∫:

```{python}
#| code-fold: true
pivoted = prices.pivot(index="date", values="price", on="ticker")
pivoted
```

## Melt/ Unpivot {.tiny}

```{python}
pivoted.unpivot(index="date", value_name="price")
```

## Tidy NBA data {.tiny .scrollable}

–ü—Ä–∏–ø—É—Å—Ç–∏–º–æ, –º–∏ —Ö–æ—á–µ–º–æ –ø–æ—Ä–∞—Ö—É–≤–∞—Ç–∏ –¥–Ω—ñ –≤—ñ–¥–ø–æ—á–∏–Ω–∫—É –∫–æ–∂–Ω–æ—ó –∫–æ–º–∞–Ω–¥–∏ –ø–µ—Ä–µ–¥ –∫–æ–∂–Ω–æ—é –≥—Ä–æ—é. –£ –ø–æ—Ç–æ—á–Ω—ñ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ñ —Ü–µ —Å–∫–ª–∞–¥–Ω–æ, –æ—Å–∫—ñ–ª—å–∫–∏ –Ω–∞–º –ø–æ—Ç—Ä—ñ–±–Ω–æ –≤—ñ–¥—Å—Ç–µ–∂—É–≤–∞—Ç–∏ –æ–±–∏–¥–≤–∞ —Å—Ç–æ–≤–ø—Ü—ñ `home_team` —ñ `away_team`

::: {.panel-tabset}
## Polars

``` {python}
#| output-location: column
tidy_pl = (
    games_pl
    .unpivot(
        index=["game_id", "date"],
        on=["away_team", "home_team"],
        value_name="team",
    )
    .sort("game_id")
    .with_columns((
        pl.col("date")
        .alias("rest")
        .diff().over("team")
        .dt.total_days() - 1).cast(pl.Int8))
    .drop_nulls("rest")
    .collect()
)
tidy_pl
```

## Pandas
``` {python}
#| output-location: column
tidy_pd = (
    games_pd.reset_index()
    .melt(
        id_vars=["game_id", "date"],
        value_vars=["away_team", "home_team"],
        value_name="team",
    )
    .sort_values("game_id")
    .assign(
        rest=lambda df: (
            df
            .sort_values("date")
            .groupby("team")
            ["date"]
            .diff()
            .dt.days
            .sub(1)
        )
    )
    .dropna(subset=["rest"])
    .astype({"rest": pd.Int8Dtype()})
)
tidy_pd
```
:::

## Tidy NBA data {.tiny .scrollable}

–¢–µ–ø–µ—Ä –º–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ `.pivot`, —â–æ–± –¥–æ–¥–∞—Ç–∏ —Ü—ñ –¥–∞–Ω—ñ –ø—Ä–æ –¥–Ω—ñ –≤—ñ–¥–ø–æ—á–∏–Ω–∫—É –Ω–∞–∑–∞–¥ –¥–æ –≤–∏—Ö—ñ–¥–Ω–æ–≥–æ —Ñ—Ä–µ–π–º—É –¥–∞–Ω–∏—Ö.

::: {.panel-tabset}
## Polars
``` {python}
by_game_pl = (
    tidy_pl
    .pivot(
        values="rest",
        index=["game_id", "date"],
        on="variable"
    )
    .rename({"away_team": "away_rest", "home_team": "home_rest"})
)
joined_pl = (
    by_game_pl
    .join(games_pl.collect(), on=["game_id", "date"])
    .with_columns([
        pl.col("home_points").alias("home_win") > pl.col("away_points"),
        pl.col("home_rest").alias("rest_spread") - pl.col("away_rest"),
    ])
)
joined_pl
```
## Pandas
``` {python}
by_game_pd = (
    tidy_pd
    .pivot(
        values="rest",
        index=["game_id", "date"],
        columns="variable"
    )
    .rename(
        columns={"away_team": "away_rest", "home_team": "home_rest"}
    )
)
joined_pd = by_game_pd.join(games_pd).assign(
    home_win=lambda df: df["home_points"] > df["away_points"],
    rest_spread=lambda df: df["home_rest"] - df["away_rest"],
)
joined_pd
```
:::

## Tidy NBA data vizualization {.tiny .scrollable}

``` {python}
#| output-location: column
import seaborn as sns
sns.set_theme(font_scale=0.6)
sns.catplot(
    tidy_pl.to_pandas(),
    x="variable",
    y="rest",
    col="team",
    col_wrap=5,
    kind="bar",
    height=1.5,
)
```

## Tidy NBA data vizualization {.tiny .scrollable}

–ü–æ–±—É–¥–æ–≤–∞ –≥—Ä–∞—Ñ—ñ–∫–∞ —Ä–æ–∑–ø–æ–¥—ñ–ª—É `rest_spread`:

::: {.panel-tabset}
## Polars
``` {python}
#| output-location: column
import numpy as np
delta_pl = joined_pl["rest_spread"]
ax = (
    delta_pl
    .value_counts()
    .drop_nulls()
    .to_pandas()
    .set_index("rest_spread")
    ["count"]
    .reindex(np.arange(delta_pl.min(), delta_pl.max() + 1), fill_value=0)
    .sort_index()
    .plot(kind="bar", color="k", width=0.9, rot=0, figsize=(9, 6))
)
ax.set(xlabel="Difference in Rest (Home - Away)", ylabel="Games")
```
## Pandas
``` {python}
#| output-location: column
delta_pd = joined_pd["rest_spread"]
ax = (
    delta_pd
    .value_counts()
    .reindex(np.arange(delta_pd.min(), delta_pd.max() + 1), fill_value=0)
    .sort_index()
    .plot(kind="bar", color="k", width=0.9, rot=0, figsize=(9, 6))
)
ax.set(xlabel="Difference in Rest (Home - Away)", ylabel="Games")
```
:::

## Tidy NBA data vizualization {.tiny .scrollable}

–ü–æ–±—É–¥–æ–≤–∞ –≥—Ä–∞—Ñ—ñ–∫–∞ –≤—ñ–¥—Å–æ—Ç–∫–∞ –≤–∏–≥—Ä–∞—à—É –∑–∞ `rest_spread`:

::: {.panel-tabset}
## Polars
``` {python}
#| output-location: column
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(9, 6))
sns.barplot(
    x="rest_spread",
    y="home_win",
    data=joined_pl.filter(pl.col("rest_spread").is_between(-3, 3, closed="both")).to_pandas(),
    color="#4c72b0",
    ax=ax,
)
```
## Pandas
``` {python}
#| output-location: column
fig, ax = plt.subplots(figsize=(9, 6))
sns.barplot(
    x="rest_spread",
    y="home_win",
    data=joined_pd.query('-3 <= rest_spread <= 3'),
    color="#4c72b0",
    ax=ax,
)
```
:::

## –©–æ—Å—å —â–µ? {.tiny}

- [DuckDB](https://duckdb.org/): —Å—Ö–æ–∂–µ –Ω–∞ SQLite, –∞–ª–µ –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é Pandas —ñ Polars.
- [Dask](https://dask.org/): —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –¥–ª—è Pandas.
- [data.table](https://rdatatable.gitlab.io/data.table/index.html): —à–≤–∏–¥–∫–∞ –æ–±—Ä–æ–±–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è R.
- [Ibis](https://ibis-project.org/): –æ–±—Ä–æ–±–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è Pandas, Polars, SQLAlchemy, DuckDB, PySpark, BigQuery.
- [cuDF](https://rapids.ai/): –æ–±—Ä–æ–±–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è GPU.
- [Vaex](https://vaex.io/): –æ–±—Ä–æ–±–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è –≤–µ–ª–∏–∫–∏—Ö –Ω–∞–±–æ—Ä—ñ–≤ –¥–∞–Ω–∏—Ö.
- [Apache Arrow](https://arrow.apache.org/): —É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π —Å—Ç–æ–≤–ø—á–∞—Å—Ç–∏–π —Ñ–æ—Ä–º–∞—Ç —ñ –±–∞–≥–∞—Ç–æ–º–æ–≤–Ω–∏–π —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞—Ä—ñ–π –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –æ–±–º—ñ–Ω—É –¥–∞–Ω–∏–º–∏ —Ç–∞ –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏ –≤ –ø–∞–º'—è—Ç—ñ
- [PySpark](https://spark.apache.org/docs/latest/api/python/index.html): –±–∞–≥–∞—Ç–æ–º–æ–≤–Ω–∏–π —Ä—É—à—ñ–π –¥–ª—è –≤–∏–∫–æ–Ω–∞–Ω–Ω—è —ñ–Ω–∂–µ–Ω–µ—Ä—ñ—ó –¥–∞–Ω–∏—Ö, –Ω–∞—É–∫–∏ –ø—Ä–æ –¥–∞–Ω—ñ —Ç–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –æ–¥–Ω–æ–≤—É–∑–ª–æ–≤–∏—Ö –º–∞—à–∏–Ω–∞—Ö –∞–±–æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö.


# –î—è–∫—É—é –∑–∞ —É–≤–∞–≥—É! {.unnumbered .unlisted background-iframe=".01_files/libs/colored-particles/index.html"}

<br> <br>

{{< iconify solar book-bold >}} [–ú–∞—Ç–µ—Ä—ñ–∞–ª–∏ –∫—É—Ä—Å—É](https://aranaur.rbind.io/lectures/mm_big_data/)

{{< iconify mdi envelope >}} ihor.miroshnychenko\@knu.ua

{{< iconify ic baseline-telegram >}} [Data Mirosh](https://t.me/araprof)

{{< iconify mdi linkedin >}} [\@ihormiroshnychenko](https://www.linkedin.com/in/ihormiroshnychenko/)

{{< iconify mdi github >}} [\@aranaur](https://github.com/Aranaur)

{{< iconify ion home >}} [aranaur.rbind.io](https://aranaur.rbind.io)
