---
title: "Advanced imports and data manipulations"
subtitle: "STAT150 | R for Data Science"
author: "Ihor Miroshnychenko"
institute: Kyiv School of Economics
from: markdown+emoji
title-slide-attributes:
    data-background-iframe: .06_files/libs/colored-particles/index.html
footer: <a href="https://teaching.kse.org.ua/course/view.php?id=3374">üîóSTAT150 | R for Data Science</a>
format:
  revealjs: 
    code-line-numbers: false
    navigation-mode: vertical
    transition: fade
    background-transition: fade
    chalkboard: true
    logo: img/kse.png
    slide-number: true
    toc: true
    toc-depth: 1
    mouse-wheel: true
    width: 1350  
    height: 759.375
    highlight-style: github
    fig-format: svg
    fig-align: center
    theme: [default, custom.scss]
    mermaid:
      theme: forest
preload-iframes: true
execute: 
  echo: true
  warning: false
editor_options: 
  chunk_output_type: console

revealjs-plugins:
  - verticator
---

```{r}
#| label: setup
#| include: false

library(tidyverse)

# Define colors
red_pink   = "#e64173"
turquoise  = "#20B2AA"
orange     = "#FFA500"
red        = "#fb6107"
blue       = "#181485"
navy       = "#150E37FF"
green      = "#8bb174"
yellow     = "#D8BD44"
purple     = "#6A5ACD"
slate      = "#314f4f"
```

# Google Sheets

## Prerequisites

```{r}
#| warning: false
#| message: false
library(googlesheets4)
library(tidyverse)
```

## Reading data

- `read_sheet()` reads data from a Google Sheet into a tibble.
- `range_read()` reads a specific range of cells.
- `gs4_create()` creates a new Google Sheet.
- `sheet_write()` writes data to a Google Sheet.

## Authentication

- The first time you use `googlesheets4`, it will prompt you to authenticate with your Google account.
- You can use `gs4_auth()` to manage authentication.
- Use `gs4_deauth()` to access public sheets without authentication.

```{r}
gs4_auth(email = "imiroshnychenko@kse.org.ua")
```

::: footer
[googlesheets4 auth](https://googlesheets4.tidyverse.org/articles/auth.html)
:::

## Sheet ID Extraction

```{r}
url <- "https://docs.google.com/spreadsheets/d/1yV6Vg1H0buf93PmlTJApvZ2-aaR6mfFedQmIKrpgANU/edit?usp=sharing"
students_sheet_id  <- str_extract(url, "(?<=/d/)[a-zA-Z0-9-_]+")
```

- `(?<=/d/)`: ensures the match comes after `/d/`
- `[a-zA-Z0-9-_]+`: matches one or more characters that are either uppercase letters, lowercase letters, digits, hyphens, or underscores

## Reading a Sheet

```{r}
students  <- read_sheet(
  students_sheet_id,
  na = c("", "NA"),
  sheet = "r4ds"
  )
students
```

## Examples from the package

```{r}
deaths_url <- gs4_example("deaths")
deaths <- read_sheet(deaths_url, range = "A5:F15")
deaths
```

## Writing to a Sheet

- `sheet_write()` writes a data frame to a specified sheet within a Google Sheets document.

```{r}
new_sheet <- tibble(
  ABC = LETTERS,
  norm = rnorm(length(LETTERS))
)

sheet_write(new_sheet, ss = students_sheet_id, sheet = "random_data")
```

# Databases

## Prerequisites

```{r}
#| warning: false
#| message: false

if (!requireNamespace("pacman")) install.packages("pacman")

pacman::p_load(
  DBI,
  duckdb,
  arrow,
  tidyverse
)

Sys.setenv(NOT_CRAN = "true")
if (!requireNamespace("polars")) install.packages("polars", repos = "https://community.r-multiverse.org")
if (!requireNamespace("tidypolars")) install.packages("tidypolars", repos = "https://community.r-multiverse.org")
```

## Types of Databases

- Row-oriented databases (e.g., PostgreSQL, MySQL, SQLite)
    + Suitable for transactional applications
- Column-oriented databases (e.g., DuckDB, ClickHouse)
    + Optimized for analytical queries
    + Store data in columns rather than rows

## Types of DBMS

DBMS (Database Management System) is software for creating and managing databases.

- Client-server:
    + PostgreSQL, MySQL, Microsoft SQL Server, Oracle Database
- Cloud:
    + Amazon RedShift, Google BigQuery, Snowflake
- In-process:
    + SQLite, DuckDB

## NYC taxi data

We will use [New York City taxi](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) data.

- Yellow taxi trips in NYC for the year 2012
- Data is partitioned by month
- Each month's data is stored in a separate Parquet file
- [Parquet](https://parquet.apache.org/) is a columnar storage file format optimized for use with big data processing frameworks
- The final dataset is ~8.5 GB compressed on disk and can take 10-20 minutes to download, depending on your internet connection.

## NYC taxi data sources {.tiny}

::: {.panel-tabset}

### From S3

```{r}
#| eval: false
library(arrow)
library(dplyr)

data_path = "nyc-taxi"

open_dataset("s3://voltrondata-labs-datasets/nyc-taxi/year=2012") |>
    write_dataset(data_path, partitioning = "month")
```

### From Google drive \w R

```{r}
#| eval: false
library(googledrive)
library(purrr)

data_path = "nyc-taxi"

jp_folder = "https://drive.google.com/drive/folders/1t4gHy4eJM47nQE3Z2mh4tid6eLbl6goJ"
folder_id = drive_get(as_id(jp_folder))

files = drive_ls(folder_id)

pmap(files, function(name, id, ...) { # 
  local_dir <- file.path(data_path, name)
  dir.create(local_dir, recursive = TRUE, showWarnings = FALSE)
  
  # —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª—ñ–≤ —É –ø—ñ–¥–ø–∞–ø—Ü—ñ
  i_dir <- drive_ls(as_id(id))
  
  # –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ñ–∞–π–ª—ñ–≤
  pwalk(list(i_dir$id, i_dir$name), function(file_id, file_name) {
    try({
      drive_download(
        as_id(file_id),
        path = file.path(local_dir, file_name),
        overwrite = FALSE
      )
    })
  })
})
```
:::

## File structure {.tiny}

- `fs` package provides cross-platform file system operations

```{r}
#| output-location: column
library(fs)

data_path = "nyc-taxi"
dir_tree(data_path)
```

::: footer
[fs package](https://fs.r-lib.org/)
:::

## Size on disk

- Calculate the total size of all Parquet files in the `nyc-taxi` directory and its subdirectories

```{r}
fs::dir_ls(data_path, recurse = TRUE, glob = "*.parquet") |> 
  fs::file_size() |> 
  sum() / (1024^3) |>
  round(2) |> 
  paste("GB")
```

## Machine specs {.smaller}

- `benchmarkme` package provides functions to benchmark your R setup and get system information

. . .

```{r}
cat("CPU:\n", benchmarkme::get_cpu()$model_name, "\n")
cat("RAM:\n", benchmarkme::get_ram() / (1024^3), "GB\n")
```

# DuckDB

## Load libraries

- `DBI` provides a common interface for interacting with databases in R.
- `duckdb` is an in-process SQL OLAP database management system.
- `dbConnect()` establishes a connection to a DuckDB database.

```{r}
library(duckdb)
con = dbConnect(duckdb(), shutdown = TRUE)
```

## Bigger than RAM

- DuckDB is designed to handle datasets larger than the available RAM.
- It uses efficient on-disk storage and processing techniques to manage large datasets.
- It employs vectorized execution and columnar storage to optimize performance.

::: footer
[No Memory? No Problem. External Aggregation in DuckDB](https://duckdb.org/2024/03/29/external-aggregation.html)
:::

## First example

> What is the average tip per passenger count?

. . .

```{sql}
#| eval: false
SELECT
  passenger_count,
  AVG(tip_amount) AS mean_tip
FROM 'nyc-taxi/**/*.parquet'
GROUP BY passenger_count
ORDER BY passenger_count
```

## DuckDB's "friendly SQL"

- DuckDB allows you to use a more concise SQL syntax for certain operations.

```{sql}
#| eval: false
FROM 'nyc-taxi/**/*.parquet'
SELECT
  passenger_count,
  AVG(tip_amount) AS mean_tip
GROUP BY ALL
ORDER BY ALL
```

::: footer
[Friendly SQL](https://duckdb.org/docs/stable/sql/dialect/friendly_sql.html)
:::

## Let's run it

```{r}
#| output-location: column
tic = Sys.time()
dat1 = dbGetQuery(
  con,
  "
  FROM 'nyc-taxi/**/*.parquet'
  SELECT
    passenger_count,
    AVG(tip_amount) AS mean_tip
  GROUP BY ALL
  ORDER BY ALL
  "
)
toc = Sys.time()

dat1

toc - tic
```

Wow! ~20 millions of rows in that dataset!

## Aggregation {.smaller}

> How does the average tip change over the months?

```{r}
#| output-location: column
tic = Sys.time()
dat2 = dbGetQuery(
  con,
  "
  FROM 'nyc-taxi/**/*.parquet'
  SELECT
    month,
    passenger_count,
    AVG(tip_amount) AS mean_tip
  WHERE month <= 3
  GROUP BY ALL
  "
    )
toc = Sys.time()

head(dat2)
toc - tic
```

Why it faster than the previous query? Reason: Subsetting along our [Hive-partitioned](https://duckdb.org/docs/stable/data/partitioning/hive_partitioning.html) parquet dataset allows DuckDB to take shortcuts.

## `EXPLAIN` query plan {.tiny}

- `EXPLAIN` provides a detailed query execution plan, showing how DuckDB will execute the SQL query.

```{r}
#| output-location: column
dbGetQuery(
  con,
  "
  EXPLAIN
    FROM 'nyc-taxi/**/*.parquet'
    SELECT
      month,
      passenger_count,
      AVG(tip_amount) AS mean_tip
    WHERE month <= 3
    GROUP BY ALL
  "
)
```

## More examples {.tiny}

> What is the average tip and fare per passenger count and trip distance?

```{r}
#| output-location: column

tic = Sys.time()
dat3 = dbGetQuery(
  con,
  "
  FROM 'nyc-taxi/**/*.parquet'
  SELECT
    passenger_count,
    trip_distance,
    AVG(tip_amount) AS mean_tip,
    AVG(fare_amount) AS mean_fare
  GROUP BY ALL
"
)
toc = Sys.time()

nrow(dat3)
head(dat3)
toc - tic
```

## Pivot {.smaller}

- `UNPIVOT` transforms columns into rows, effectively rotating the data.
- `PIVOT` transforms rows into columns, allowing for aggregation of data.

```{r}
#| output-location: column

tic = Sys.time()
dat_long = dbGetQuery(
  con,
  "
  WITH tmp_table AS (
    FROM 'nyc-taxi/**/*.parquet'
    SELECT
      passenger_count,
      trip_distance,
      AVG(tip_amount) AS mean_tip,
      AVG(fare_amount) AS mean_fare
    GROUP BY ALL
  )
  UNPIVOT tmp_table
  ON mean_tip, mean_fare
  INTO
    NAME variable
    VALUE amount
  "
)
toc = Sys.time()

head(dat_long)
toc - tic
```

## Create new tables

- `CREATE TABLE AS` creates a new table and populates it with the result of a query.

```{r}
#| output-location: column

tic = Sys.time()
dbExecute(
    con,
    "
    CREATE TABLE taxi2 AS
      FROM 'nyc-taxi/**/*.parquet'
      SELECT
        passenger_count,
        trip_distance,
        AVG(tip_amount) AS mean_tip,
        AVG(fare_amount) AS mean_fare
      GROUP BY ALL
    "
)
toc = Sys.time()

dbListTables(con)
```

## Back to reshaping {.smaller}

- `UNPIVOT` on the newly created table

```{r}
#| output-location: column

dbGetQuery(
  con,
  "
  UNPIVOT taxi2
  ON mean_tip, mean_fare
  INTO
    NAME variable
    VALUE amount
  LIMIT 5
  "
)
```

## Joins {.smaller}

What this query does?

```{r}
#| output-location: column-fragment

tic = Sys.time()
dbGetQuery(
  con,
  "
  WITH 
    mean_tips AS (
      FROM 'nyc-taxi/**/*.parquet'
      SELECT
        month,
        AVG(tip_amount) AS mean_tip
      GROUP BY month
    ),
    mean_fares AS (
      FROM 'nyc-taxi/**/*.parquet'
      SELECT
        month,
        AVG(fare_amount) AS mean_fare
      GROUP BY month 
    )
  FROM mean_tips
  LEFT JOIN mean_fares
  USING (month)
  SELECT *
  ORDER BY mean_tips.month
  "
)
toc = Sys.time()
toc - tic
```

## Windowing {.tiny}

- `OVER` clause defines a [window](https://duckdb.org/2021/10/13/windowing.html) (set of rows) for the function to operate on.

> What is the average tip and trip distance for each decile of trip distance?

```{r}
#| output-location: column-fragment

tic = Sys.time()
dbGetQuery(
  con,
  "
  WITH trip_deciles AS (
    FROM 'nyc-taxi/**/*.parquet'
    SELECT
      tip_amount,
      trip_distance,
      NTILE(10) OVER (ORDER BY trip_distance) AS decile
    USING SAMPLE 1%
  )
  FROM trip_deciles
  SELECT
    decile,
    AVG(trip_distance) AS mean_distance,
    AVG(tip_amount) AS mean_tip
  GROUP BY ALL
  ORDER BY ALL
  "
)
toc = Sys.time()

toc - tic
```

::: {.callout-warning}
I'm using a 1% random sample of my data here (see the [`USING SAMPLE 1%`](https://duckdb.org/docs/stable/sql/query_syntax/sample) statement). Why? 
:::

## Close connection

```{r}
dbDisconnect(con)
```


# DuckDB + dplyr


## Load libraries
```{r}
library(duckdb)
library(dplyr, warn.conflicts = FALSE)
library(tidyr, warn.conflicts = FALSE)
```

## Create a database connection

- `dplyr::tbl()` creates a lazy table reference to a database table.

```{r}
con <- dbConnect(duckdb(), shutdown = TRUE)

nyc <- tbl(con, "read_parquet('nyc-taxi/**/*.parquet', hive_partitioning = true)") # <1>
```

1. `nyc = tbl(con, "nyc-taxi/**/*.parquet")` will also work, but safer to use the read_parquet function.

## First example

This next command runs instantly because all **computation is deferred** (i.e., lazy eval). In other words, it is just a **query object**.

```{r}
q1 <-  nyc |>
  summarize(
    mean_tip = mean(tip_amount),
    .by = passenger_count
  )
```

## Explaining the query {.tiny}

- `explain()` shows the SQL query that will be sent to the database when the query is executed.

```{r}
#| output-location: column
explain(q1)
```

## Show the SQL query

- `show_query()` displays the SQL query that will be executed.

```{r}
show_query(q1)
```

## Execute the query {.tiny}

- `collect()` executes the query and retrieves the results into R.

```{r}
#| output-location: column
tic <-  Sys.time()
dat1 <- collect(q1)
toc <- Sys.time()

dat1
```

## Aggregation {.smaller}

> How does the average tip and fare change over the months and passenger count?

```{r}
#| output-location: column
q2 <- nyc |>
  filter(month <= 3) |>
  summarize(
    across(c(tip_amount, fare_amount), mean),
    .by = c(month, passenger_count)
  )
q2
```

## Explain the query {.tiny .scrollable}

```{r}
#| output-location: column
explain(q2)
```

## More aggregation {.smaller}

```{r}
#| output-location: column
q3 <- nyc |>
  group_by(passenger_count, trip_distance) |>
  summarize(
    across(c(tip_amount, fare_amount), mean),
  ) 
collect(q3)
```

## Pivot {.smaller}

```{r}
#| output-location: column

q3 |>
  pivot_longer(tip_amount:fare_amount) |>
  collect()
```

## Joins {.smaller}

```{r}
#| output-location: column
mean_tips  <- nyc |> summarise(
  mean_tips = mean(tip_amount),
  .by = month)

mean_fares <- nyc |> summarise(
  mean_fares = mean(fare_amount),
  .by = month)

left_join(
  mean_fares,
  mean_tips
  ) |>
  collect()
```

## Windowing

::: {.callout-caution}
Sampling with dplyr is less efficient than DuckDB's native `USING SAMPLE` SQL, so creating deciles on large data may be slower.
:::

# Arrow + duckdb

## Load libraries

- `arrow` provides a high-performance interface for reading and writing Parquet files.

```{r}
#| warning: false
#| message: false
library(arrow)
library(duckdb)
library(dplyr)
library(tidyr)
```

## Dataset

With `arrow`, you can access the dataset directly using `open_dataset()` - no need to connect to a database.

```{r}
nyc2 <- open_dataset("nyc-taxi")
```

## Take a look

Printing `nyc2` shows its column names and types.

```{r}
#| output-location: column

nyc2
```

## To duckdb {.tiny}

Use `to_duckdb()` to convert the Arrow dataset for use with DuckDB and `dplyr`.

Arrow and DuckDB work together efficiently --- [no data is copied](https://duckdb.org/2021/12/03/duck-arrow.html), just pointers are shared for fast access.

```{r}
#| output-location: column
to_duckdb(nyc2)
```

## Example {.smaller}

All dplyr verbs work as expected.

```{r}
#| output-location: column
nyc2 |>
  to_duckdb() |>
  summarise(
    mean_tip = mean(tip_amount),
    .by = passenger_count
  ) |>
  collect()
```

## Experimental: `duckplyr` {.smaller}

[`duckplyr`](https://duckplyr.tidyverse.org/) is a [new package](https://duckdb.org/2024/04/02/duckplyr.html) that lets you use `dplyr` syntax, but powered by DuckDB for fast data manipulation.

You can swap `library(dplyr)` for `library(duckplyr)` and get efficient queries, even on regular R data frames.

```{r}
#| output-location: column
library(duckplyr, warn.conflicts = FALSE)

duckplyr_df_from_parquet("nyc-taxi/**/*.parquet") |>
  summarise(
    mean_tip = mean(tip_amount),
    .by = passenger_count
  )
```

# Polars

## Load libraries

- `polars` is a fast DataFrame library implemented in Rust.

```{r}
#| warning: false
library(polars)
```

## Scan data

- `scan_parquet()` reads Parquet files into a Polars DataFrame.

```{r}
nyc <- pl$scan_parquet("nyc-taxi/**/*.parquet", hive_partitioning=TRUE)
nyc
```

## First example

Polars operations are registered as queries until they are collected.

```{r}
q1 <- (
    nyc
    $group_by("passenger_count")
    $agg(
        pl$col("tip_amount")$mean()#$alias("mean_tip") ## alias is optional
    )
    $sort("passenger_count")
)
q1
```

## Polars style

Polars chains methods with `$`, similar to Python. For multiline queries, wrap in parentheses and put each method on a new line.

```{r}
#| eval: false
nyc$group_by(
    "passenger_count"
)$agg(
    pl$col("tip_amount")$mean()
)$sort("passenger_count")
```

## Collect the result {.smaller}

- `collect()` executes the query and retrieves the results into R.

```{r}
#| output-location: column
tic = Sys.time()
dat1 = q1$collect()
toc = Sys.time()

dat1
toc - tic
```

## Aggregation {.smaller}

- How does the average tip change over the months and passenger count?
- `q2` is a query object until collected.
- `q2$explain()` shows the optimized query plan.

```{r}
#| output-location: column
q2 <- (
    nyc
    $filter(pl$col("month") <= 3)
    $group_by("month", "passenger_count")
    $agg(pl$col("tip_amount")$mean()$alias("mean_tip"))
    $sort("passenger_count")
) 

# q2              
cat(q2$explain())
```

## Run and collect {.smaller}

```{r}
#| output-location: column

tic = Sys.time()
dat2 = q2$collect()
toc = Sys.time()

dat2
toc - tic
```

## High-dimensional grouping example {.smaller}

- This query provides an example where `polars` is noticeably **slower** than DuckDB.

```{r}
#| output-location: column
q3 <- (
    nyc
    $group_by("passenger_count", "trip_distance")
    $agg(
        pl$col("tip_amount")$mean()$alias("mean_tip"),
        pl$col("fare_amount")$mean()$alias("mean_fare")
        )
    $sort("passenger_count", "trip_distance")
)

tic <- Sys.time()
dat3 <- q3$collect()
toc <- Sys.time()

dat3
toc - tic
```

## Pivot {.smaller}

- `pivot` $\rightarrow$ long to wide transformation
- `unpivot` $\rightarrow$ wide to long transformation

```{r}
#| output-location: column

dat3$unpivot(index = c("passenger_count", "trip_distance"))
```

## Joins {.smaller}

```{r}
#| output-location: column

mean_tips  = nyc$group_by("month")$agg(pl$col("tip_amount")$mean())
mean_fares = nyc$group_by("month")$agg(pl$col("fare_amount")$mean())

(
    mean_tips
    $join(
        mean_fares,
        on = "month",
        how = "left"
    )
    $collect()
)
```

## Experimental: `tidypolars` {.smaller}

[`tidypolars`](https://tidypolars.tidyverse.org/) provides a `dplyr`-like interface for `polars`.

```{r}
library(polars)
library(tidypolars)
library(dplyr, warn.conflicts = FALSE)
library(tidyr, warn.conflicts = FALSE)

nyc = scan_parquet_polars("nyc-taxi/**/*.parquet")

nyc |> 
    summarise(mean_tip = mean(tip_amount), .by = passenger_count) |>
    compute() # collect() -> return a data.frame
```

# BigQuery with R

## About BigQuery

BigQuery is a serverless, highly scalable, and cost-effective multi-cloud data warehouse designed for business agility.

- Accessibility: Access data from anywhere with an internet connection.
- Economy. Pay only for the storage and compute resources you use.
- Data availability. Several [sample tables](https://cloud.google.com/bigquery/public-data/#sample_tables) to get you started. And some [public datasets](https://www.reddit.com/r/bigquery/wiki/datasets/) too.

## GCP

- You need a Google Cloud Platform (GCP) account.
- Create a new project in the [GCP Console](https://console.cloud.google.com/).
- Enable the BigQuery API for your project.

## Sign up for GCP

![](img/gcp-1.jpg)

## Sign up for GCP

![](img/gcp-2.jpg)

## Sign up for GCP

::: {.columns}
::: {.column}
![](img/gcp-3.jpg)
:::
::: {.column}
```
–©–æ–± –∑–∞–≤–µ—Ä—à–∏—Ç–∏ –æ–ø–µ—Ä–∞—Ü—ñ—é –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ –∫–æ–¥: GOOGLE*MZW 123456
```
:::
:::

## Project ID

- You can find your Project ID in the GCP Console dashboard.

```{r}
library(bigrquery)

billing_id = Sys.getenv("GCP_PROJECT") # usethis::edit_r_environ()
```

## Example 1. US birth data

```{r}
library(DBI)
library(dplyr)

bq_con <- 
  dbConnect(
    bigrquery::bigquery(),
    project = "publicdata",
    dataset = "samples",
    billing = billing_id
    )

dbListTables(bq_con)
```

## Example 1. US birth data

- `natality`: Describes all United States births registered in the 50 States, the District of Columbia, and New York City from 1969 to 2008.
- ~ 22 GB

```{r}
#| output-location: column
natality <- tbl(bq_con, "natality")
head(natality)
```

## Example 1. US birth data

> How has the average birth weight changed over time?

```{r}
bw <- 
  natality |>
  filter(!is.na(state)) |> ## optional to remove some outliers
  group_by(year) |>
  summarise(weight_pounds = mean(weight_pounds, na.rm=TRUE)) |>
  collect()
```

## Example 1. US birth data

```{r}
library(ggplot2)
bw |>
  ggplot(aes(year, weight_pounds)) +
  geom_line()
```

## Example 1. US birth data

Collecting by state

```{r}
bw_st <- 
  natality |>
  filter(!is.na(state)) |>
  group_by(year, state, is_male) |>
  summarise(weight_pounds = mean(weight_pounds, na.rm=TRUE)) |>
  mutate(gender = ifelse(is_male, "Male", "Female")) |>
  collect()
```

## Example 1. US birth data {.smaller}

> Which states had the highest average birth weights in 2000 and 2008?

```{r}
#| output-location: column
states <- c("CA","DC","OR","TX","VT")
bw_st <- bw_st |> arrange(gender, year)

bw_st |>
  ggplot(aes(year, weight_pounds, group=state)) +
  geom_line(col="grey75", lwd = 0.25) +
  geom_line(
    data = bw_st |> filter(state %in% states),
    aes(col=fct_reorder2(state, year, weight_pounds)),
    lwd=0.75
    ) +
  facet_wrap(~gender) +
  scale_color_brewer(palette = "Set1", name=element_blank()) +
  labs(
    title = "Mean birth weight, by US state over time",
    subtitle = "Selected states highlighted",
    x = NULL, y = "Pounds",
    caption = "STAT150 | Data sourced from Google BigQuery"
    ) +
  theme_minimal()
```

```{r}
#| eval: false
dbDisconnect(bq_con)
```

## Example 2. Global Fishing Watch

- [Global Fishing Watch](https://globalfishingwatch.org/) (GFW) is a nonprofit organization that uses technology to promote ocean sustainability.

```{r}
#| output-location: column
gfw_con <- 
  dbConnect(
    bigrquery::bigquery(),
    project = "global-fishing-watch",
    dataset = "global_footprint_of_fisheries",
    billing = billing_id
    )

dbListTables(gfw_con)
```

## Example 2. Global Fishing Watch {.smaller}

- `fishing_effort` table contains estimates of fishing effort (in hours) for all industrial fishing vessels with AIS data from 2012 to 2016.

```{r}
#| output-location: column

effort <- tbl(gfw_con, "fishing_effort")
effort
```

## Example 2. Global Fishing Watch {.smaller}

> Which country had the highest fishing effort?

```{r}
effort |>
  group_by(flag) |>
  summarise(total_fishing_hours = sum(fishing_hours, na.rm=TRUE)) |>
  arrange(desc(total_fishing_hours)) |>
  collect()
```

## Example 2. Global Fishing Watch {.smaller}

::: {.columns}
::: {.column}
```{r}
resolution <- 1

globe <-
  effort |>
  filter(fishing_hours > 0) |>
  mutate(
    lat_bin = lat_bin/100,
    lon_bin = lon_bin/100
    ) |>
  mutate(
    lat_bin_center = floor(lat_bin/resolution)*resolution + 0.5*resolution,
    lon_bin_center = floor(lon_bin/resolution)*resolution + 0.5*resolution
    ) |>
  group_by(lat_bin_center, lon_bin_center) |>
  summarise(fishing_hours = sum(fishing_hours, na.rm=TRUE)) |>
  collect()
```
:::
::: {.column}

```{r}
#| output-location: slide
#| fig-align: center
#| fig-width: 16
#| fig-height: 9
globe |>
  filter(fishing_hours > 1) |>
  ggplot() +
  geom_tile(aes(x=lon_bin_center, y=lat_bin_center, fill=fishing_hours))+
  scale_fill_viridis_c(
    name = "Fishing hours (log scale)",
    trans = "log",
    breaks = scales::log_breaks(n = 5, base = 10),
    labels = scales::comma
    ) +
  labs(
    title = "Global fishing effort",
    subtitle = paste0("Effort binned at the ", resolution, "¬∞ level."),
    y = NULL, x = NULL,
    caption = "STAT150 | Data from Global Fishing Watch"
    ) +
  theme_minimal() +
  theme(axis.text=element_blank())
```
:::
:::


# Questions? {.unnumbered .unlisted background-iframe=".06_files/libs/colored-particles/index.html"}

<br> <br>

{{< iconify solar book-bold >}} [Course materials](https://teaching.kse.org.ua/course/view.php?id=3374)

{{< iconify mdi envelope >}} imiroshnychenko\@kse.org.ua

{{< iconify ic baseline-telegram >}} [@araprof](https://t.me/araprof)

{{< iconify mdi youtube >}} [@datamirosh](https://www.youtube.com/@datamirosh)

{{< iconify mdi linkedin >}} [\@ihormiroshnychenko](https://www.linkedin.com/in/ihormiroshnychenko/)

{{< iconify mdi github >}} [\@aranaur](https://github.com/Aranaur)

{{< iconify ion home >}} [aranaur.rbind.io](https://aranaur.rbind.io)
