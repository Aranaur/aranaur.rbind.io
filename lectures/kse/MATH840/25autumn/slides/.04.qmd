---
title: "Forecasting workflow"
subtitle: "MATH840 | Time Series"
author: "Ihor Miroshnychenko"
institute: Kyiv School of Economics
from: markdown+emoji
title-slide-attributes:
    data-background-iframe: .04_files/libs/colored-particles/index.html
footer: <a href="https://teaching.kse.org.ua/course/view.php?id=3416">üîóMATH840 | Time Series</a>
format:
  revealjs: 
    code-line-numbers: false
    navigation-mode: vertical
    transition: fade
    background-transition: fade
    chalkboard: true
    logo: img/kse.png
    slide-number: true
    toc: true
    toc-depth: 1
    mouse-wheel: true
    width: 1350  
    height: 759.375
    highlight-style: github
    fig-format: svg
    fig-align: center
    theme: [default, custom.scss]
    mermaid:
      theme: forest
preload-iframes: true
execute: 
  echo: false
  warning: false
editor_options: 
  chunk_output_type: console

revealjs-plugins:
  - verticator
---

```{python}
#| label: setup
#| include: false

# Define colors
red_pink   = "#e64173"
turquoise  = "#20B2AA"
orange     = "#FFA500"
red        = "#fb6107"
blue       = "#181485"
navy       = "#150E37FF"
green      = "#8bb174"
yellow     = "#D8BD44"
purple     = "#6A5ACD"
slate      = "#314f4f"

import warnings
warnings.filterwarnings(
    "ignore",
    category=UserWarning,
    message=".*FigureCanvasAgg is non-interactive.*"
)
import os
os.environ["NIXTLA_ID_AS_COL"] = "true"
import numpy as np
np.set_printoptions(suppress=True)
np.random.seed(1)
import random
random.seed(1)
import pandas as pd
pd.set_option("max_colwidth", 100)
pd.set_option("display.precision", 3)
from utilsforecast.plotting import plot_series as plot_series_utils
import seaborn as sns
sns.set_style("whitegrid")
import matplotlib.pyplot as plt
plt.style.use("ggplot")
plt.rcParams.update({
    "figure.figsize": (8, 5),
    "figure.dpi": 100,
    "savefig.dpi": 300,
    "figure.constrained_layout.use": True,
    "axes.titlesize": 12,
    "axes.labelsize": 10,
    "xtick.labelsize": 9,
    "ytick.labelsize": 9,
    "legend.fontsize": 9,
    "legend.title_fontsize": 10,
})
import matplotlib as mpl
from cycler import cycler
mpl.rcParams['axes.prop_cycle'] = cycler(color=[slate, blue, turquoise, orange, red, green, purple, yellow, red_pink])
from fpppy.utils import plot_series

from IPython.display import Image
from functools import partial
from statsmodels.tsa.seasonal import STL
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.stats.diagnostic import acorr_ljungbox
from utilsforecast.evaluation import evaluate
from utilsforecast.feature_engineering import pipeline, trend
from utilsforecast.losses import rmse, mae, mape as _mape, mase, quantile_loss, mqloss

def mape(df, models, id_col = "unique_id", target_col = "y"):
    df_mape = _mape(df, models, id_col=id_col, target_col=target_col)
    df_mape.loc[:, df_mape.select_dtypes(include='number').columns] *= 100
    return df_mape

from statsforecast import StatsForecast
from statsforecast.models import SklearnModel
from statsforecast.utils import ConformalIntervals
from statsforecast.models import (
    WindowAverage,
    Naive,
    SeasonalNaive,
    RandomWalkWithDrift,
    HistoricAverage,
)
from fpppy.models import LinearRegression

from great_tables import GT
```

# A tidy forecasting workflow

## A tidy forecasting workflow

The process of producing forecasts can be broken down into a series of steps:

1. Preparing data
2. Data visualization
3. Specifying a forecasting model
4. Model estimation
5. Accuracy and performance evaluation
6. Producing forecasts

## A tidy forecasting workflow

```{python}
# %%
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches

# --- Setup the Plot ---
# Create a figure and axes
fig, ax = plt.subplots(figsize=(10, 6))

# Set the axis limits to provide space for the diagram
ax.set_xlim(-4, 4)
ax.set_ylim(-2, 2)

# Hide the axes and ticks for a clean look
ax.axis('off')

# --- Define Node Positions and Styles ---
# Coordinates for each text label (node)
positions = {
    'Tidy': (-3.5, 0),
    'Visualise': (-1.5, 0),
    'Specify': (0, 1.2),
    'Estimate': (1.5, 0),
    'Evaluate': (0, -1.2),
    'Forecast': (3.5, 0),
}

# Style for the text labels
node_style = {
    'ha': 'center',
    'va': 'center',
    'fontsize': 14,
    'fontname': 'sans-serif'
}

# Style for the arrows
arrow_style = dict(
    arrowstyle='->, head_width=0.3, head_length=0.6',
    color='black',
    linewidth=1.5
)


# --- Draw Nodes (Text Labels) ---
for label, pos in positions.items():
    ax.text(pos[0], pos[1], label, **node_style)


# --- Draw Edges (Arrows) ---
# Straight arrows
# Tidy -> Visualise
tidy_to_visualise = mpatches.FancyArrowPatch(
    (positions['Tidy'][0] + 0.5, positions['Tidy'][1]),
    (positions['Visualise'][0] - 0.7, positions['Visualise'][1]),
    **arrow_style
)
ax.add_patch(tidy_to_visualise)

# Estimate -> Forecast
estimate_to_forecast = mpatches.FancyArrowPatch(
    (positions['Estimate'][0] + 0.6, positions['Estimate'][1]),
    (positions['Forecast'][0] - 0.7, positions['Forecast'][1]),
    **arrow_style
)
ax.add_patch(estimate_to_forecast)


# Curved arrows for the loop
# We use connectionstyle with a radius value (rad) to create the curve
# Visualise -> Specify
vis_to_spec = mpatches.FancyArrowPatch(
    (-1.2, 0.2), # Start point offset from Visualise
    (-0.4, 1.1), # End point offset from Specify
    connectionstyle="arc3,rad=-0.5",
    **arrow_style
)
ax.add_patch(vis_to_spec)

# Specify -> Estimate
spec_to_est = mpatches.FancyArrowPatch(
    (0.4, 1.1),  # Start point
    (1.2, 0.2),  # End point
    connectionstyle="arc3,rad=-0.5",
    **arrow_style
)
ax.add_patch(spec_to_est)

# Estimate -> Evaluate
est_to_eval = mpatches.FancyArrowPatch(
    (1.2, -0.2), # Start point
    (0.4, -1.1), # End point
    connectionstyle="arc3,rad=-0.5", # Negative radius to curve downwards
    **arrow_style
)
ax.add_patch(est_to_eval)

# Evaluate -> Visualise
eval_to_vis = mpatches.FancyArrowPatch(
    (-0.4, -1.1),# Start point
    (-1.2, -0.2),# End point
    connectionstyle="arc3,rad=-0.5", # Negative radius
    **arrow_style
)
ax.add_patch(eval_to_vis)


# --- Display the Plot ---
# Ensure the aspect ratio is equal to avoid distortion
ax.set_aspect('equal', adjustable='box')
plt.title("Data Analysis Workflow", fontsize=16)
plt.show()
```

## Data preparation {.smaller}

```{python}
#| label: load-data
#| echo: true

gsheetkey = '1wrSjwahn_OCY6JcS6Bumxu9ucuzGX1eN2yeQTIVhF3c'
url = f'https://docs.google.com/spreadsheet/ccc?key={gsheetkey}&output=csv'

gdp_df = pd.read_csv(url, parse_dates=["ds"])
gdp_df[["GDP", "Population"]] = gdp_df[["GDP", "Population"]].interpolate()
gdp_df["y"] = gdp_df["GDP"] / gdp_df["Population"]
gdp_df = gdp_df.drop(["Code", "Growth", "CPI", "Imports", "Exports"],
  axis=1)
gdp_df.head()
```

## Data visualization

```{python}
#| echo: true
plot_series(gdp_df, ids=["Sweden"],
            xlabel="Year [1Y]", ylabel="$US",
            title="GDP per capita for Sweden")
```

## Train the model

```{python}
#| echo: true
train_df = gdp_df.copy()
train_df.drop(["GDP", "Population"], axis=1, inplace=True)
sweden_df = train_df.query("unique_id == 'Sweden'")

train_features, valid_features = pipeline(
    sweden_df,
    features=[trend],
    freq="YE",
    h=3,
)
sf = StatsForecast(
    models=[SklearnModel(LinearRegression())],
    freq="YE",
)
```

## Produce forecasts

```{python}
#| echo: true
sf.fit(df=train_features)
fcasts = sf.predict(
    h=3,
    X_df=valid_features,
)
fcasts = sf.fitted_[0][0].model_["model"].add_prediction_intervals(fcasts,
  valid_features.rename(columns={"trend": "x1"}))
```

---

```{python}
plot_series(train_df, fcasts,
            level=[80, 95],
            ids=["Sweden"],
            xlabel="Year",
            ylabel="$US",
            title="GDP per capita for Sweden",
            rm_legend=False
            )
```

# Some simple forecasting methods

## Data {.smaller}

```{python}
#| echo: true
gsheetkey = '10Y3HqO91-87jSvxmmxuTlAlbKyD67pZ9hWP8RIp7JSE'
url = f'https://docs.google.com/spreadsheet/ccc?key={gsheetkey}&output=csv'

production_df = pd.read_csv(url,
  parse_dates=["ds"])
production_df.head()
```

## Mean method {.smaller}

- Forecast of all future values is the mean of the historical data
- Forecast: $\hat{y}_{t+h|t} = \bar{y}_t = \frac{1}{T} \sum_{i=1}^{T} y_i$

```{python}
#| fig-align: center
#| echo: true
#| output-location: column
df = production_df[production_df['unique_id'] == 'Bricks'].dropna()
sf = StatsForecast(models=[HistoricAverage()], freq='QE')
fcasts_avg = sf.forecast(h=20, df=df)
plot_series(df, fcasts_avg,
            xlabel="Quarter",
            ylabel="Production",
            title="Mean method forecasts for Bricks production")
```

## Na√Øve method {.smaller}

- Forecast of all future values is the last observed value
- Forecast: $\hat{y}_{t+h|t} = y_t$

```{python}
#| fig-align: center
#| echo: true
#| output-location: column
sf = StatsForecast(models=[Naive()], freq='QE')
fcasts_naive = sf.forecast(h=20, df=df)
plot_series(df, fcasts_naive,
            xlabel="Quarter",
            ylabel="Production",
            title="Na√Øve method forecasts for Bricks production")
```

## Seasonal na√Øve method {.smaller}

- Forecast of all future values is the last observed value from the same season
- Forecast: $\hat{y}_{t+h|t} = y_{t+h-m(k+1)}$, where $m$ is the seasonal period and $k = (h-1)/m$

```{python}
#| fig-align: center
#| echo: true
#| output-location: column
sf = StatsForecast(models=[SeasonalNaive(season_length=4)], freq='QE')
fcasts_snaive = sf.forecast(h=20, df=df)
plot_series(df, fcasts_snaive,
            xlabel="Quarter",
            ylabel="Production",
            title="Seasonal na√Øve method forecasts for Bricks")
```

## Drift method {.smaller}

- Forecast equals last observed value plus average change observed in historical data
- Forecast: $\hat{y}_{T+h \mid T}=y_T+\frac{h}{T-1} \sum_{t=2}^T\left(y_t-y_{t-1}\right)=y_T+h\left(\frac{y_T-y_1}{T-1}\right)$.
- Equivalent to fitting a line between the first and last observation and extrapolating it into the future

```{python}
#| fig-align: center
#| echo: true
#| output-location: column
sf = StatsForecast(models=[RandomWalkWithDrift()], freq='QE')
fcasts_drift = sf.forecast(h=20, df=df)
plot_series(df, fcasts_drift,
            xlabel="Quarter",
            ylabel="Production",
            title="Drift method forecasts for Bricks production")
```

## Example: Australian beer production

```{python}
beers_df = production_df.query("unique_id == 'Beer' and ds >= '1992-01-01'")
train = beers_df[:-14]
test = beers_df[-14:]

avg_method = HistoricAverage()
naive_method = Naive()
seasonal_naive_method = SeasonalNaive(4)

sf = StatsForecast(
    models=[avg_method, naive_method, seasonal_naive_method],
    freq=pd.offsets.QuarterBegin(1)
)
sf.fit(train)

fcasts = sf.predict(h=14)
fcasts["y"] = test["y"].values

plot_series(train, fcasts,
            xlabel="Quarter",
            ylabel="Megalitres",
            title="Forecasts for quarterly beer production",
            rm_legend=False)
```

## Example: Google‚Äôs daily closing stock price

```{python}
gsheetkey = '1t5ZT0wHdunWVflzwp_vL-qRjRBlAleH4WJ23mL44cWA'
url = f'https://docs.google.com/spreadsheet/ccc?key={gsheetkey}&output=csv'

gafa_df = pd.read_csv(url, parse_dates=["ds"])
goog_df = gafa_df.query(
    "unique_id == 'GOOG_Close' and '2015-01-01' <= ds <= '2016-01-31'"
)

train = goog_df.query("ds.dt.year == 2015")
test = goog_df.query("ds.dt.year == 2016")

train["ds"] = np.arange(len(train))
test["ds"] = np.arange(len(test))

avg_method = HistoricAverage()
naive_method = Naive()
drift_method = RandomWalkWithDrift()
sf = StatsForecast(models=[drift_method, avg_method, naive_method], freq=1)
sf.fit(train)

fcasts = sf.predict(h=len(test))
fcasts["y"] = test["y"].values

plot_series(train, fcasts,
            xlabel="day",
            ylabel="$US",
            title="Google daily closing stock prices (Jan 2015-Jan 2016)",
            rm_legend=False)
```

# Fitted values and residuals

## Fitted values {.smaller}

- $\hat{y}_{t|t-1}$ is the forecast of $y_t$ based on all available information up to time $t-1$.
- We call this the **fitted value**
- Sometimes drop the subscript and simply refer to it as $\hat{y}_t$.
- Often not true forecast since parameters are estimated on all data.

::: {.callout-tip icon="false"}
## For example

- $\hat{y}_{t}=\bar{y}$ for average method
- $\hat{y}_{t|t-1}=y_{t-1}+(y_T-y_1)/(T-1)$ for drift method
:::

## Forecasting residuals {.smaller}

- The residuals are the differences between the observed values and the fitted values: $e_t = y_t - \hat{y}_{t|t-1}$.

Assumptions:

1. $\{e_t\}$ [uncorrelated]{.hi}. If they aren't, then information left in residuals can be used to improve forecasts.
2. $\{e_t\}$ [have mean zero]{.hi}. If they don't, then the model is systematically over- or under-predicting.

Useful properties (for distribution and prediction intervals):

3. $\{e_t\}$ are [normally distributed]{.hi-slate}. If they aren't, then prediction intervals may be invalid.
4. $\{e_t\}$ are [homoscedastic]{.hi-slate}. If they aren't, then prediction intervals may be invalid.

# Residual diagnostics

## Google daily closing stock prices {.smaller}

```{python}
#| echo: true
#| fig-align: center
train = goog_df.query("ds.dt.year == 2015")
plot_series(train,
            xlabel="day [1]",
            ylabel="$US",
            title="Google daily closing stock prices in 2015")
```

## Google stock prices and Naive method {.smaller}

```{python}
#| echo: true
naive_method = Naive()
sf = StatsForecast(models=[naive_method], freq='1d')
sf.forecast(h=14, df=train, fitted=True)
fitted_values = sf.forecast_fitted_values()
train["fitted"] = fitted_values["Naive"].values
train["resid"] = train["y"] - train["fitted"]
train["innov"] = train["y"] - train["fitted"]
train.head()
```

## True vs Fitted values {.smaller}

```{python}
#| echo: true
#| fig-align: center
fig, ax = plt.subplots()
ax.plot(train["ds"], train["y"], label="Observed", color=blue)
ax.plot(train["ds"], train["fitted"], label="Fitted", color=orange)
ax.legend()
```

## Last observations {.smaller}

```{python}
#| echo: true
#| fig-align: center
fig, ax = plt.subplots()
ax.plot(train["ds"][-40:], train["y"][-40:], label="Observed", color=blue)
ax.plot(train["ds"][-40:], train["fitted"][-40:], label="Fitted", color=orange)
ax.legend()
```


## Residual diagnostics {.smaller}

```{python}
#| echo: true
#| fig-align: center
train["resid"] = train["y"].diff().values
plot_series(train, target_col="resid",
            xlabel="day [1]",
            ylabel="$US",
            title="Residuals from the naive method")
```

## Histogram of residuals {.smaller}

```{python}
#| echo: true
#| fig-align: center
ax = train["resid"].hist()
ax.set_title("Histogram of residuals")
plt.show()
```

## ACF of residuals {.smaller}

```{python}
#| echo: true
#| fig-align: center
fig = plot_acf(train["resid"][1:], zero=False, auto_ylims=True,
         bartlett_confint=False, title="Residuals from the na√Øve method")
```

## All together {.smaller}

```{python}
#| echo: true
#| output-location: slide
#| fig-align: center
def plot_diagnostics(data):
  fig = plt.figure(figsize=(8, 5))
  ax1 = fig.add_subplot(2, 2, (1, 2))
  ax1.plot(data['ds'], data["resid"])
  ax1.set_title("Innovation Residuals")
  ax2 = fig.add_subplot(2, 2, 3)
  plot_acf(data["resid"].dropna(), ax=ax2, zero=False,
    bartlett_confint=False, auto_ylims=True)
  ax2.set_title("ACF Plot")
  ax2.set_xlabel('lag[1]')
  ax3 = fig.add_subplot(2, 2, 4)
  ax3.hist(data["resid"], bins=20)
  ax3.set_title("Histogram")
  ax3.set_xlabel(".resid")
  ax3.set_ylabel("Count")
  plt.tight_layout()
  plt.show()

plot_diagnostics(train)
```

## ACF of residuals

- We assume that the [residuals are white noise]{.hi}. If they are not, then there is information left in the residuals that can be used to improve the forecasts.
- So a standard residual diagnostic is to [plot the ACF]{.hi} of the residuals and check for significant autocorrelations.
- We [*expect*]{.hi} all autocorrelations to be close to zero and within the 95% confidence bounds.

## Portmanteau test {.smaller}

$r_k$ --- autocorrelation of residual at lag $k$.

Consider a whole set of $r_k$ values, and develop a test to see whether they are significantly different from zero.

- $H_0$: variable follows a white noise process (all $r_k = 0$ for $k = 1, 2, \ldots, l$)
- $H_a$: variable does not follow a white noise process (at least one $r_k \neq 0$ for $k = 1, 2, \ldots, l$)

::: {.callout-note icon="false"}
## Box-Pierce test

$$Q = T \sum_{k=1}^{l} r_k^2$$

where $T$ is the number of residuals and $l$ is the number of lags being tested.
:::

- If each $r_k$ close to zero, then $Q$ will be small.
- If some $r_k$ significantly different from zero, then $Q$ will be large.

## Portmanteau test {.smaller}

$r_k$ --- autocorrelation of residual at lag $k$.

Consider a whole set of $r_k$ values, and develop a test to see whether they are significantly different from zero.

::: {.callout-note icon="false"}
## Ljung-Box test

$$Q^* = T(T+2) \sum_{k=1}^{l} \frac{r_k^2}{T-k}$$

where $T$ is the number of residuals and $l$ is the number of lags being tested.
:::

- Preferences: $l = 10$ for non-seasonal data, $l = 2 \times m$ for seasonal data with period $m$.
- Better performance for small samples compared to Box-Pierce test.

## Portmanteau test for naive method residuals {.smaller}

- If data are white noise, then $Q^*$ has a $\chi^2$ distribution with $l$ degrees of freedom.
- lag = $l$

```{python}
#| echo: true
#| output-location: column
resid_test = acorr_ljungbox(train["resid"].dropna(),
  boxpierce=True)
resid_test.round(4).head()
```

## Portmanteau test for RW {.smaller}

```{python}
#| echo: true
#| output-location: column
train = goog_df.query("ds.dt.year == 2015")
drift_method = RandomWalkWithDrift()
sf = StatsForecast(models=[drift_method], freq="B")
sf.fit(train)
fcasts = sf.forecast(df=train, h=10, fitted=True)
insample_preds = sf.forecast_fitted_values()
insample_preds["resid"] = insample_preds["y"] - insample_preds["RWD"]
resid_test = acorr_ljungbox(insample_preds["resid"].dropna(),
  boxpierce=True)
resid_test.round(4).head()
```

# Forecasting using transformations

## Backtransformation {.smaller}

We must reverse the transformation to obtain forecasts on the original scale.

For Box-Cox transformation with parameter $\lambda$:

<br>

::: {.columns}
::: {.column}
Box-Cox transformation:

$$
w_t^{(\lambda)} =
\begin{cases}
\frac{\text{sign}(y_t)|y_t|^{(\lambda)} - 1}{\lambda} & \text{if } \lambda \neq 0 \\
\log(y_t) & \text{if } \lambda = 0
\end{cases}
$$
:::
::: {.column}
Box-Cox backtransformation:

$$
y_t =
\begin{cases}
\text{sign}(\lambda w_t^{(\lambda)} + 1) |\lambda w_t^{(\lambda)} + 1|^{1/\lambda} & \text{if } \lambda \neq 0 \\
\exp(w_t^{(\lambda)}) & \text{if } \lambda = 0
\end{cases}
$$

:::
:::

---

```{python}
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
from matplotlib.patches import ConnectionPatch

# 1. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ñ—ñ–≥—É—Ä–∏ —Ç–∞ –¥–≤–æ—Ö –ø—ñ–¥–≥—Ä–∞—Ñ—ñ–∫—ñ–≤
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))

# --- –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ –±—ñ–ª–∏–π —Ñ–æ–Ω –¥–ª—è —Ñ—ñ–≥—É—Ä–∏ —Ç–∞ –æ—Å–µ–π ---
fig.patch.set_facecolor('white')
ax1.set_facecolor('white')
ax2.set_facecolor('white')
# ---------------------------------------------------------

fig.suptitle(
  "Transformations to reduce skewness",
  fontsize=18, y=0.98
)

# --- –ì—Ä–∞—Ñ—ñ–∫ 1: –ê—Å–∏–º–µ—Ç—Ä–∏—á–Ω–∏–π (–≤–∏—Ö—ñ–¥–Ω–∏–π) —Ä–æ–∑–ø–æ–¥—ñ–ª ---

# –ì–µ–Ω–µ—Ä—É—î–º–æ –¥–∞–Ω—ñ –¥–ª—è –ø—Ä–∞–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω—å–æ–≥–æ –∞—Å–∏–º–µ—Ç—Ä–∏—á–Ω–æ–≥–æ —Ä–æ–∑–ø–æ–¥—ñ–ª—É
a = 4  # –ü–∞—Ä–∞–º–µ—Ç—Ä –∞—Å–∏–º–µ—Ç—Ä—ñ—ó
x_skewed = np.linspace(stats.skewnorm.ppf(0.001, a), stats.skewnorm.ppf(0.999, a), 200)
pdf_skewed = stats.skewnorm.pdf(x_skewed, a)

# –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –º–æ–¥—É, –º–µ–¥—ñ–∞–Ω—É —Ç–∞ —Å–µ—Ä–µ–¥–Ω—î
mean_val = stats.skewnorm.mean(a)
median_val = stats.skewnorm.median(a)
mode_val = x_skewed[np.argmax(pdf_skewed)] # –ú–æ–¥–∞ - —Ü–µ –ø—ñ–∫ —Ä–æ–∑–ø–æ–¥—ñ–ª—É

# –ú–∞–ª—é—î–º–æ –∫—Ä–∏–≤—É —Ä–æ–∑–ø–æ–¥—ñ–ª—É
ax1.plot(x_skewed, pdf_skewed, 'k', linewidth=1.5)

# –ú–∞–ª—é—î–º–æ –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ñ –ª—ñ–Ω—ñ—ó –¥–ª—è –º–æ–¥–∏, –º–µ–¥—ñ–∞–Ω–∏ —Ç–∞ —Å–µ—Ä–µ–¥–Ω—å–æ–≥–æ
ax1.vlines([mode_val, median_val, mean_val], 0,
       [stats.skewnorm.pdf(mode_val, a),
      stats.skewnorm.pdf(median_val, a),
      stats.skewnorm.pdf(mean_val, a)],
       colors='k', lw=2)

# –î–æ–¥–∞—î–º–æ —Ç–µ–∫—Å—Ç–æ–≤—ñ –ø—ñ–¥–ø–∏—Å–∏
ax1.text(mode_val, stats.skewnorm.pdf(mode_val, a) + 0.05, 'mode', ha='center', fontsize=14)
ax1.text(median_val, stats.skewnorm.pdf(median_val, a) + 0.08, 'median', ha='center', fontsize=14)
ax1.text(mean_val + 0.1, stats.skewnorm.pdf(mean_val, a) + 0.04, 'mean', ha='left', fontsize=14)

# --- –ì—Ä–∞—Ñ—ñ–∫ 2: –°–∏–º–µ—Ç—Ä–∏—á–Ω–∏–π (—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–æ–≤–∞–Ω–∏–π) —Ä–æ–∑–ø–æ–¥—ñ–ª ---

# –ì–µ–Ω–µ—Ä—É—î–º–æ –¥–∞–Ω—ñ –¥–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–æ–∑–ø–æ–¥—ñ–ª—É
x_norm = np.linspace(-3, 3, 200)
pdf_norm = stats.norm.pdf(x_norm)

# –ú–∞–ª—é—î–º–æ –∫—Ä–∏–≤—É
ax2.plot(x_norm, pdf_norm, 'k', linewidth=1.5)

# –ú–∞–ª—é—î–º–æ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—É –ª—ñ–Ω—ñ—é (—Å–µ—Ä–µ–¥–Ω—î = –º–µ–¥—ñ–∞–Ω–∞ = –º–æ–¥–∞)
ax2.vlines(0, 0, stats.norm.pdf(0), 'k', lw=2)
ax2.text(0, stats.norm.pdf(0) + 0.05, 'symmetric', ha='center', fontsize=16)

# --- –°—Ç—Ä—ñ–ª–∫–∏ —Ç–∞ –∞–Ω–æ—Ç–∞—Ü—ñ—ó ---

# 1. –ì–æ–ª–æ–≤–Ω–∞ —á–æ—Ä–Ω–∞ —Å—Ç—Ä—ñ–ª–∫–∞: –∑'—î–¥–Ω—É—î —Å–µ—Ä–µ–¥–Ω—î —Å–∏–º–µ—Ç—Ä–∏—á–Ω–æ–≥–æ —Ä–æ–∑–ø–æ–¥—ñ–ª—É –∑ –º–µ–¥—ñ–∞–Ω–æ—é –∞—Å–∏–º–µ—Ç—Ä–∏—á–Ω–æ–≥–æ
con = ConnectionPatch(
  xyA=(0, 0), xyB=(median_val, 0),
  coordsA='data', coordsB='data',
  axesA=ax2, axesB=ax1,
  arrowstyle="->", shrinkB=5,
  connectionstyle="arc3,rad=-0.4", # –í–∏–≥–∏–Ω —Å—Ç—Ä—ñ–ª–∫–∏ –≤–Ω–∏–∑
  color='k', lw=2
)
ax2.add_artist(con)

# 2. –ó–µ–ª–µ–Ω–∞ –∞–Ω–æ—Ç–∞—Ü—ñ—è –∑ –ø–æ—è—Å–Ω–µ–Ω–Ω—è–º
ax1.annotate(
  'adjust to get to\nthe mean',
  xy=(mean_val, 0.2), xycoords='data',
  xytext=(mean_val + 0.3, 0.1), textcoords='data',
  ha='left', va='top', fontsize=14, color='green',
  arrowprops=dict(
    arrowstyle='->',
    connectionstyle="arc3,rad=0.3",
    color='green',
    lw=2
  )
)

# --- –§—ñ–Ω–∞–ª—å–Ω–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –≤–∏–≥–ª—è–¥—É –≥—Ä–∞—Ñ—ñ–∫—ñ–≤ ---
for ax in [ax1, ax2]:
  ax.set_yticks([])
  ax.set_xticks([])
  # –ú–∞–ª—é—î–º–æ –æ—Å—ñ X —Ç–∞ Y
  ax.axhline(0, color='k', lw=1)
  ax.axvline(ax.get_xlim()[0], color='k', lw=1)
  # –ü—Ä–∏–±–∏—Ä–∞—î–º–æ –∑–∞–π–≤—ñ —Ä–∞–º–∫–∏
  for spine in ['right', 'top', 'left', 'bottom']:
    ax.spines[spine].set_visible(False)
  ax.set_ylim(bottom=-0.3) # –î–æ–¥–∞—î–º–æ –º—ñ—Å—Ü–µ –∑–Ω–∏–∑—É –¥–ª—è –∞–Ω–æ—Ç–∞—Ü—ñ–π

plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()
```

## Modeling with transformations {.smaller}

```{python}
#| echo: true
#| output-location: column
#| fig-align: center
gsheetkey = '1V2HAc5bwBXq1KJg9Dv7dX0R0C3s1mcGxfWmEVwoI1bs'
url=f'https://docs.google.com/spreadsheet/ccc?key={gsheetkey}&output=csv'

egg_df = pd.read_csv(url, parse_dates=["ds"])
egg_df_log = egg_df.copy()
egg_df_log['y'] = np.log(egg_df_log['y'])

# plot original and log-transformed data
fig, ax = plt.subplots(2, 1, figsize=(10, 8))
ax[0].plot(egg_df['ds'], egg_df['y'], color=blue)
ax[0].set_title('Original Egg Production Data')
ax[0].set_xlabel('Date')
ax[0].set_ylabel('Egg Production')
ax[1].plot(egg_df_log['ds'], egg_df_log['y'], color=orange)
ax[1].set_title('Log-Transformed Egg Production Data')
ax[1].set_xlabel('Date')
ax[1].set_ylabel('Log(Egg Production)')
plt.tight_layout()
plt.show()
```

---

```{python}
#| echo: true
#| output-location: slide
#| fig-align: center
rwd_method = RandomWalkWithDrift()
sf = StatsForecast(models=[rwd_method], freq="YE")
sf.fit(egg_df_log)
fcasts = sf.forecast(df=egg_df_log, h=50, level=[80, 95])
sigma_h = (fcasts['RWD-hi-80'] - fcasts['RWD-lo-80']) / (2 * 1.28)
sigma_h_squared = sigma_h**2
bias_adjusted = np.exp(fcasts['RWD']) * (1 + sigma_h_squared/2)
fcasts_original = fcasts.copy()
columns_to_transform = \
  ['RWD', 'RWD-lo-80', 'RWD-lo-95', 'RWD-hi-80', 'RWD-hi-95']
for col in columns_to_transform:
    fcasts_original[col] = np.exp(fcasts[col])
fcasts_original['RWD-adjusted'] = bias_adjusted

fig, ax = plt.subplots()
ax.plot(egg_df['ds'], egg_df['y'], color='black', label='Historical',
  linewidth=1)
ax.fill_between(fcasts_original['ds'],
                fcasts_original['RWD-lo-80'],
                fcasts_original['RWD-hi-80'],
                alpha=0.3, color='#B8BFFF', label='80% CI')
ax.plot(fcasts_original['ds'], fcasts_original['RWD'],
        color='blue', linestyle='--', label='Median forecast', linewidth=1)
ax.plot(fcasts_original['ds'], fcasts_original['RWD-adjusted'],
        color='blue', linestyle='-', label='Bias-adjusted mean', linewidth=1)
ax.set_title('Annual egg prices')
ax.set_xlabel('year')
ax.set_ylabel('$US (in cents adjusted for inflation)')
handles, labels = fig.axes[0].get_legend_handles_labels()
fig.legend(
  handles,
  labels,
  loc="center left",
  bbox_to_anchor=(1.02, 0.5),
  borderaxespad=0.0,
  frameon=False,
)
plt.show()
```

## Bias adjustment {.smaller}

- Back transformed point forecassts are [median]{.hi}.
- Back transformed **PI** have the **correct** coverage.

Back-transformed means:

1. Let $X$ have mean $\mu$ and variance $\sigma^2$.
2. Let $f(x)$ be back-transformation function, and $Y = f(X)$.
3. Taylor series expansion about $\mu$:
$$
Y = f(x) \approx f(\mu) + (X - \mu) f'(\mu) + \frac{f''(\mu)(X - \mu)^2}{2} 
$$

4. Taking expectations:
$$
E[Y] \approx f(\mu) + \frac{f''(\mu) \sigma^2}{2}
$$

## Bias adjustment

Box-Cox back-transformation:

$\begin{aligned} y_t & = \begin{cases}\exp \left(w_t\right) & \lambda=0 ; \\ \left(\lambda w_t+1\right)^{1 / \lambda} & \lambda \neq 0 .\end{cases} \\ f(x) & = \begin{cases}e^x & \lambda=0 ; \\ (\lambda x+1)^{1 / \lambda} & \lambda \neq 0 .\end{cases} \\ f^{\prime \prime}(x) & = \begin{cases}e^x & \lambda=0 ; \\ (1-\lambda)(\lambda x+1)^{1 / \lambda-2} & \lambda \neq 0 .\end{cases} \end{aligned}$

$E[Y] \approx \begin{cases}e^\mu\left[1+\frac{\sigma^2}{2}\right] & \lambda=0 ; \\ (\lambda \mu+1)^{1 / \lambda}\left[1+\frac{\sigma^2(1-\lambda)}{2(\lambda \mu+1)^2}\right] & \lambda \neq 0 .\end{cases}$

# Forecasting with decomposition

## Forecasting and decomposition

$$
y_t = \hat{S_t} + \hat{A_t}
$$

- $\hat{S_t}$ is the seasonal component
- $\hat{A_t}$ is seasonally adjusted data (trend + remainder)

. . .

- Forecast $\hat{S_t}$ using [seasonal na√Øve method]{.hi}
- Forecast $\hat{A_t}$ using [appropriate method]{.hi} (e.g., drift method)
- **Combine** forecasts to obtain final forecasts

## US Retail Employment {.smaller}

```{python}
#| echo: true
#| output-location: column
#| fig-align: center
gsheetkey = '1VoHDwazrwCVOyooH7AyBqMAJZmq5sD7jRgzzPl7AyZA'
url = f'https://docs.google.com/spreadsheet/ccc?key={gsheetkey}&output=csv'

us_employment_df = pd.read_csv(url,
  parse_dates=["ds"])
us_retail_employment_df = us_employment_df.query(
    "Title == 'Retail Trade' and ds.dt.year >= 1992"
)

train_df = us_retail_employment_df.drop(["Title"], axis=1)

stl = STL(train_df["y"].values, period=12, robust=True, trend_deg=7)
res = stl.fit()

train_df["seasonal"] = res.seasonal
train_df["y_adjusted"] = train_df["y"] - train_df["seasonal"]

# Plot original and seasonally adjusted data
fig, ax = plt.subplots(2, 1, figsize=(10, 8))
ax[0].plot(train_df['ds'], train_df['y'], color='blue')
ax[0].set_title('Original US Retail Employment Data')
ax[1].plot(train_df['ds'], train_df['y_adjusted'], color='orange')
ax[1].set_title('Seasonally Adjusted US Retail Employment Data')
plt.tight_layout()
plt.show()
```

## Seasonally Adjusted with naive method

```{python}
#| echo: true
#| output-location: slide
#| fig-align: center
naive_method = Naive()
sf = StatsForecast(models=[naive_method], freq="M")
adjusted_fcasts = sf.forecast(h=24, level=[80, 95], df=train_df,
                              target_col="y_adjusted", fitted=True)

adjusted_fcasts_fitted = sf.forecast_fitted_values()
plot_series(train_df, adjusted_fcasts, target_col="y_adjusted",
            level=[80, 95],
            xlabel="Month",
            ylabel="Number of people",
            title="US retail employment",
            rm_legend=False)
```

## Seasonal component with seasonal naive

```{python}
#| echo: true
#| output-location: slide
#| fig-align: center
seasonal_naive_method = SeasonalNaive(season_length=12)
sf = StatsForecast(models=[seasonal_naive_method], freq="M")
seasonal_fcasts = sf.forecast(h=24, level=[80, 95], df=train_df,
                               target_col="seasonal", fitted=True)

seasonal_fcasts_fitted = sf.forecast_fitted_values()
plot_series(train_df, seasonal_fcasts, target_col="seasonal",
            level=[80, 95],
            xlabel="Month",
            ylabel="Number of people",
            title="US retail employment",
            rm_legend=False)
```

## Final forecasts

```{python}
#| echo: true
#| output-location: slide
#| fig-align: center
seasonal_naive = SeasonalNaive(12)
sf = StatsForecast(models=[seasonal_naive], freq="M")
seasonal_fcasts = sf.forecast(h=24, level=[80, 95], df=train_df,
  target_col="seasonal", fitted=True)
merged_df = adjusted_fcasts.merge(seasonal_fcasts, on=['unique_id', 'ds'],
  how='inner')
merged_df_fitted = adjusted_fcasts_fitted.merge(sf.forecast_fitted_values(),
  on=['unique_id', 'ds'], how='inner')
merged_df_fitted["combined"] = merged_df_fitted['Naive'] + \
  merged_df_fitted['SeasonalNaive']
merged_df_fitted = merged_df_fitted.merge(train_df, on=["unique_id", "ds"],
  how="right")
final_df = merged_df[['unique_id', 'ds']].copy() # Start with identifiers
final_df['combined'] = merged_df['Naive'] + merged_df['SeasonalNaive']
train_df["resid"] = merged_df_fitted["y"].values - \
  merged_df_fitted["Naive"].values - \
  merged_df_fitted["SeasonalNaive"].values
final_df['combined-lo-80'] = merged_df['Naive-lo-80'] + \
  merged_df['SeasonalNaive-lo-80']
final_df['combined-lo-95'] = merged_df['Naive-lo-95'] + \
  merged_df['SeasonalNaive-lo-95']
final_df['combined-hi-80'] = merged_df['Naive-hi-80'] + \
  merged_df['SeasonalNaive-hi-80']
final_df['combined-hi-95'] = merged_df['Naive-hi-95'] + \
  merged_df['SeasonalNaive-hi-95']

plot_series(train_df, final_df, target_col="y", level=[80, 95],
            xlabel="Month",
            ylabel="Number of people",
            title="US retail employment",
            rm_legend=False)
```

## Residual diagnostics {.smaller}

```{python}
#| echo: true
#| fig-align: center
plot_diagnostics(train_df)
```

# Evaluating point forecast accuracy

## Training and test sets {.smaller}

```{python}
#| fig-align: center

# --- –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫—É ---
fig, ax = plt.subplots(figsize=(12, 2))

# –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ –º–µ–∂—ñ, —â–æ–± –±—É–ª–æ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –º—ñ—Å—Ü—è
ax.set_ylim(-1, 1)
ax.set_xlim(-1, 26)

# –ü—Ä–∏–±–∏—Ä–∞—î–º–æ –æ—Å—ñ —Ç–∞ —Ä–∞–º–∫—É –¥–ª—è —á–∏—Å—Ç–æ–≥–æ –≤–∏–≥–ª—è–¥—É
ax.axis('off')

# --- –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö —Ç–∞ –∫–æ–ª—å–æ—Ä—ñ–≤ ---
# –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ—á–æ–∫ –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö —Ç–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
n_train = 18
n_test = 5

# –°—Ç–≤–æ—Ä—é—î–º–æ –º–∞—Å–∏–≤–∏ –∑ –ø–æ–∑–∏—Ü—ñ—è–º–∏ —Ç–æ—á–æ–∫ –Ω–∞ –æ—Å—ñ X
train_points = np.arange(0, n_train)
test_points = np.arange(n_train, n_train + n_test)

# –ö–æ–ª—å–æ—Ä–∏, —â–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—é—Ç—å –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—é
train_color = '#377eb8'  # –°–∏–Ω—ñ–π
test_color = '#d95f02'   # –ü–æ–º–∞—Ä–∞–Ω—á–µ–≤–∏–π
axis_color = 'black'

# --- –ú–∞–ª—é—î–º–æ –µ–ª–µ–º–µ–Ω—Ç–∏ –≥—Ä–∞—Ñ—ñ–∫—É ---

# 1. –ì–æ–ª–æ–≤–Ω–∞ –≤—ñ—Å—å —á–∞—Å—É –∑—ñ —Å—Ç—Ä—ñ–ª–∫–æ—é
# –ú–∞–ª—é—î–º–æ –ª—ñ–Ω—ñ—é, –∞ –ø–æ—Ç—ñ–º –¥–æ–¥–∞—î–º–æ —Å—Ç—Ä—ñ–ª–∫—É –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é annotate
ax.axhline(y=0, color=axis_color, zorder=1)
# –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ annotate –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å—Ç—Ä—ñ–ª–∫–∏ –Ω–∞ –∫—ñ–Ω—Ü—ñ
ax.annotate('', xy=(n_train + n_test + 1, 0), xytext=(-1, 0),
            arrowprops=dict(arrowstyle="->", color=axis_color))


# 2. –ú–∞–ª—é—î–º–æ —Ç–æ—á–∫–∏ –¥–∞–Ω–∏—Ö
# –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ
ax.scatter(train_points, np.zeros_like(train_points),
           color=train_color, s=100, zorder=2)

# –¢–µ—Å—Ç–æ–≤—ñ –¥–∞–Ω—ñ
ax.scatter(test_points, np.zeros_like(test_points),
           color=test_color, s=100, zorder=2)


# 3. –î–æ–¥–∞—î–º–æ —Ç–µ–∫—Å—Ç–æ–≤—ñ –ø—ñ–¥–ø–∏—Å–∏
# –ü—ñ–¥–ø–∏—Å –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
train_text_pos = np.mean(train_points)
ax.text(train_text_pos, 0.4, 'Training data',
        ha='center', va='center', fontsize=20, color=train_color)

# –ü—ñ–¥–ø–∏—Å –¥–ª—è —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
test_text_pos = np.mean(test_points)
ax.text(test_text_pos, 0.4, 'Test data',
        ha='center', va='center', fontsize=20, color=test_color)

# –ü—ñ–¥–ø–∏—Å "time"
ax.text(n_train + n_test + 1.5, 0.1, 'time',
        ha='left', va='center', fontsize=18, color=axis_color)

# --- –í—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É ---
plt.show()
```

- A model which fits the training data well will not necessarily forecast well.
- A perfect fit can always be obtained by using a model with enough parameters.
- Over-fitting a model to data is just as bad as failing to identify a systematic pattern in the data.

## Forecast errors

$$
e_{T+h} = y_{T+h} - \hat{y}_{T+h|T}
$$

- Unlike residuals, forecast errors on the test set involve multi-step forecasts.
- These are true forecast errors as the test data is not used in model fitting.

---

```{python}
#| echo: true
#| output-location: slide
#| fig-align: center
train_df = beers_df.query("ds.dt.year <= 2007")
test_df = beers_df.query("ds.dt.year > 2007")

mean_method = HistoricAverage()
naive_method = Naive()
drift_method = RandomWalkWithDrift()
seasonal_naive = SeasonalNaive(4)

sf = StatsForecast(
    models=[drift_method, mean_method, naive_method, seasonal_naive],
    freq="Q"
)
preds = sf.forecast(h=len(test_df), df=train_df)
preds["y"] = test_df["y"].values

plot_series(beers_df, preds,
            xlabel="Quarter",
            ylabel="Megalitres",
            title="Forecasts for quarterly beer production",
            rm_legend=False)
```

## Measures of forecast accuracy {.smaller}

::: {.columns}
::: {.column}
$y_{T+h}$ --- [actual value]{.hi} at time $T+h$

$\hat{y}_{T+h|T}$ --- [forecast value]{.hi} made at time $T$ for time $T+h$

$e_{T+h} = y_{T+h} - \hat{y}_{T+h|T}$ --- [forecast error]{.hi} at time $T+h$
:::
::: {.column}
- **Mean Error (ME)**: $\text{ME} = \frac{1}{n} \sum_{h=1}^{n} e_{T+h}$
- **Mean Absolute Error (MAE)**: $\text{MAE} = \frac{1}{n} \sum_{h=1}^{n} |e_{T+h}|$
- **Mean Squared Error (MSE)**: $\text{MSE} = \frac{1}{n} \sum_{h=1}^{n} e_{T+h}^2$
- **Root Mean Squared Error (RMSE)**: $\text{RMSE} = \sqrt{\frac{1}{n} \sum_{h=1}^{n} e_{T+h}^2}$
- **Mean Absolute Percentage Error (MAPE)**: $\text{MAPE} = \frac{100}{n} \sum_{h=1}^{n} \left| \frac{e_{T+h}}{y_{T+h}} \right|$
:::
:::

- MAE, MSE, RMSE are all scale-dependent.
- MAPE is scale-independent but can be problematic if actual values are close to zero.

## Scaled errors {.smaller}

Proposed by [Hyndman and Koehler (2006)](https://www.sciencedirect.com/science/article/abs/pii/S0169207006000239?via%3Dihub):

- For [non-seasonal data]{.hi}, scale errors using naive method:

$$
q_i = \frac{e_i}{\frac{1}{T-1} \sum_{t=2}^{T} |y_t - y_{t-1}|}
$$

- For [seasonal data]{.hi}, scale errors using seasonal naive method:

$$
q_i = \frac{e_i}{\frac{1}{T-m} \sum_{t=m+1}^{T} |y_t - y_{t-m}|}
$$

## Scaled errors {.smaller}

::: {.columns}
::: {.column}
- **Mean Absolute Scaled Error (MASE)**:

$$
\text{MASE} = \frac{1}{n} \sum_{i=1}^{n} |q_i|
$$
:::
::: {.column}
- **Root Mean Squared Scaled Error (RMSSE)**:

$$
\text{RMSSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} q_i^2}
$$

where

$$
q_i^2 = \frac{e_i^2}{\frac{1}{T-1} \sum_{t=m+1}^{T} (y_t - y_{t-1})^2}
$$

and we set $m=1$ for non-seasonal data.
:::
:::

---

```{python}
plot_series(beers_df, preds,
            xlabel="Quarter",
            ylabel="Megalitres",
            title="Forecasts for quarterly beer production",
            rm_legend=False)
```

```{python}
#| echo: true
#| output-location: slide
evaluation = evaluate(preds, 
  metrics=[rmse, mae, mape,
  partial(mase, seasonality=4)],
  train_df = train_df)

evaluation.round(2)
```

# Evaluating distributional accuracy

## Quantile scores

```{python}
#| echo: true
#| output-location: slide
#| fig-align: center
train_df = goog_df.query("ds.dt.year == 2015")
test_df = goog_df.query("ds.dt.year == 2016")
train_df["ds"] = np.arange(len(train_df["ds"]))
test_df["ds"] = np.arange(len(test_df["ds"]))

naive_model = Naive()

sf = StatsForecast(models=[naive_model], freq=1)
preds = sf.forecast(h=19, level=[80], df=train_df)
preds["y"] = test_df["y"].values

plot_series(train_df, preds, level=[80],
            xlabel="day",
            ylabel="$US",
            title="Google closing stock prices",
            rm_legend=False,)
```

## Quantile scores {.tiny}

::: {.columns}
::: {.column}
- For a given quantile level $\alpha$, the quantile score (QS) is defined as:

$$
Q_{p, t}= \begin{cases}2(1-p)\left(f_{p, t}-y_t\right), & \text { if } y_t<f_{p, t} \\ 2 p\left(y_t-f_{p, t}\right), & \text { if } y_t \geq f_{p, t}\end{cases}
$$

where $f_{p, t}$ is quantile forecast at level $p$ for time $t$. 

$y_t$ is the actual value at time $t$.

Expect probability $(y_t < f_{p, t}) = p$.
:::
::: {.column}
```{python}
#| eval: false
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation

# –í–∏–∑–Ω–∞—á–∞—î–º–æ —Ñ—É–Ω–∫—Ü—ñ—é –∫–≤–∞–Ω—Ç–∏–ª—å–Ω–æ–≥–æ —Ä–∞—Ö—É–Ω–∫—É (pinball loss)
def quantile_score(error, p):
    """
    –†–æ–∑—Ä–∞—Ö–æ–≤—É—î –∫–≤–∞–Ω—Ç–∏–ª—å–Ω–∏–π —Ä–∞—Ö—É–Ω–æ–∫.
    error: y_true - y_pred
    p: –∫–≤–∞–Ω—Ç–∏–ª—å (0 < p < 1)
    """
    return np.where(error < 0, 2 * (1 - p) * (-error), 2 * p * error)

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –∞–Ω—ñ–º–∞—Ü—ñ—ó
fig, ax = plt.subplots(figsize=(8, 6))
ax.set_xlim(-10, 10)
ax.set_ylim(0, 10)
ax.set_xlabel("Forecast error ($y_t - f_{p,t}$)")
ax.set_ylabel("Quantile score ($Q_{p,t}$)")
ax.grid(True)

line, = ax.plot([], [], lw=2.5)
title = ax.set_title("")
ax.axvline(0, color='grey', linestyle='--', lw=1)

# –î–∞–Ω—ñ –¥–ª—è –∞–Ω—ñ–º–∞—Ü—ñ—ó
p_values = np.arange(0.95, 0.04, -0.05)
error_range = np.linspace(-10, 10, 400)

# –§—É–Ω–∫—Ü—ñ—è —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó –¥–ª—è –∞–Ω—ñ–º–∞—Ü—ñ—ó
def init():
    line.set_data([], [])
    title.set_text("")
    return line, title

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –∫–æ–∂–Ω–æ–≥–æ –∫–∞–¥—Ä—É –∞–Ω—ñ–º–∞—Ü—ñ—ó
def update(frame):
    p = p_values[frame]
    qs = quantile_score(error_range, p)
    
    line.set_data(error_range, qs)
    title.set_text(f"–ó–∞–ª–µ–∂–Ω—ñ—Å—Ç—å $Q_{{p,t}}$ –≤—ñ–¥ –ø–æ–º–∏–ª–∫–∏ –ø—Ä–∏ $p = {p:.2f}$")
    
    # –û–Ω–æ–≤–ª—é—î–º–æ –∫–æ–ª—ñ—Ä –ª—ñ–Ω—ñ—ó –¥–ª—è –∫—Ä–∞—â–æ—ó –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
    # –ö–æ–ª—ñ—Ä –∑–º—ñ–Ω—é—î—Ç—å—Å—è –≤—ñ–¥ —Å–∏–Ω—å–æ–≥–æ (p~1) –¥–æ —á–µ—Ä–≤–æ–Ω–æ–≥–æ (p~0)
    color_intensity = (p - 0.05) / 0.9
    line.set_color(plt.cm.coolwarm(color_intensity))
    
    return line, title

# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∞–Ω—ñ–º–∞—Ü—ñ—ó
ani = animation.FuncAnimation(
    fig, 
    update, 
    frames=len(p_values),
    init_func=init, 
    blit=True,
    interval=200  # –Ü–Ω—Ç–µ—Ä–≤–∞–ª –º—ñ–∂ –∫–∞–¥—Ä–∞–º–∏ –≤ –º—ñ–ª—ñ—Å–µ–∫—É–Ω–¥–∞—Ö
)

output_dir = 'img'
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∞–Ω—ñ–º–∞—Ü—ñ—é —è–∫ GIF
gif_path = os.path.join(output_dir, 'quantile_score_animation.gif')
ani.save(gif_path, writer='pillow', fps=5)

# –î–ª—è –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è –≤ Jupyter/Quarto
plt.close(fig) # –ó–∞–ø–æ–±—ñ–≥–∞—î–º–æ —Å—Ç–∞—Ç–∏—á–Ω–æ–º—É –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—é
```

![](img/quantile_score_animation.gif)
:::
:::

- Low $Q_{p,t}$ is better.

## Quantile scores {.smaller}

- One-step ahead 10% quantile forecast (for 4 January 2016) is $f_{0.1, t} = 744.54$.
- Actual value is $y_t = 741.84$. Then:

$$
Q_{0.1, t} = 2 \times 0.1 \times (741.84 - 744.54) = 4.86
$$

```{python}
evaluation = evaluate(preds, metrics=[quantile_loss], level=[80])

methods = ['Naive']
evaluation_transformed = pd.DataFrame({
    'Method': methods,
    'Quantile loss (q=0.1)': evaluation[evaluation['metric'] ==
        'quantile_loss_q0.1'][methods].iloc[0].values,
    'Quantile loss (q=0.9)': evaluation[evaluation['metric'] ==
        'quantile_loss_q0.9'][methods].iloc[0].values,
})
evaluation_transformed.round(4)
```

## Winkler score

Winkler score evaluates the accuracy of prediction intervals.

For a $(1-\alpha)100\%$ prediction interval $[l_{\alpha, t}, u_{\alpha, t}]$:

$$
W_{\alpha, t}= \begin{cases}\left(u_{\alpha, t}-\ell_{\alpha, t}\right)+\frac{2}{\alpha}\left(\ell_{\alpha, t}-y_t\right) & \text { if } y_t<\ell_{\alpha, t} \\ \left(u_{\alpha, t}-\ell_{\alpha, t}\right) & \text { if } \ell_{\alpha, t} \leq y_t \leq u_{\alpha, t} \\ \left(u_{\alpha, t}-\ell_{\alpha, t}\right)+\frac{2}{\alpha}\left(y_t-u_{\alpha, t}\right) & \text { if } y_t>u_{\alpha, t}\end{cases}
$$

where $l_{\alpha, t}$ and $u_{\alpha, t}$ are the lower and upper bounds of the prediction interval at time $t$, respectively.

## Winkler score

- One-step ahead 80% prediction interval for 4 January 2016 is $[l_{0.2, t}, u_{0.2, t}] = [744.54,773.22]$.
- Actual value is $y_t = 741.84$. Then:

$$
W_{0.2, t} = (773.22 - 744.54) + \frac{2}{0.2} (744.54 - 741.84) = 55.68
$$

## Continuous Ranked Probability Score {.smaller}

$$
\text{CRPS}(F_t, y_t) = \int_{-\infty}^{\infty} (F_t(z) - \mathbf{1}\{y_t \leq z\})^2 dz
$$

- $F_t(z)$ is the predictive cumulative distribution function (CDF) at time $t$.
- $y_t$ is the actual observed value at time $t$.
- $\mathbf{1}\{y_t \leq z\}$ is an indicator function that equals 1 if $y_t \leq z$ and 0 otherwise.
- CRPS measures the difference between the predicted CDF and the actual outcome, integrating over all possible values.
- Lower CRPS values indicate better probabilistic forecasts.

---

```{python}
#| echo: true
#| output-location: slide
train_df = goog_df.query("ds.dt.year == 2015")
test_df = goog_df.query("ds.dt.year == 2016")
mean_method = HistoricAverage()
naive_method = Naive()
drift_method = RandomWalkWithDrift()
sf = StatsForecast(
  models=[drift_method, naive_method, mean_method], freq="B"
)
levels = list(np.arange(0, 100, 1 / 10))
preds = sf.forecast(df=train_df, h=len(test_df), level=levels)
preds["y"] = test_df["y"].values
models = ['RWD', 'HistoricAverage', 'Naive']
crps_df = evaluate(
    df=preds,
    models=models,
    metrics = [mqloss],
    level=levels,
)
crps_df = crps_df[models].T.reset_index()
crps_df.columns = ["method", "CRPS"]
crps_df["CRPS"] *= 2
crps_df.round(4)
```

## Scale-free comparisons using skill scores

- Skill score compares forecast accuracy relative to a benchmark model.
- For a given accuracy measure $M$, the skill score is defined as:

$$
\text{Skill Score} = 1 - \frac{M_{\text{model}}}{M_{\text{benchmark}}}
$$

```{python}
naive_crps = crps_df.query("method == 'Naive'")["CRPS"].iloc[0]
crps_df['skill_score'] = crps_df.apply(
    lambda row: (naive_crps - row['CRPS']) / \
      naive_crps if row['method'] != 'Naive' else 0,
    axis=1
)
crps_df.round(2)
```

# Times series cross-validation

## Time series cross-validation {.smaller}

Traditional evaluation:

```{python}
import matplotlib.pyplot as plt
import numpy as np

# --- –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫—É ---
fig, ax = plt.subplots(figsize=(12, 2))

# –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ –º–µ–∂—ñ, —â–æ–± –±—É–ª–æ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –º—ñ—Å—Ü—è
ax.set_ylim(-1, 1)
ax.set_xlim(-1, 26)

# –ü—Ä–∏–±–∏—Ä–∞—î–º–æ –æ—Å—ñ —Ç–∞ —Ä–∞–º–∫—É –¥–ª—è —á–∏—Å—Ç–æ–≥–æ –≤–∏–≥–ª—è–¥—É
ax.axis('off')

# --- –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö —Ç–∞ –∫–æ–ª—å–æ—Ä—ñ–≤ ---
# –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ—á–æ–∫ –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö —Ç–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
n_train = 18
n_test = 5

# –°—Ç–≤–æ—Ä—é—î–º–æ –º–∞—Å–∏–≤–∏ –∑ –ø–æ–∑–∏—Ü—ñ—è–º–∏ —Ç–æ—á–æ–∫ –Ω–∞ –æ—Å—ñ X
train_points = np.arange(0, n_train)
test_points = np.arange(n_train, n_train + n_test)

# –ö–æ–ª—å–æ—Ä–∏, —â–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—é—Ç—å –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—é
train_color = blue
test_color = orange
axis_color = 'black'

# --- –ú–∞–ª—é—î–º–æ –µ–ª–µ–º–µ–Ω—Ç–∏ –≥—Ä–∞—Ñ—ñ–∫—É ---

# 1. –ì–æ–ª–æ–≤–Ω–∞ –≤—ñ—Å—å —á–∞—Å—É –∑—ñ —Å—Ç—Ä—ñ–ª–∫–æ—é
# –ú–∞–ª—é—î–º–æ –ª—ñ–Ω—ñ—é, –∞ –ø–æ—Ç—ñ–º –¥–æ–¥–∞—î–º–æ —Å—Ç—Ä—ñ–ª–∫—É –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é annotate
ax.axhline(y=0, color=axis_color, zorder=1)
# –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ annotate –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å—Ç—Ä—ñ–ª–∫–∏ –Ω–∞ –∫—ñ–Ω—Ü—ñ
ax.annotate('', xy=(n_train + n_test + 1, 0), xytext=(-1, 0),
            arrowprops=dict(arrowstyle="->", color=axis_color))


# 2. –ú–∞–ª—é—î–º–æ —Ç–æ—á–∫–∏ –¥–∞–Ω–∏—Ö
# –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ
ax.scatter(train_points, np.zeros_like(train_points),
           color=train_color, s=100, zorder=2)

# –¢–µ—Å—Ç–æ–≤—ñ –¥–∞–Ω—ñ
ax.scatter(test_points, np.zeros_like(test_points),
           color=test_color, s=100, zorder=2)


# 3. –î–æ–¥–∞—î–º–æ —Ç–µ–∫—Å—Ç–æ–≤—ñ –ø—ñ–¥–ø–∏—Å–∏
# –ü—ñ–¥–ø–∏—Å –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
train_text_pos = np.mean(train_points)
ax.text(train_text_pos, 0.4, 'Training data',
        ha='center', va='center', fontsize=20, color=train_color)

# –ü—ñ–¥–ø–∏—Å –¥–ª—è —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
test_text_pos = np.mean(test_points)
ax.text(test_text_pos, 0.4, 'Test data',
        ha='center', va='center', fontsize=20, color=test_color)

# –ü—ñ–¥–ø–∏—Å "time"
ax.text(n_train + n_test + 1.5, 0.1, 'time',
        ha='left', va='center', fontsize=18, color=axis_color)

# --- –í—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É ---
plt.show()
```

---

Time series cross-validation:

```{python}
#| fig-align: center
def plot_ts_cross_validation(
    n_folds=15,
    initial_train_size=4,
    h=1,
    step=1,
    window_size=25,  # –ù–æ–≤–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è —Ñ—ñ–∫—Å–æ–≤–∞–Ω–æ—ó –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —Ç–æ—á–æ–∫
    ax=None
):
    """
    –í—ñ–∑—É–∞–ª—ñ–∑—É—î –ø–µ—Ä–µ—Ö—Ä–µ—Å–Ω—É –≤–∞–ª—ñ–¥–∞—Ü—ñ—é –¥–ª—è —á–∞—Å–æ–≤–∏—Ö —Ä—è–¥—ñ–≤ –∑ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏–º –≤—ñ–∫–Ω–æ–º.

    –ü–∞—Ä–∞–º–µ—Ç—Ä–∏:
    - n_folds (int): –ö—ñ–ª—å–∫—ñ—Å—Ç—å —ñ—Ç–µ—Ä–∞—Ü—ñ–π (–∑—Ä—ñ–∑—ñ–≤) –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó.
    - initial_train_size (int): –ü–æ—á–∞—Ç–∫–æ–≤–∏–π —Ä–æ–∑–º—ñ—Ä —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–æ–≥–æ –Ω–∞–±–æ—Ä—É.
    - h (int): –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è.
    - step (int): –ö—Ä–æ–∫, –Ω–∞ —è–∫–∏–π –∑–±—ñ–ª—å—à—É—î—Ç—å—Å—è —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏–π –Ω–∞–±—ñ—Ä.
    - window_size (int): –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ—á–æ–∫ –¥–ª—è –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è –≤ –∫–æ–∂–Ω–æ–º—É —Ä—è–¥–∫—É.
    - ax (matplotlib.axes.Axes, optional): –í—ñ—Å—å –¥–ª—è –º–∞–ª—é–≤–∞–Ω–Ω—è.
    """
    # –ö–æ–ª—å–æ—Ä–∏
    train_color = blue
    test_color = orange
    future_color = 'lightgray'
    axis_color = 'black'

    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫—É, —è–∫—â–æ –Ω–µ –Ω–∞–¥–∞–Ω–æ —ñ—Å–Ω—É—é—á–∏–π
    if ax is None:
        fig, ax = plt.subplots(figsize=(12, 8))
    
    # –Ü—Ç–µ—Ä–∞—Ü—ñ—è –ø–æ –∫–æ–∂–Ω–æ–º—É –∑—Ä—ñ–∑—É –ø–µ—Ä–µ—Ö—Ä–µ—Å–Ω–æ—ó –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
    for i in range(n_folds):
        y_pos = n_folds - 1 - i  # –ü–æ–∑–∏—Ü—ñ—è –ø–æ –æ—Å—ñ Y (–º–∞–ª—é—î–º–æ –∑–≥–æ—Ä–∏ –¥–æ–Ω–∏–∑—É)
        
        # –†–æ–∑–º—ñ—Ä —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–æ–≥–æ –Ω–∞–±–æ—Ä—É –¥–ª—è –ø–æ—Ç–æ—á–Ω–æ–≥–æ –∑—Ä—ñ–∑—É
        current_train_size = initial_train_size + i * step
        
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ —Ç–æ—á–∫–∏ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑—É
        forecast_start = current_train_size
        forecast_end = forecast_start + h
        
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ —Ç–æ—á–∫–∏ –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –≤ –º–µ–∂–∞—Ö window_size
        all_points = np.arange(window_size)
        
        # –†–æ–∑–ø–æ–¥—ñ–ª—è—î–º–æ —Ç–æ—á–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è—Ö
        train_points = all_points[all_points < forecast_start]
        gap_points = all_points[(all_points >= forecast_start) & (all_points < forecast_end - 1)]
        test_point = all_points[all_points == forecast_end - 1]
        future_points = all_points[all_points >= forecast_end]

        # 1. –ú–∞–ª—é—î–º–æ –≤—ñ—Å—å —á–∞—Å—É –∑—ñ —Å—Ç—Ä—ñ–ª–∫–æ—é
        ax.arrow(0, y_pos, window_size, 0, 
                 head_width=0.2, head_length=0.4, 
                 fc=axis_color, ec=axis_color, 
                 length_includes_head=True, zorder=1)

        # 2. –ú–∞–ª—é—î–º–æ —Ç–æ—á–∫–∏ –¥–∞–Ω–∏—Ö
        ax.scatter(train_points, np.full_like(train_points, y_pos),
                   color=train_color, s=50, zorder=2)
        if h > 1 and len(gap_points) > 0:
            ax.scatter(gap_points, np.full_like(gap_points, y_pos),
                       color=future_color, s=50, zorder=2)
        if len(test_point) > 0:
            ax.scatter(test_point, y_pos,
                       color=test_color, s=50, zorder=2)
        if len(future_points) > 0:
            ax.scatter(future_points, np.full_like(future_points, y_pos),
                       color=future_color, s=50, zorder=2)

    # --- –§—ñ–Ω–∞–ª—å–Ω–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –≤–∏–≥–ª—è–¥—É ---
    ax.set_xlabel('time', fontsize=14)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['bottom'].set_visible(False)
    ax.get_yaxis().set_visible(False)
    ax.set_xticks([])
    ax.set_xlim(-1, window_size + 1)
    ax.set_ylim(-1, n_folds)

# –ü—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Ñ—É–Ω–∫—Ü—ñ—ó –¥–ª—è –≤—ñ–¥—Ç–≤–æ—Ä–µ–Ω–Ω—è –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ –º–∞–ª—é–Ω–∫–∞
plot_ts_cross_validation(n_folds = 15, h=1, window_size=25)
plt.show()
```

---

Time series cross-validation:

```{python}
#| fig-align: center
plot_ts_cross_validation(n_folds = 15, h=2, window_size=25)
plt.show()
```

---

Time series cross-validation:

```{python}
#| fig-align: center
plot_ts_cross_validation(n_folds = 15, h=3, window_size=25)
plt.show()
```

---

Time series cross-validation:

```{python}
#| fig-align: center
plot_ts_cross_validation(n_folds = 15, h=4, window_size=25)
plt.show()
```

## Cross-validation {.smaller}

`cross_validation()` function from StatsForecast package:



```{python}
#| echo: true
#| output-location: column
goog_2015 = goog_df.query("ds.dt.year == 2015")
drift_method = RandomWalkWithDrift()

sf = StatsForecast(models=[drift_method], freq="B")
cv_df = sf.cross_validation(h=1, df=goog_2015, step_size=1, test_size=249)

cv_df.round(2).head()
```

## Cross-validation {.smaller}

```{python}
#| echo: true
#| output-location: column
evaluation_cv = evaluate(
    df=cv_df,
    models=['RWD'],
    metrics=[rmse, mae, mape],
    train_df=goog_2015
)
evaluation_cv.round(2)
```


# Questions? {.unnumbered .unlisted background-iframe=".04_files/libs/colored-particles/index.html"}

<br> <br>

{{< iconify solar book-bold >}} [Course materials](https://teaching.kse.org.ua/course/view.php?id=3416)

{{< iconify mdi envelope >}} imiroshnychenko\@kse.org.ua

{{< iconify ic baseline-telegram >}} [@araprof](https://t.me/araprof)

{{< iconify mdi youtube >}} [@datamirosh](https://www.youtube.com/@datamirosh)

{{< iconify mdi linkedin >}} [\@ihormiroshnychenko](https://www.linkedin.com/in/ihormiroshnychenko/)

{{< iconify mdi github >}} [\@aranaur](https://github.com/Aranaur)

{{< iconify ion home >}} [aranaur.rbind.io](https://aranaur.rbind.io)
