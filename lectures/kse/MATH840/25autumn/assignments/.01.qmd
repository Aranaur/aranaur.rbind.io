---
title: "Assignment 01: Time Series Data Collection and Validation"
subtitle: "MATH840: Time Series"
author: "Ihor Miroshnychenko"
institute: Kyiv School of Economics
format:
  html: 
    highlight-style: github
    theme: [default, custom.scss]
execute: 
  echo: true
  warning: false
editor_options: 
  chunk_output_type: console
---

## üéØ Goals

The goal of this assignment is to introduce you to the diversity of time series data and prepare you for future analysis and forecasting tasks. You will practice finding, loading, and performing fundamental quality checks on various time series datasets, which you will use for subsequent assignments in this course.

## üìù Instructions

### Part 1: Data Collection

1.  **Find 3 to 5 distinct time series datasets.** These datasets will be your personal corpus for the rest of the course. They should represent a variety of the characteristics discussed in the lecture. Aim for diversity in:
    *   **Frequency:**
        *   High-frequency (e.g., hourly, daily)
        *   Low-frequency (e.g., monthly, quarterly, or yearly)
    *   **Behavioral Characteristics:** Try to find at least one dataset for each of the following patterns:
        *   A clear **trend** (upward or downward).
        *   Strong **seasonality** (a repeating pattern over a fixed period).
        *   **Cyclical** patterns (long-term fluctuations without a fixed period). --- *optional*
        *   **Noisy** or seemingly random behavior. --- *optional*

2.  **Suggested Data Sources:** You are encouraged to find data that interests you personally or professionally. Here are some excellent places to start:
    *   [Kaggle Datasets](https://www.kaggle.com/datasets)
    *   [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)
    *   [Google Dataset Search](https://datasetsearch.research.google.com/)
    *   National statistical agencies (e.g., [U.S. Bureau of Labor Statistics](https://www.bls.gov/data/), [Eurostat](https://ec.europa.eu/eurostat/data/database))
    *   Financial data providers (e.g., Yahoo Finance, Quandl)
    *   [Monash Time Series Forecasting Repository](https://forecastingdata.org/) (home of the M-Competitions)

3.  **Data Description:** For each dataset you select, provide a brief summary including:
    *   The name of the dataset.
    *   The source (a direct link if possible).
    *   A brief description of what the data measures.
    *   The frequency of the data (e.g., daily, monthly).
    *   The time span covered by the data (start and end dates).

### Part 2: Data Validation and Initial Analysis

For each of your chosen datasets, perform the following steps using Python, primarily with the `pandas`/`polars`/`duckdb` or other libraries of your choice:

1.  **Load Data:** Load the data into a `pandas` DataFrame. Ensure your time column is correctly parsed and set as a `datetime` object.

2.  **Check for Missing Values:**
    *   Determine if there are any missing values (`NaN`) in the dataset.
    *   Count the total number of missing values for each variable.
    *   **Note:** Your task is to *identify and document* them, not to impute them at this stage.

3.  **Validate the Timestamp Sequence:**
    *   Verify that the time series is continuous according to its frequency. For example, in a daily dataset, are there any missing days? In a monthly dataset, are all months accounted for?
    *   You can approach this by checking the difference between consecutive timestamps or by creating an expected date range and checking for missing entries.

4.  **Check for Duplicates:**
    *   Identify if there are duplicate entries for the same timestamp.
    *   If duplicates are found, describe your proposed strategy for handling them (e.g., remove them, keeping the first or last entry; average the values). You do not need to implement the solution yet, just describe your plan.

## üì§ Submission

Submit a Quarto document (`.qmd`) or Jupyter Notebook (`.ipynb`) and the rendered HTML[^html] file containing:

*   A summary of each dataset you selected (as described in Part 1).
*   The code and outputs for loading, validating, and performing the initial analysis on each dataset (as described in Part 2).
*   A brief discussion of any issues you encountered during the validation process and how you plan to address them in future analyses.

[^html]: If you render to HTML, please publish your report on Quarto Pub or GitHub Pages and include the link in your submission.