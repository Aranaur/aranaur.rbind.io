---
title: "t-test"
subtitle: "Probability and Statistics"
author: "Ihor Miroshnychenko"
institute: Kyiv School of Economics
from: markdown+emoji
footer: "Probability and Statistics"
format:
  revealjs: 
    code-line-numbers: false
    navigation-mode: vertical
    transition: fade
    background-transition: fade
    chalkboard: true
    logo: img/kse.png
    slide-number: true
    # toc: true
    # toc-depth: 1
    mouse-wheel: true
    width: 1350  
    height: 759.375
    # fig-height: 9
    # fig-width: 16
    highlight-style: github
    fig-format: svg
    theme: [default, custom.scss]
    mermaid:
      theme: forest
preload-iframes: true
execute: 
  echo: true
  warning: false
editor_options: 
  chunk_output_type: console

revealjs-plugins:
  - verticator
---

```{r}
#| label: setup
#| include: false

library(tidyverse)

# Define colors
red_pink   = "#e64173"
turquoise  = "#20B2AA"
orange     = "#FFA500"
red        = "#fb6107"
blue       = "#181485"
navy       = "#150E37FF"
green      = "#8bb174"
yellow     = "#D8BD44"
purple     = "#6A5ACD"
slate      = "#314f4f"
```

# $t$-test 

## üìà Task {.smaller}

> Our company wants to switch from one DBMS to another. The main criterion for the transition is ‚Äúthe time spent per day loading new data.‚Äù If **previously** it took an average of **10 hours** to update the database on a daily basis, we want to find a new DBMS that will do it faster than **7 hours**.
>
> To do this, we decided to transfer all the data to the new database under test. During one week, we will calculate the data loading time every day, and if the average time for updating is less than 7 hours, then we will completely switch to the new DBMS. Your task is to come up with a way to test the hypothesis that the new DBMS is better than the old one.

The sample is:

- `6.9, 6.45, 6.32, 6.88, 6.19, 7.13, 6.76` --- the time to load into the new database by day in hours.

## üìä Hypothesis

- $X_1, X_2, ..., X_7$ --- the time of loading new data into the DBMS in hours for each day of the experiment
- $X$ from the normal distribution.
- Average time of loading into the DBMS: `r mean(c(6.9, 6.45, 6.32, 6.88, 6.19, 7.13, 6.76)) %>% round(2)` hours.

```{r}
#| label: data-gen
#| include: false

X = c(6.9, 6.45, 6.32, 6.88, 6.09, 7.13, 6.76)
cat("Average time of loading into the DBMS:" , mean(X) |> round(2), "hours")
```

- $H_0: \bar{X} \geq 7$
- $H_1: \bar{X} < 7$

## üìê $Z$-test

- The statistic $Z(X) = \sqrt{n}\dfrac{\overline X - \mu_0}{\sqrt{\sigma^2}}$

. . .

Then you just need to calculate the following statistics: $$\sqrt{n}\dfrac{\overline X - 7}{\sqrt{\sigma^2}} \overset{H_0}{\sim} \mathcal{N}(0, 1)$$

. . .

::: {.callout-important}
But there's one problem: [we don't know $\sigma^2$!]{.hi}
:::

## üìê $t'$-test {.smaller}

$\widehat{\sigma^2} =S^2 = \dfrac{1}{n - 1}\underset{i=1}{\overset{n}{\sum}}(X_i - \overline X)^2$

. . .

```{r}
#| label: sigma-hat
#| include: false

cat("Sample variance: ", var(X))
```

Sample variance: `r var(X) %>% round(2)`

. . .

Let's introduce a new test, the $t$-test, in which we substitute:

- $T(X) := \sqrt{n}\dfrac{\overline X - \mu_0}{\sqrt{S^2}}$
- $T(X) \overset{H_0}{\sim} \mathcal{N}(0, 1)$ ü§®

. . .

> It remains to be seen: Is it true that at $H_0$ the distribution of the statistic T is standard normal?

## üìä Distribution of statistics

- We will generate a $X$ sample $M$ times and calculate the statistic $T(X)$ each time.
- As a result, we will get a sample of size $M$ for $T(X)$ and can draw a histogram of the distribution. 
- We will also plot the distribution $\mathcal{N}(0, 1)$ separately. 
- If the empirical distribution visually coincides with the theoretical normal distribution, then everything is fine. If not, then we cannot simply replace $\sigma^2$ with $S^2$.
    - Additionally, let's see what happens if we replace $T(X)$ with $Z(X)$.

## R and üìä Statistics distribution {.smaller visibility="hidden"}

- $M = 100000$
- Sample size: $n = 7$

```{r}
#| label: t-test-sim-def

M <- 100000
sample_size <- 7

plot_data <- tibble(
  T_X = map_dbl(1:M, ~ {
    sample <- rnorm(sample_size, mean = 5, sd = 3)
    sqrt(sample_size) * (mean(sample) - 5) / sqrt(var(sample))
  }),
  Z_X = map_dbl(1:M, ~ {
    sample <- rnorm(sample_size, mean = 5, sd = 3)
    sqrt(sample_size) * (mean(sample) - 5) / 3
  })
) %>% 
  pivot_longer(everything(), names_to = "Statistic", values_to = "Value")

plot_data
```

## üìä Statistics distribution {.smaller}

```{r}
#| label: t-test-sim-run
#| echo: false
#| fig-align: center
#| fig-width: 22
#| fig-height: 5

plot_data %>% 
    ggplot(aes(x = Value, group = Statistic)) +
    geom_histogram(aes(y = ..density..), bins = 30, fill = "skyblue", alpha = 0.6) +
    geom_density(aes(color = blue), size = 1) +
    stat_function(aes(color = red), fun = dnorm, args = list(mean = 0, sd = 1), size = 1) +
    facet_wrap(~ Statistic, scales = "free") +
    labs(
      title = "Distribution of statistics T(X) and Z(X)",
      x = "Value",
      y = "Density",
      color = "Distribution"
    ) +
    theme_minimal() +
    xlim(quantile(plot_data$Value, 0.001), quantile(plot_data$Value, 0.999)) +
    scale_color_manual(values = c(red, blue), labels = c("Empirical", "Theoretical"))
```

::: {.callout-important icon=false}
- The $Z$-test works here: $\sqrt{n}\dfrac{\overline X - \mu_0}{\sqrt{\sigma^2}} \sim \mathcal{N}(0, 1)$.
- But this is not the case for $T(X)$! [They are different! So the $t'$-test is not suitable for the original problem!]{.hi}
:::

## üìù Why did it happen? {.tiny}

- $\sigma$ = 3

$S^2$ is a random variable!

Let's look at the distribution of $\sqrt{S^2}$ on the same normal distribution.

```{r}
#| label: sigma-hat-sim
#| echo: false
#| fig-align: center

sigma_hat <- map_dbl(1:M, ~ {
  sample <- rnorm(sample_size, mean = 5, sd = 3)
  sqrt(var(sample))
})

tibble(Sigma_hat = sigma_hat) %>% 
  ggplot(aes(x = Sigma_hat)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "skyblue", alpha = 0.6) +
  geom_density(aes(color = blue), size = 1) +
  labs(
    title = "Distribution of the sample standard deviation",
    x = "Value",
    y = "Density",
    color = "Distribution"
  ) +
  theme_minimal() +
  xlim(quantile(sigma_hat, 0.001), quantile(sigma_hat, 0.999)) +
  scale_x_continuous(breaks = seq(0, 6, 1)) +
  theme(legend.position = "none")
```

::: {.callout-note icon=false}
## Conclusion.

$T(X) \overset{H_0}{\nsim} \mathcal{N}(0, 1)$ üò≠
:::

## What is the distribution of $T(X)$? {.tiny40}

::: {.callout-caution collapse="true" icon=false}

1. Let $X_1 \ldots X_n \sim \mathcal{N}(\mu, \sigma^2)$

2. Let $\xi_1 \ldots \xi_n \sim \mathcal{N}(0, 1)$. Then $\eta=\xi_1^2 +\ ... +\xi_n^2 \sim \chi^2_n$, &mdash; [**$\chi^2$ distribution with $n$ degrees of freedom**](https://en.wikipedia.org/wiki/Chi-squared_distribution).
    - Then $\underset{i=1}{\overset{n}{\sum}}\left(\xi_i - \overline \xi \right)^2 \sim \chi^2_{n-1}$. [Proof](https://en.wikipedia.org/wiki/Cochran%27s_theorem) ü´†

    - $S^2_X = \dfrac{1}{n - 1}\underset{i=1}{\overset{n}{\sum}}(X_i - \overline X)^2$

    - $\xi_i := \dfrac{X_i - \mu}{\sigma} \sim \mathcal{N}(0, 1)$. Then $S^2_{\xi} = \dfrac{1}{\sigma^2}S^2_X$.

    - So $\dfrac{(n - 1)\cdot S^2_X}{\sigma^2} = \underset{i=1}{\overset{n}{\sum}}\left(\xi_i - \overline \xi \right)^2 \sim \chi^2_{n-1}$.

3. Let $\xi \sim \mathcal{N}(0, 1), \eta \sim \chi^2_k$ and $\xi$ with $\eta$ be independent. Then the statistic $\zeta = \dfrac{\xi}{\sqrt{\eta/k}} \sim t_{k}$ &mdash; from the [Student's distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution) with $k$ degrees of freedom.

    - $\xi := \sqrt{n}\dfrac{\overline X - \mu_0}{\sigma} \sim \mathcal{N}(0, 1)$
    - $\eta := \dfrac{(n - 1)\cdot S^2_X}{\sigma^2} \sim \chi^2_{n-1}$
    - $\xi$ and $\eta$ are [independent](https://math.stackexchange.com/questions/4165803/overlinex-and-s2-are-independent)
    - Then
:::

$$
\begin{align}
T = \sqrt{n}\dfrac{\overline X - \mu_0}{\sqrt{S^2}} = \frac{\sqrt{n}\dfrac{\overline X - \mu_0}{\sigma}}{\sqrt{\dfrac{(n - 1)\cdot S^2_X}{(n - 1)\sigma^2}}} = \dfrac{\xi}{\sqrt{\dfrac{\eta}{n-1}}} \sim t_{n - 1}
\end{align}
$$

## üìà Problem and üìê $t$-test {.smaller}

$T = \sqrt{n}\dfrac{\overline X - \mu_0}{\sqrt{S^2}} \sim t_{n - 1}$ --- taken from a Student's distribution with $n - 1$ degrees of freedom.

<br>

```{r}
#| label: t-test-data
#| output-location: column
#| include: false

X = c(6.9, 6.45, 6.32, 6.88, 6.09, 7.13, 6.76)
t_stat = sqrt(7) * (mean(X) - 7) / sqrt(var(X))
cat("t-statistic:", t_stat |> round(2))
```

- X = `r X`
- $n$ = `r length(X)`
- $S$ = `r var(X) %>% round(2)`
- $t$-statistic = `r t_stat %>% round(2)`
- $t_{n - 1}$ = `r qt(0.05, df = length(X) - 1) %>% round(2)`
- $p$-value = `r pt(t_stat, df = length(X) - 1) %>% round(4)`






<br>

```{r}
#| label: t-test-p-value
#| output-location: column
#| include: false

p_value = pt(t_stat, df = sample_size - 1)
cat("p-value:", p_value |> round(4))
```

<br>

```{r}
#| label: t.test
#| output-location: column
#| include: false

t.test(X, mu = 7, alternative = "less")
```

# Confidence intervals 

## üìä Confidence interval for the mean: option 1 {.tiny}

[Confidence interval]{.hi} --- set $m$: the test does not reject $H_0: \mu = m$ at the significance level $\alpha$.

- Let $\mu$ be the true mean of the sample. We also know that for $H_0: \sqrt{n}\dfrac{\overline X - m}{\sqrt{S^2}} \sim t_{n - 1}$.

- We are interested in $m$ such that: $\left\{-t_{n-1, 1 - \frac{\alpha}{2}} < \sqrt{n}\dfrac{\overline X - m}{\sqrt{S^2}} < t_{n-1, 1 - \frac{\alpha}{2}} \right\}$, in which case the criterion will not be rejected.

- Let's write it so that only $m$ remains in the center:

$$\left\{\overline X - \dfrac{t_{n - 1, 1 - \alpha/2} \sqrt{S^2}}{\sqrt{n}} < m < \overline X + \dfrac{t_{n - 1, 1 - \alpha/2} \sqrt{S^2}}{\sqrt{n}}\right\}$$

- Then the confidence interval:

$$CI_{\mu} = \left(\overline X \pm \dfrac{t_{n - 1, 1 - \alpha/2} \sqrt{S^2}}{\sqrt{n}} \right),$$ 

where $S^2 = \dfrac{1}{n - 1}\underset{i=1}{\overset{n}{\sum}}(X_i - \overline X)^2$

## üìä Confidence interval for the mean: option 2 {.tiny}

[Classical definition](https://en.wikipedia.org/wiki/Confidence_interval#Definition) of a confidence interval:

> A confidence interval for a parameter $\theta$ of confidence level $1 - \alpha$ is a pair of statistics $L(X), R(X)$ such that $P(L(X) < \theta < R(X)) = 1 - \alpha$.

$$
\begin{align}
&T(X) = \sqrt{n}\dfrac{\overline X - \mu}{\sqrt{S^2}} \sim t_{n - 1} \Rightarrow \\
&P\left(-t_{n - 1, 1-\alpha/2} < \sqrt{n}\dfrac{\overline X - \mu}{\sqrt{S^2}} < t_{n - 1, 1-\alpha/2} \right) = 1 - \alpha \Leftrightarrow \\
&P\left(\overline X - \dfrac{t_{n - 1, 1 - \alpha/2} \sqrt{S^2}}{\sqrt{n}}  < \mu < \overline X + \dfrac{t_{n - 1, 1 - \alpha/2} \sqrt{S^2}}{\sqrt{n}} \right) = 1 - \alpha
\end{align}
$$

- Then the confidence interval:

$$CI_{\mu} = \left(\overline X \pm \dfrac{t_{n - 1, 1 - \alpha/2} \sqrt{S^2}}{\sqrt{n}} \right)$$

## üìà Task and üìä Confidence interval

```{r}
#| label: ci
#| output-location: column
#| include: false

sample  <- rnorm(100, mean = 10, sd = 2)
alpha <- 0.05
n <- length(sample)

t_stat <- qt(1 - alpha / 2, df = n - 1)
t_stat

mean(sample) + c(-1, 1) * t_stat * sd(sample) / sqrt(n)

t.test(sample, conf.level = 1 - alpha)$conf.int
```

$$CI_{\mu} = \left(\overline X \pm \dfrac{t_{n - 1, 1 - \alpha/2} \sqrt{S^2}}{\sqrt{n}} \right)$$

```{r}
#| label: ci-data
#| output-location: column
#| include: false

X = c(6.9, 6.45, 6.32, 6.88, 6.09, 7.13, 6.76)
alpha <- 0.05
n <- length(X)

t_stat <- qt(1 - alpha / 2, df = n - 1)
t_stat

mean(X) + c(-1, 1) * t_stat * sd(X) / sqrt(n)

t.test(X, conf.level = 1 - alpha)$conf.int
```

- X = `r X`
- $\bar{X}$ = `r mean(X) %>% round(2)`
- $n$ = `r length(X)`
- $S$ = `r sd(X) %>% round(2)`
- $t_{n - 1, 1 - \alpha/2}$ = `r t_stat %>% round(2)`

Then the confidence interval for the mean is: `r t.test(X, conf.level = 1 - alpha)$conf.int`

# Two-sample $t$-test. The task of AB testing 

## üìä Task

> We have promotion services on our website. We want to start giving discounts on them to attract more people and start earning more money. To do this, we decided to conduct an AB test:
> We did not give discounts to one half of the users, and in the other half, we gave discounts to all new users. We need to understand whether we started to earn more money.

This time we have 2 samples: $A$ - control, and $B$ - test.

## üìä Task {.tiny40}

$$H_0: \mathbb{E} A = \mathbb{E} B \; \text{vs.} \; H_1: \mathbb{E}A < \mathbb{E} B$$

:::: {.columns}

::: {.column width="50%"}
1. **Both samples are normal**. 

Then there are 2 criteria depending on our knowledge of variance:

- $\sigma^2_A = \sigma^2_B$.
    
    Then:

    - $S^2_{pooled} = \dfrac{(N - 1)S^2_A + (M - 1)S^2_B}{N + M - 2}$, where N, M are the size of the control and test, respectively. And the criterion is as follows:
    - $T(A, B) = \dfrac{\overline A - \overline B}{S_{pooled}\sqrt{1/N + 1/M}} \overset{H_0}{\sim} T_{n + m - 2}$
   
- $\sigma^2_A \neq \sigma^2_B$.
    
    Then:
    
    - $T(A, B) = \dfrac{\overline A - \overline B}{\sqrt{S^2_{A}/N + S^2_{B}/M}} \overset{H_0}{\sim} T_{v}$
    - –¥–µ $v = \dfrac{\left(\dfrac{S^2_{A}}{N} + \dfrac{S^2_{B}}{M} \right)^2}{\left(\dfrac{(S^2_{A})^2}{N^2(N - 1)} + \dfrac{(S^2_{B})^2}{M^2(M-1)} \right)}$
:::

::: {.column width="50%"}
2. **At least 1 sample is abnormal**. 

Then the normal approximation with a large sample size, the $t'$-test criterion, comes into play:

- $T(A, B) = \dfrac{\overline A - \overline B}{\sqrt{S^2_{A}/N + S^2_{B}/M}} \overset{H_0}{\sim} \mathcal{N}(0, 1)$
:::

::::

## Two-Sample $t$-test and penguins {.tiny}

```{r}
#| label: adelie-plot
#| echo: false
#| fig-align: center

library(palmerpenguins)

adelie <- penguins %>%
  drop_na() %>%
        filter(species == "Adelie")

adelie %>% 
  ggplot(aes(x = body_mass_g, fill = sex)) +
  geom_density(size = 1 , alpha = 0.7) +
  labs(
    title = "Distribution of the body mass of Adelie penguins",
    x = "Body mass, g",
    y = "Density") +
  theme_minimal() +
  scale_fill_manual(values = c(red_pink, blue))
```

- $H_0$: $\mu_m = \mu_f$
- $H_1$: $\mu_m \neq \mu_f$

$T(A, B) =$ `r t.test(adelie$body_mass_g ~ adelie$sex, var.equal = TRUE)$statistic %>% round(2)`

$p$-value = `r sprintf("%.30f", t.test(adelie$body_mass_g ~ adelie$sex, var.equal = TRUE)$p.value)`

$CI_{\mu} =$ `r t.test(adelie$body_mass_g ~ adelie$sex, var.equal = TRUE)$conf.int`